{
  "5c6107d6da56297340b1a17e": {
    "code_links": [
      "None"
    ],
    "tasks": [
      "Memory management in shared-memory switches"
    ],
    "datasets": [
      "None"
    ],
    "methods": [
      "Longest Queue Drop policy"
    ],
    "results": [
      "Longest Queue Drop policy is 1.5-competitive"
    ],
    "paper_id": "5c6107d6da56297340b1a17e",
    "title": "The Longest Queue Drop Policy for Shared-Memory Switches is\n  1.5-competitive",
    "abstract": "  We consider the Longest Queue Drop memory management policy in shared-memory switches consisting of $N$ output ports. The shared memory of size $M\\geq N$ may have an arbitrary number of input ports. Each packet may be admitted by any incoming port, but must be destined to a specific output port and each output port may be used by only one queue. The Longest Queue Drop policy is a natural online strategy used in directing the packet flow in buffering problems. According to this policy and assuming unit packet values and cost of transmission, every incoming packet is accepted, whereas if the shared memory becomes full, one or more packets belonging to the longest queue are preempted, in order to make space for the newly arrived packets. It was proved in 2001 [Hahne et al., SPAA '01] that the Longest Queue Drop policy is 2-competitive and at least $\\sqrt{2}$-competitive. It remained an open question whether a (2-\\epsilon) upper bound for the competitive ratio of this policy could be shown, for any positive constant \\epsilon. We show that the Longest Queue Drop online policy is 1.5-competitive. "
  },
  "5c610851da56297340b34fa7": {
    "code_links": [
      "None"
    ],
    "tasks": [
      "Language Equivalence Problem in MM-QFAs"
    ],
    "datasets": [
      "None"
    ],
    "methods": [
      "Undecidability Proof",
      "Reduction to Emptiness Problem"
    ],
    "results": [
      "Both strict and non-strict language equivalence are undecidable",
      "Undecidability of non-strict and strict emptiness of MM-QFA and EQFA"
    ],
    "paper_id": "5c610851da56297340b34fa7",
    "title": "Undecidability of MM-QFAs Language Equivalence Problem",
    "abstract": "  Let $L_{>\\lambda}(\\mathcal{A})$ and $L_{\\geq\\lambda}(\\mathcal{A})$ be the languages recognized by {\\em measure many 1-way quantum finite automata (MM-QFA)} (or,{\\em enhanced 1-way quantum finite automata(EQFA)}) $\\mathcal{A}$ with strict and non-strict cut-point $\\lambda$, respectively. We consider the language equivalence problem and show the following   1. both strict and non-strict language equivalence are undecidable;   2. we provide an another proof of the undecidability of non-strict and strict emptiness of MM-QFA and EQFA, and then reducing the language equivalence problem to emptiness problem;   3. lastly, we obtain some other properties which can be derived from the above results. "
  },
  "5c6107a8da56297340b1242b": {
    "code_links": "None",
    "tasks": [
      "None"
    ],
    "datasets": [
      "None"
    ],
    "methods": [
      "None"
    ],
    "results": [
      "None"
    ],
    "raw_response": "```json\n{\n    \"code_links\": [\"None\"],\n    \"tasks\": [\"Equivalence of quantum finite automata\", \"Language equivalence of quantum finite automata\", \"Minimization of quantum finite automata\"],\n    \"datasets\": [\"None\"],\n    \"methods\": [\n        \"Equivalence condition for $k_1$-letter and $k_2$-letter quantum finite automata\",\n        \"Equivalence condition for $k_1$-letter and $k_2$-letter measure many quantum finite automata\",\n        \"Undecidability of non-strict cut-point language equivalence for $k$-letter quantum finite automata\",\n        \"Undecidability of strict and non-strict cut-point language equivalence for $k$-letter measure many quantum finite automata\"\n    ],\n    \"results\": [\n        \"Equivalence condition: $(n_1^2+n_2^2-1)|\\Sigma|^{k-1}+k$-equivalence\",\n        \"Undecidability of non-strict cut-point language equivalence for $k$-letter quantum finite automata\",\n        \"Undecidability of strict and non-strict cut-point language equivalence for $k$-letter measure many quantum finite automata\",\n        \"Minimization problem of one-way quantum finite automata remains open\"\n    ]\n}\n```",
    "paper_id": "5c6107a8da56297340b1242b",
    "title": "On equivalence, languages equivalence and minimization of multi-letter\n  and multi-letter measure-many quantum automata",
    "abstract": "  We first show that given a $k_1$-letter quantum finite automata $\\mathcal{A}_1$ and a $k_2$-letter quantum finite automata $\\mathcal{A}_2$ over the same input alphabet $\\Sigma$, they are equivalent if and only if they are $(n_1^2+n_2^2-1)|\\Sigma|^{k-1}+k$-equivalent where $n_1$, $i=1,2$, are the numbers of state in $\\mathcal{A}_i$ respectively, and $k=\\max\\{k_1,k_2\\}$. By applying a method, due to the author, used to deal with the equivalence problem of {\\it measure many one-way quantum finite automata}, we also show that a $k_1$-letter measure many quantum finite automaton $\\mathcal{A}_1$ and a $k_2$-letter measure many quantum finite automaton $\\mathcal{A}_2$ are equivalent if and only if they are $(n_1^2+n_2^2-1)|\\Sigma|^{k-1}+k$-equivalent where $n_i$, $i=1,2$, are the numbers of state in $\\mathcal{A}_i$ respectively, and $k=\\max\\{k_1,k_2\\}$.   Next, we study the language equivalence problem of those two kinds of quantum finite automata. We show that for $k$-letter quantum finite automata, the non-strict cut-point language equivalence problem is undecidable, i.e., it is undecidable whether $L_{\\geq\\lambda}(\\mathcal{A}_1)=L_{\\geq\\lambda}(\\mathcal{A}_2)$ where $0<\\lambda\\leq 1$ and $\\mathcal{A}_i$ are $k_i$-letter quantum finite automata. Further, we show that both strict and non-strict cut-point language equivalence problem for $k$-letter measure many quantum finite automata are undecidable. The direct consequences of the above outcomes are summarized in the paper.   Finally, we comment on existing proofs about the minimization problem of one way quantum finite automata not only because we have been showing great interest in this kind of problem, which is very important in classical automata theory, but also due to that the problem itself, personally, is a challenge. This problem actually remains open. "
  },
  "5c61084eda56297340b34285": {
    "code_links": [
      "http://epetitions.direct.gov.uk",
      "https://petitions.whitehouse.gov"
    ],
    "tasks": [
      "Petition signing dynamics analysis"
    ],
    "datasets": [
      "UK government petitions",
      "US White House petitions"
    ],
    "methods": [
      "Multiplicative process model framework",
      "Hourly resolution data analysis"
    ],
    "results": [
      "Over 99% of petitions fail to get 10,000 signatures",
      "0.1% attain 100,000 signatures for parliamentary debate (0.7% in the US)",
      "Average outreach factor decays to 0.1% after 10 hours in the UK and 30 hours in the US",
      "Petition's fate is virtually set after a day or two"
    ],
    "paper_id": "5c61084eda56297340b34285",
    "title": "Rapid rise and decay in petition signing",
    "abstract": "  Contemporary collective action, much of which involves social media and other Internet-based platforms, leaves a digital imprint which may be harvested to better understand the dynamics of mobilization. Petition signing is an example of collective action which has gained in popularity with rising use of social media and provides such data for the whole population of petition signatories for a given platform. This paper tracks the growth curves of all 20,000 petitions to the UK government petitions website (http://epetitions.direct.gov.uk) and 1,800 petitions to the US White House site (https://petitions.whitehouse.gov), analyzing the rate of growth and outreach mechanism. Previous research has suggested the importance of the first day to the ultimate success of a petition, but has not examined early growth within that day, made possible here through hourly resolution in the data. The analysis shows that the vast majority of petitions do not achieve any measure of success; over 99 percent fail to get the 10,000 signatures required for an official response and only 0.1 percent attain the 100,000 required for a parliamentary debate (0.7 percent in the US). We analyze the data through a multiplicative process model framework to explain the heterogeneous growth of signatures at the population level. We define and measure an average outreach factor for petitions and show that it decays very fast (reducing to 0.1 pervent after 10 hours in the UK and 30 hours in the US). After a day or two, a petition's fate is virtually set. The findings challenge conventional analyses of collective action from economics and political science, where the production function has been assumed to follow an S-shaped curve. "
  },
  "5c6108cfda56297340b51099": {
    "code_links": [
      "None"
    ],
    "tasks": [
      "Formulating guarantees of differential privacy in terms of Bayesian adversary inferences"
    ],
    "datasets": [
      "None"
    ],
    "methods": [
      "Bayesian formulation of differential privacy",
      "(epsilon,delta)-differential privacy analysis"
    ],
    "results": [
      "Formulation satisfied by vanilla and (epsilon,delta)-differential privacy",
      "Provides guidance for setting parameters in (epsilon,delta)-differential privacy"
    ],
    "paper_id": "5c6108cfda56297340b51099",
    "title": "On the `Semantics' of Differential Privacy: A Bayesian Formulation",
    "abstract": "  Differential privacy is a definition of \"privacy'\" for algorithms that analyze and publish information about statistical databases. It is often claimed that differential privacy provides guarantees against adversaries with arbitrary side information. In this paper, we provide a precise formulation of these guarantees in terms of the inferences drawn by a Bayesian adversary. We show that this formulation is satisfied by both \"vanilla\" differential privacy as well as a relaxation known as (epsilon,delta)-differential privacy. Our formulation follows the ideas originally due to Dwork and McSherry [Dwork 2006]. This paper is, to our knowledge, the first place such a formulation appears explicitly. The analysis of the relaxed definition is new to this paper, and provides some concrete guidance for setting parameters when using (epsilon,delta)-differential privacy. "
  },
  "5c6107f4da56297340b1fd02": {
    "code_links": [
      "None"
    ],
    "tasks": [
      "Quantum Data Processing Inequality"
    ],
    "datasets": [
      "None"
    ],
    "methods": [
      "Maximal Correlation Measure",
      "Data Processing Inequality for Maximal Correlation"
    ],
    "results": [
      "Bound on set of states generated under local operations with arbitrary copies of resource state"
    ],
    "paper_id": "5c6107f4da56297340b1fd02",
    "title": "A New Quantum Data Processing Inequality",
    "abstract": "  Quantum data processing inequality bounds the set of bipartite states that can be generated by two far apart parties under local operations; Having access to a bipartite state as a resource, two parties cannot locally transform it to another bipartite state with a mutual information greater than that of the resource state. But due to the additivity of quantum mutual information under tensor product, the data processing inequality gives no bound when the parties are provided with arbitrary number of copies of the resource state. In this paper we introduce a measure of correlation on bipartite quantum states, called maximal correlation, that is not additive and gives the same number when computed for multiple copies. Then by proving a data processing inequality for this measure, we find a bound on the set of states that can be generated under local operations even when an arbitrary number of copies of the resource state is available. "
  },
  "5c6107f0da56297340b1ef03": {
    "code_links": [
      "None"
    ],
    "tasks": [
      "Proving Church's Thesis"
    ],
    "datasets": [
      "None"
    ],
    "methods": [
      "Universal Turing machine with finite tape"
    ],
    "results": [
      "Accomplishment of Post (1936) program"
    ],
    "paper_id": "5c6107f0da56297340b1ef03",
    "title": "Proof of Church's Thesis",
    "abstract": "  We prove that if our calculating capability is that of a universal Turing machine with a finite tape, then Church's thesis is true. This way we accomplish Post (1936) program. "
  },
  "5c61082dda56297340b2c509": {
    "code_links": [
      "None"
    ],
    "tasks": [
      "Empirical evaluation of inference methods for uncertain reasoning"
    ],
    "datasets": [
      "Pathfinder"
    ],
    "methods": [
      "Bayes' theorem",
      "odds-likelihood updating",
      "Dempster-Shafer theory of belief"
    ],
    "results": [
      "Comparison of diagnostic accuracy using expert-rating and decision-theoretic metrics"
    ],
    "paper_id": "5c61082dda56297340b2c509",
    "title": "An Empirical Comparison of Three Inference Methods",
    "abstract": "  In this paper, an empirical evaluation of three inference methods for uncertain reasoning is presented in the context of Pathfinder, a large expert system for the diagnosis of lymph-node pathology. The inference procedures evaluated are (1) Bayes' theorem, assuming evidence is conditionally independent given each hypothesis; (2) odds-likelihood updating, assuming evidence is conditionally independent given each hypothesis and given the negation of each hypothesis; and (3) a inference method related to the Dempster-Shafer theory of belief. Both expert-rating and decision-theoretic metrics are used to compare the diagnostic accuracy of the inference methods. "
  },
  "5c610804da56297340b23f2f": {
    "code_links": [
      "None"
    ],
    "tasks": [
      "AdaBoost Convergence Analysis"
    ],
    "datasets": [
      "high dimensional real-world datasets"
    ],
    "methods": [
      "Optimal AdaBoost as a dynamical system",
      "ergodic theory",
      "constructive proofs of approximations",
      "ergodic dynamical system analysis"
    ],
    "results": [
      "almost universal existence of time averages",
      "convergence of classifier, generalization error, and margins",
      "evidence for AdaBoost cycling and being an ergodic dynamical system",
      "quick stabilization of time averages"
    ],
    "paper_id": "5c610804da56297340b23f2f",
    "title": "On the Convergence Properties of Optimal AdaBoost",
    "abstract": "  AdaBoost is one of the most popular ML algorithms. It is simple to implement and often found very effective by practitioners, while still being mathematically elegant and theoretically sound. AdaBoost's interesting behavior in practice still puzzles the ML community. We address the algorithm's stability and establish multiple convergence properties of \"Optimal AdaBoost,\" a term coined by Rudin, Daubechies, and Schapire in 2004. We prove, in a reasonably strong computational sense, the almost universal existence of time averages, and with that, the convergence of the classifier itself, its generalization error, and its resulting margins, among many other objects, for fixed data sets under arguably reasonable conditions. Specifically, we frame Optimal AdaBoost as a dynamical system and, employing tools from ergodic theory, prove that, under a condition that Optimal AdaBoost does not have ties for best weak classifier eventually, a condition for which we provide empirical evidence from high dimensional real-world datasets, the algorithm's update behaves like a continuous map. We provide constructive proofs of several arbitrarily accurate approximations of Optimal AdaBoost; prove that they exhibit certain cycling behavior in finite time, and that the resulting dynamical system is ergodic; and establish sufficient conditions for the same to hold for the actual Optimal-AdaBoost update. We believe that our results provide reasonably strong evidence for the affirmative answer to two open conjectures, at least from a broad computational-theory perspective: AdaBoost always cycles and is an ergodic dynamical system. We present empirical evidence that cycles are hard to detect while time averages stabilize quickly. Our results ground future convergence-rate analysis and may help optimize generalization ability and alleviate a practitioner's burden of deciding how long to run the algorithm. "
  },
  "5c610864da56297340b3958a": {
    "code_links": [
      "None"
    ],
    "tasks": [
      "Approximate rank-k SVD factorization"
    ],
    "datasets": [
      "None"
    ],
    "methods": [
      "Randomized projection techniques",
      "Parallel solutions",
      "Fast computation around $k \times k$ matrices"
    ],
    "results": [
      "None"
    ],
    "paper_id": "5c610864da56297340b3958a",
    "title": "SVD Factorization for Tall-and-Fat Matrices on Parallel Architectures",
    "abstract": "  We demonstrate an implementation for an approximate rank-k SVD factorization, combining well-known randomized projection techniques with previously known paralel solutions in order to compute steps of the random projection based SVD procedure. We structure the problem in a way that it reduces to fast computation around $k \\times k$ matrices computed on a single machine, greatly easing the computability of the problem. The paper is also a tutorial on paralel linear algebra methods using a plain architecture without burdensome frameworks. "
  }
}