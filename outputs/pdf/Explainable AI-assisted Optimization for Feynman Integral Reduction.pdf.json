{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Optimizing Feynman integral reduction"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "FunSearch (LLMs + Genetic Algorithms)"
  ],
  "results": [
    "Improved memory usage and computational efficiency",
    "Significant reduction in required seeding integrals",
    "Factor of 3058 improvement over traditional methods for certain integrals"
  ],
  "title": "Explainable AI-assisted Optimization for Feynman Integral Reduction.pdf",
  "abstract": "By developing a priority function through the FunSearch algorithm, which com- bines large language models and genetic algorithms, we achieve significant improvements in memory usage and computational efficiency compared to traditional methods. Our approach demonstrates substantial reductions in the required seeding integrals, making previously intractable integrals more manageable. Tested on a variety of Feynman integrals, including one-loop and multi-loop cases with planar and non-planar configurations, our method demonstrates remarkable scalability and adapt- ability. For reductions of certain Feynman integrals with many dots and numerators, we observed an improvement by a factor of 3058 compared to traditional methods. This work provides a powerful and interpretable framework for optimizing IBP reductions, paving the way for more efficient and practical calculations in high-energy physics. I. INTRODUCTION Precision theoretical predictions in particle physics are crucial for rigorously testing the Standard Model and interpreting experimental results, as even minor dis- crepancies between theory and observation can reveal new physics. Due to the asymptotic freedom of Quan- tum Chromodynamics (QCD) and collinear factoriza- tion, theoretical predictions for hadronic observables at high-energy hadron colliders can be formulated within the framework of perturbative theory. In perturba- tive quantum field theory (QFT), two primary meth- ods are commonly used to compute the Feynman inte- grals that contribute to scattering amplitudes or cross- sections: integration-by-parts (IBP) reductions [1, 2] and the differential equation (DE) method [3\u20135]. Notably, the DE method itself relies heavily on IBP reductions, mak- ing IBP reductions a major computational bottleneck in high-precision calculations. Traditionally, most of the IBP reductions were car- ried out using the Laporta algorithm and Laporta seed- ing [6], which, while effective, become increasingly in- efficient as the complexity of the problem grows. In recent years, the finite field methods [7, 8], which avoid intermediate expression swell, have been intro- duced to enhance IBP reductions and improve com- putational feasibility. However, these approaches also encounter challenges when applied to highly intricate problems. Additionally, alternative approaches such as syzygy techniques [9\u201314], intersection number the- ory [15, 16], and block-triangular form improved re- duction [17\u201319] have emerged, offering new perspec- \u2217zhuoyangsong@stu.pku.edu.cn \u2020 tongzhi.yang@physik.uzh.ch \u2021 qinghongcao@pku.edu.cn \u00a7 mingxingluo@csrc.ac.cn \u00b6 zhuhx@pku.edu.cn tives on IBP reductions. Building on these techniques, several public packages have been developed to per- form IBP reductions, including: Air [20], LiteRed [21, 22], FIRE6 [23, 24], Reduze [25, 26], Kira [27\u201329], Forcer [30], FiniteFlow [31], NeatIBP [32], Blade [19], AmpRed [33]. To further enhance the efficiency of IBP reductions, we introduce a novel approach that optimizes the re- duction of Feynman integrals through a priority function developed via the FunSearch algorithm [34]. FunSearch combines large language models (LLMs) with genetic al- gorithms to evolve and refine priority functions that can minimize IBP system size and improve computational ef- ficiency. By leveraging the scalability and interpretabil- ity of LLMs, this approach provides heuristic solutions that can be extended to more complex problems, offer- ing a new avenue for advancing precision calculations in particle physics. Our work focuses on optimizing the IBP reductions process by identifying an optimal subset of seeding integrals that are sufficient to solve the target inte- grals. We demonstrate the effectiveness of our approach through explicit examples involving one-loop and multi- loop Feynman integrals, including both planar and non- planar six-particle phase-space integral families. Our re- sults show that the priority function method achieves substantial improvements in memory usage and compu- tational efficiency compared to traditional methods, en- abling the reduction of previously intractable integrals. In recent years, we have witnessed increasing applica- tions of Artificial Intelligence (AI) in high-energy theo- retical physics, despite the inherent tension between the need for interpretability in theoretical physics and the probabilistic, black-box nature of AI systems. Substan- tial progress has been made, including applications of machine learning to the string-theory landscape [35\u201338], scattering amplitudes [39\u201346], jet physics [47\u201357], Par- ton Distribution Functions [58\u201360], as well as advances in understanding neural networks through the framework arXiv:2502.09544v1 [hep-ph] 13 Feb 2025"
}