{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Offline Reinforcement Learning"
  ],
  "datasets": [
    "D4RL"
  ],
  "methods": [
    "Dual Alignment Maximin Optimization (DAMO)"
  ],
  "results": [
    "Outperforms existing methods on D4RL benchmark",
    "Achieves competitive performance on high-quality datasets"
  ],
  "title": "Dual Alignment Maximin Optimization for Offline Model-based RL.pdf",
  "abstract": "Offline reinforcement learning agents face signifi- cant deployment challenges due to the synthetic- to-real distribution mismatch. While most prior research has focused on improving the fidelity of synthetic sampling and incorporating off-policy mechanisms, the directly integrated paradigm often fails to ensure consistent policy behavior in biased models and underlying environmen- tal dynamics, which inherently arise from dis- crepancies between behavior and learning poli- cies. In this paper, we first shift the focus from model reliability to policy discrepancies while optimizing for expected returns, and then self- consistently incorporate synthetic data, deriving a novel actor-critic paradigm, Dual Alignment Maximin Optimization (DAMO). It is a unified framework to ensure both model-environment pol- icy consistency and synthetic and offline data com- patibility. The inner minimization performs dual conservative value estimation, aligning policies and trajectories to avoid out-of-distribution states and actions, while the outer maximization en- sures that policy improvements remain consis- tent with inner value estimates. Empirical evalua- tions demonstrate that DAMO effectively ensures model and policy alignments, achieving competi- tive performance across diverse benchmark tasks. 1. Introduction Offline reinforcement learning (RL) (Lange et al., 2012; Levine et al., 2020) aims to learn policies from a pre- collected dataset generated by a behavioral policy within the real environment. This paradigm helps avoid the safety risks and high costs associated with direct interactions, mak- ing RL applicable in real-life scenarios, such as healthcare 1School of Mathematical Sciences, University of Chinese Academy of Sciences, Beijing, China 2JD, Beijing, China. Cor- respondence to: Firstname1 Lastname1 <first1.last1@xxx.edu>, Firstname2 Lastname2 <first2.last2@www.uk>. Proceedings of the 41 st International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s). decision-making support and autonomous driving (Emerson et al., 2023; Sinha et al., 2022; Mnih et al., 2015). Offline model-based RL (Yu et al., 2020; Kidambi et al., 2020; Sun et al., 2023) improves upon this by training a dynamics model and using it to generate synthetic data for policy training or planning. The introduction of dynamics models enhances the sample efficiency and allows the agent to an- swer counterfactual queries (Levine et al., 2020). However, despite strong performance in learned models, policies often degrade significantly when deployed in real environments due to mismatches between synthetic and real distributions. Most prior works have focused on enhancing the reliabil- ity of synthetic samplings from learned models, and di- rectly incorporating off-policy optimization methods, such as SAC (Haarnoja et al., 2018), to address this mismatch. The representative approach is MOPO (Yu et al., 2020), which quantifies model uncertainty by measuring the pre- dicted variance of learned models and subsequently pe- nalizes model-generated trajectories with high uncertainty. This methodology has inspired subsequent research like MOReL (Kidambi et al., 2020) and MOBILE (Sun et al., 2023), which adopt similar uncertainty-aware frameworks. These approaches help generate synthetic data that remains compatible with the offline dataset and construct conserva- tive value functions to mitigate out-of-distribution (OOD) actions. However, creating a perfect learned model from limited offline datasets is impossible, and as illustrated in Fig. 1, simply combining these methods with off-policy mechanisms still fails to resolve the inherent discrepancies in policy behaviors between the learned model and the un- derlying environment, leading to inconsistent policy perfor- mance and leaving the distribution shift problem unsolved. In this paper, we trace the inherent root of distribution shift challenges in offline RL to the discrepancies between behav- ior and learning policies in the underlying environmental dynamics. While optimizing for expected returns, we intro- duce a regularized policy optimization objective for offline RL, that constrains the visitation distribution discrepancies between the behavioral and the learning policies in real environments. We further self-consistently incorporate syn- thetic data into this objective, deriving a novel maximin optimization objective for offline model-based RL. Building on these objectives, we propose Dual Alignment Maximin Optimization (DAMO), a consistent actor-critic framework 1 arXiv:2502.00850v1 [cs.LG] 2 Feb 2025"
}