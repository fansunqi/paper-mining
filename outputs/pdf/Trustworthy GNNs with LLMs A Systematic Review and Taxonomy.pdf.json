{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Trustworthy GNNs"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "LLM-GNN integration",
    "Graph Neural Networks (GNNs)",
    "Large Language Models (LLMs)",
    "Reliability",
    "Robustness",
    "Privacy",
    "Reasoning"
  ],
  "results": [
    "None"
  ],
  "title": "Trustworthy GNNs with LLMs A Systematic Review and Taxonomy.pdf",
  "abstract": "With the extensive application of Graph Neural Networks (GNNs) across various domains, their trustworthiness has emerged as a focal point of research. Some existing studies have shown that the integration of large language models (LLMs) can improve the semantic understanding and gen- eration capabilities of GNNs, which in turn im- proves the trustworthiness of GNNs from various aspects. Our review introduces a taxonomy that offers researchers a clear framework for compre- hending the principles and applications of different methods and helps clarify the connections and dif- ferences among various approaches. Then we sys- tematically survey representative approaches along the four categories of our taxonomy. Through our taxonomy, researchers can understand the appli- cable scenarios, potential advantages, and limita- tions of each approach for the the trusted integra- tion of GNNs with LLMs. Finally, we present some promising directions of work and future trends for the integration of LLMs and GNNs to improve model trustworthiness. 1 Introduction Graphs are data structures that are widely used in a vari- ety of real-world scenarios [Xia et al., 2021]. Graph Neural Networks (GNNs) [Wu et al., 2020] have achieved remark- able success in many fields due to their powerful modeling ability for graph-structured data, such as autonomous driv- ing [Xiao et al., 2023], recommendation systems [Zhang et al., 2023a], and crop breeding [Pan et al., 2024b] . With the deployment of GNNs in this highly sensitive fields, the trust- worthiness of GNNs decisions has become a key bottleneck. GNNs are increasingly expected to be reliable, robust, and privacy-preserving to gain trust. In recent years, significant progress has been made in large language models (LLMs) such as GPT [Brown et al., 2020] and DeepSeek [Guo et al., 2025]. These variants have shown superior performance in many natural language processing \u2217Equal Contribution \u2020Corresponding author (zhangzeyu@mail.hzau.edu.cn) Figure 1: Applications of the integration of graphs and LLMs have driven the increased demand for model trustworthiness. tasks, such as sentiment analysis, machine translation, and text classification [Zhao et al., 2023]. Beyond traditional NLP applications, there is growing interest in using LLMs to process various data modalities, such as text-attributed graphs (TAGs) [Yang et al., 2021]. Recent studies have shown that integrating LLMs into GNNs can substantially enhance node representations and improve model performance [He et al., 2023; Chen et al., 2024]. This naturally raises an important question: Can the integration of LLMs and GNNs also enhance the trustwor- thiness of graph-based models? More specifically, how can LLMs and GNNs be effectively combined to improve model trustworthiness? LLMs help in trusted GNN-related tasks. LLMs have significantly transformed how we interact with graph data, particularly in scenarios where nodes contain rich textual at- tributes. The integration of LLMs and graphs has been shown to be successful in a variety of graph-related tasks [Li et al., 2023]. As illustrated in Figure 1, the growing adoption of this integration has led to increasing demands for trust- worthy models. Numerous studies have shown that LLM- augmented GNNs can enhance trustworthiness. For example, LLM4RGNN [Zhang et al., 2024c] leverages the inferential capabilities of LLMs to identify malicious edges and recover missing important. The integration of graphs and LLMS has a significant im- pact on the trustworthiness of graph-related tasks from dif- ferent perspectives. To provide a systematic overview, as de- arXiv:2502.08353v1 [cs.LG] 12 Feb 2025"
}