{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Signature Reconstruction from Randomized Signatures"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Controlled ordinary differential equations",
    "Taylor expansions",
    "Randomized signatures",
    "Linear independence",
    "Signature reconstruction"
  ],
  "results": [
    "The number of signature features, which can be reconstructed from the non-linear flow of the controlled ordinary differential equation, is exponential in its hidden dimension",
    "A general linear independence condition on arbitrary vector fields, under which the signature features up to some fixed order can always be reconstructed"
  ],
  "title": "Signature Reconstruction from Randomized Signatures.pdf",
  "abstract": "Controlled ordinary differential equations driven by continuous bounded variation curves can be considered a continuous time analogue of recurrent neural networks for the construction of expressive features of the input curves. We ask up to which extent well known signature features of such curves can be reconstructed from con- trolled ordinary differential equations with (untrained) random vector fields. The answer turns out to be algebraically involved, but essentially the number of signature features, which can be reconstructed from the non-linear flow of the controlled ordi- nary differential equation, is exponential in its hidden dimension, when the vector fields are chosen to be neural with depth two. Moreover, we characterize a general linear independence condition on arbitrary vector fields, under which the signature features up to some fixed order can always be reconstructed. Algebraically speak- ing this complements in a quantitative manner several well known results from the theory of Lie algebras of vector fields and puts them in a context of machine learning. 1 arXiv:2502.03163v1 [math.CA] 5 Feb 2025"
}