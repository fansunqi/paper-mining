{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Large Language Model Training"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Synthetic data generation",
    "Weak labeling",
    "Boosting-style algorithm"
  ],
  "results": [
    "Improved performance with minimal curation",
    "Arbitrarily high accuracy results",
    "Convergence to optimal LLM",
    "Reduced model collapse"
  ],
  "title": "Escaping Collapse the Strength of Weak Data for Large Language Model Training.pdf",
  "abstract": "Synthetically-generated data plays an increasingly larger role in training large language models. However, while synthetic data has been found to be useful, studies have also shown that without proper curation it can cause LLM performance to plateau, or even \u201ccollapse\u201d, after many training iterations. In this paper, we formalize this question and develop a theoretical framework to investigate how much curation is needed in order to ensure that LLM performance continually improves. We find that the requirements are nearly minimal. We describe a training procedure that converges to an optimal LLM even if almost all of the non-synthetic training data is of poor quality. Our analysis is inspired by boosting, a classic machine learning technique that leverages a very weak learning algorithm to produce an arbitrarily good classifier. Our training procedure subsumes many recently proposed methods for training LLMs on synthetic data, and thus our analysis sheds light on why they are successful, and also suggests opportunities for future improvement. We present experiments that validate our theory, and show that dynamically focusing labeling resources on the most challenging examples \u2014 in much the same way that boosting focuses the efforts of the weak learner \u2014 leads to improved performance. 1 Introduction Large Language Models (LLMs) represent the frontier of artificial intelligence, and are trained on vast amounts of human-generated data. However, much of the high-quality publicly available data on the Internet has been exhausted, and limits on generating new tokens threaten to slow progress on LLM training. As a consequence, synthetically-generated datasets are playing an important role in the training of LLMs. Synthetic data have been shown to improve the performance of real large models on a range of tasks [Bai et al., 2022, Zelikman et al., 2022, Gulcehre et al., 2023, Singh et al., 2024]. On the other hand, the circuitous nature of training new LLMs on data generated by previous generations of LLMs has caused concerns of model collapse [Shumailov et al., 2024, Alemohammad et al., 2024]. Since publicly available sources contain an increasingly large proportion of machine-generated content, synthetic data will be used for training, deliberately or inadvertently. \u2217Authors ordered alphabetically. Author contributions are listed at the end. 1 arXiv:2502.08924v1 [cs.LG] 13 Feb 2025"
}