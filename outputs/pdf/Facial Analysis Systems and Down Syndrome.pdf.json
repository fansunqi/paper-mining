{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Face recognition",
    "Age prediction",
    "Image labelling"
  ],
  "datasets": [
    "UTKFace",
    "Google searches",
    "iStock",
    "Pexels"
  ],
  "methods": [
    "ClarifAI",
    "AWS Rekognition"
  ],
  "results": [
    "EG male: 85% accuracy, EG female: 97% accuracy",
    "EG male: 66% correct age prediction, EG female: 90% correct age prediction",
    "EG: higher occurrences of aesthetic labels, CG: higher occurrences of education labels"
  ],
  "title": "Facial Analysis Systems and Down Syndrome.pdf",
  "abstract": ". The ethical, social and legal issues surrounding facial anal- ysis technologies have been widely debated in recent years. Key critics have argued that these technologies can perpetuate bias and discrimi- nation, particularly against marginalized groups. We contribute to this field of research by reporting on the limitations of facial analysis sys- tems with the faces of people with Down syndrome: this particularly vulnerable group has received very little attention in the literature so far. This study involved the creation of a specific dataset of face images. An experimental group with faces of people with Down syndrome, and a control group with faces of people who are not affected by the syndrome. Two commercial tools were tested on the dataset, along three tasks: gender recognition, age prediction and face labelling. The results show an overall lower accuracy of prediction in the experi- mental group, and other specific patterns of performance differences: i) high error rates in gender recognition in the category of males with Down syndrome; ii) adults with Down syndrome were more often incorrectly labelled as children; iii) social stereotypes are propagated in both the control and experimental groups, with labels related to aesthetics more often associated with women, and labels related to education level and skills more often associated with men. These results, although limited in scope, shed new light on the biases that alter face classification when applied to faces of people with Down syn- drome. They confirm the structural limitation of the technology, which is inherently dependent on the datasets used to train the models. Keywords: Datasets \u00b7 Face recognition \u00b7 Face attribute estimation \u00b7 Gender recognition \u00b7 Age estimation \u00b7 Image labelling \u00b7 AI and disability \u00b7 Down syndrome \u00b7 AI bias. 1 Introduction and motivation This preprint has not undergone peer review (when applicable) or any post- submission improvements or corrections. The Version of Record of this con- tribution is published in \u201cMachine Learning and Principles and Practice of Knowledge Discovery in Databases. ECML PKDD 2023. Communications in Computer and Information Science, vol 2133. Springer, Cham.\u201d, and is avail- able online at https://doi.org/10.1007/978-3-031-74630-7 10 In recent years, the ethical, social and legal implications of Facial Analysis Systems (FASs) arose in several parts of the world. Several municipalities and governments banned the use of facial recognition technologies in public spaces, such as the city of San Francisco [17]. In Italy, a moratorium [14] suspended the arXiv:2502.06341v1 [cs.CV] 10 Feb 2025"
}