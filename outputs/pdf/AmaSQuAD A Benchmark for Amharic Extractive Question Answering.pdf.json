{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Extractive Question Answering"
  ],
  "datasets": [
    "AmaSQuAD",
    "SQuAD 2.0",
    "AmQA"
  ],
  "methods": [
    "Translation-based data generation framework",
    "Fine-tuning of XLM-R model",
    "Cosine similarity",
    "Longest Common Subsequence (LCS)"
  ],
  "results": [
    "F1 score: 44.41% - 57.55% (AmaSQuAD)",
    "F1 score: 67.80% - 68.80% (AmQA)"
  ],
  "title": "AmaSQuAD A Benchmark for Amharic Extractive Question Answering.pdf",
  "abstract": "\u2014This research presents a novel framework for trans- lating extractive question-answering datasets into low-resource languages, as demonstrated by the creation of the AmaSQuAD dataset, a translation of SQuAD 2.0 into Amharic. The method- ology addresses challenges related to misalignment between translated questions and answers, as well as the presence of multiple answer instances in the translated context. For this purpose, we used cosine similarity utilizing embeddings from a fine-tuned BERT-based model for Amharic and Longest Common Subsequence (LCS). Additionally, we fine-tune the XLM-R model on the AmaSQuAD synthetic dataset for Amharic Question- Answering. The results show an improvement in baseline per- formance, with the fine-tuned model achieving an increase in the F1 score from 36.55% to 44.41% and 50.01% to 57.5% on the AmaSQuAD development dataset. Moreover, the model demon- strates improvement on the human-curated AmQA dataset, increasing the F1 score from 67.80% to 68.80% and the exact match score from 52.50% to 52.66%.The AmaSQuAD dataset is publicly available Datasets1. Index Terms\u2014Question Answering, AmaSQUAD, BERT, XLM-R, Longest Common Subsequence, Machine Translation I. INTRODUCTION The latest census results have shown that Amharic is spoken by approximately 57.5 million people in Ethiopia of which 25.1 million individuals within the country have adopted it as a second language [1]. In addition, next to Arabic, Amharic is the second most widespread Semitic language [1]. Despite being the second most widespread Semitic language, Amharic lacks substantial Natural Language Processing (NLP) resources and tools [2]. This shortage of resources has hin- dered the development of robust systems that could greatly benefit various natural language tasks. Question answering (QA) is a Natural Language Processing task that aims to accurately provide answers to natural lan- guage questions [3]. Unlike search engines, which present a list of relevant documents for a question, QA systems provide a definite answer, enhancing user experience. Search engines such as Google and Bing incorporate a QA system in their systems to respond precisely to questions [3]. QA could be categorized into two tasks - Open-domain QA (OpenQA) and Machine Reading Comprehension (MRC) based on the contextual information given to the system. OpenQA is an approach that leverages large unstructured text data to provide 1https://huggingface.co/datasets/nebhailema/AmaSquad answers to questions in natural language [3]. The traditional question-answering pipeline in the OpenQA approach involves processes such as Document Retrieval and Answer Extraction with a component known as a Reader [3]. During document retrieval, the goal is to identify relevant passages and doc- uments that may contain answers to the question, employ- ing techniques such as TF-IDF, BM25, or search engines. Following this, the answer is extracted from the identified and pertinent documents. In the case of MRC, the aim is to read a document and extract the answer without having to do document retrieval. Similar to OpenQA, this is achieved by using the Reader component. The Reader component of a QA system can be either extractive or generative readers [3]. Extractive readers ex- tract answers by predicting a span from retrieved documents, while generative readers utilize sequence-to-sequence models to produce answers in natural language. Moreover, OpenQA systems have recently adopted modern architecture by incorpo- rating neural Machine Reading Comprehension. A substantial amount of high-quality data has been generated to train OpenQA systems, including datasets like MS MARCO [4], CNN/Daily Mail [5], and SQuAD [6]. In contrast to the abundance of high-quality datasets avail- able for widely spoken languages, the Amharic language faces a dataset scarcity. The largest Amharic dataset collected by [7] includes only 2,628 sets of questions and answers . Consequently, to our knowledge, no expansive extractive QA dataset is currently available for the Amharic language. Without a sufficiently large and diverse dataset, it becomes challenging to train models capable of providing accurate responses, thus further impeding progress in the development of Amharic QA system development. In response to the scarcity of datasets for training Amharic Question Answering Models, we build a translation-based data generation framework valuable for extractive QA. Recognizing the limited availability of comprehensive datasets in Amharic, the study employs Google Translate to transform the widely used SQuAD 2.0 training and development dataset into an Amharic dataset named AmaSQuAD. In addition to creating the AmaSQuAD dataset, we aim to leverage this dataset, which is specifically tailored for the extractive approach to train an MRC-based Amharic question-answering model. We aim to accomplish this by fine-tuning the XLM-R model. Furthermore, we anticipate that the developed framework can arXiv:2502.02047v1 [cs.CL] 4 Feb 2025"
}