{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Code Generation with LLM",
    "Plagiarism in AI-generated Code"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "JPlag",
    "TheFuzz",
    "Wilcoxon signed-rank test"
  ],
  "results": [
    "3.35% of the cases in the worst case",
    "Significant increase in the number of methods across all ranges",
    "High-temperature values reduce the likelihood of receiving recommendations with copyleft code",
    "Temperature 2 promotes the generation of original code",
    "Explicit instruction did not lead to a substantial decrease in similarity"
  ],
  "title": "On the Possibility of Breaking Copyleft Licenses when Reusing Code Generated by ChatGPT.pdf",
  "abstract": "\u2014AI assistants can help developers by recommending code to be included in their implementations (e.g., suggesting the implementation of a method from its signature). Although useful, these recommendations may mirror copyleft code available in public repositories, exposing developers to the risk of reusing code that they are allowed to reuse only under certain constraints (e.g., a specific license for the derivative software). This paper presents a large-scale study about the frequency and magnitude of this phenomenon in ChatGPT. In particular, we generate more than 70,000 method implementations using a range of configurations and prompts, revealing that a larger context increases the likelihood of reproducing copyleft code, but higher temperature settings can mitigate this issue. Index Terms\u2014AI-assisted coding, code generation, copyleft licenses, intellectual property I. INTRODUCTION Several AI assistants, such as ChatGPT1, GitHub Copilot2, and Google Gemini3, are available to help developers complete their development tasks. Multiple studies reveal that, although far from perfect, these tools may produce useful results with the potential of accelerating code development [1]\u2013[5]. AI assistants exploit LLMs trained on a huge corpus of data, including code, to provide recommendations. This raises concerns in terms of the ownership and rights to reuse the code generated by these tools when the recommended code matches the code in the training set. This issue is particularly severe if we consider that training code could be protected by restrictive licenses that forbid or limit reuse4. That is, a developer may inadvertently break the license terms associated with the reused code by simply accepting a recommendation. A key question about the recommendations produced by AI assistants is thus: \u201cIs the recommended code original, or is it a copy of existing code protected by a restrictive license?\u201d Answering this question can be extremely important to prevent potentially unethical behavior and possible legal issues. Previous studies based on the now-obsolete GPT-2 models suggest that LLMs can memorize and reproduce code present in their training data. In particular, Al-Kaswan et al. [6] re- ported that LLMs may generate completions for code prefixes 1https://openai.com/chatgpt/ 2https://github.com/features/copilot 3https://gemini.google.com/ 4For instance, open source code protected by copyleft licenses can be reused only under specific constraints on how the derivative code is licensed. encountered during training by appending the corresponding code suffix from the training data. Similarly, Yang et al. [7] found that training code can be observed in the output for a range of diverse prompts. A more recent study [8] investigated the concern of code memorization in GPT-4 models, considering code protected by restrictive licenses. The investigation reported a high probabil- ity (more than 50%) for models to return code protected by copyleft licenses if queried for the generation of a function\u2019s implementation whose signature is present in the training set. This study aims to broaden existing evidence and derive further insights about the risks related to the reuse of the code originated by GPT models. In particular, we investigate how GPT-4 performs in the generation of Java methods, studying the similarity between the recommended code and the corresponding open source code available under restrictive licenses using the JPlag plagiarism detection tool5. Further, our investigation considers multiple dimensions not studied in the context of GPT-4 models, so far. First, we study if and to what extent the context may influence the generation of code protected by restrictive li- censes. In particular, we investigate cases in which devel- opers, having already accepted recommendations containing licensed code, may face an increased likelihood of receiving additional recommendations of code protected by restrictive licenses. We specifically consider the case of classes that already contain code that mirrors open-source code protected by copyleft licenses, and study its impact on recommendations. We investigate two paradigmatic cases: one considering the presence of all the methods in the class but the one that has to be generated matching an open-source implementation protected by a copyleft license, and another one only consid- ering the presence of access methods matching an open-source implementation protected by a copyleft license. The first case captures the situation where the context is the largest possible, encompassing every other method within the class. Instead, the second case investigates to what extent the presence of simple and commonly implemented methods with minimal semantic content (i.e., the access methods) may influence the generation of licensed code. 5https://helmholtz.software/software/jplag arXiv:2502.05023v1 [cs.SE] 7 Feb 2025"
}