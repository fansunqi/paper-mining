{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Fine-tuning Large Language Models"
  ],
  "datasets": [
    "GSM8K",
    "MATH",
    "ECQA",
    "BoolQ",
    "SQuAD",
    "MMLU",
    "MMLU_PRO",
    "AGIEval",
    "MBPP",
    "API-BANK",
    "DROP",
    "ARC Challenge"
  ],
  "methods": [
    "Alignment-based metric",
    "Perplexity",
    "Semantic similarity",
    "Combined criterion"
  ],
  "results": [
    "Average accuracy improvement of 0.64% over the best data generation method",
    "Spearman correlation of 0.2408 compared to other methods"
  ],
  "title": "Efficient Response Generation Method Selection for Fine-Tuning Large Language Models.pdf",
  "abstract": "The training data for fine-tuning large language models (LLMs) is typically structured as input- output pairs. However, for many tasks, there can be multiple equally valid output variations for the same input. Recent studies have ob- served that the choice of output variation used in training can affect the model\u2019s performance. This raises an important question: how can we generate the most effective output from the many possible response generation strategy op- tions? Rather than relying on the traditional but resource-intensive train-and-evaluate approach, this paper proposes a scalable, approximate method for estimating the quality of a small subset of generated training data derived from the same input. We then evaluate how well this small subset of generated output fits the target model we are trying to train. We present a large- scale benchmark covering diverse reasoning- based datasets to support our study. The central idea is that a good output should closely resemble the output generated by the target LLM. We formalize this \u2019closeness\u2019 as the expected alignment score between a candi- date output and the output sampled from the target LLM. We connect this measurement to the perplexity metric used in previous literature and demonstrate that leveraging an alignment- based metric can provide better predictions of model performance. Using this strategy, we can evaluate a small subset of the generated output from each response generation strategy option, then select the most effective strategy. We show that an LLM trained on data gener- ated by the selected strategy could lead to a significant performance gain in many cases. 1 Introduction When instruction-tuning an LLM, training data con- sists of question-response pairs, where multiple *Equal contribution. \u2020Corresponding author. valid responses can be generated for the same in- put. Previous studies (Ren et al., 2024) show that datasets with identical input questions but differ- ent responses can lead to varied learning outcomes, even when responses contain similar levels of detail. This raises a key question: how can we construct re- sponses that are most effective for the target LLM? Prior research has explored improving responses by adding details or rationales, such as structur- ing ground truth step by step (Hsieh et al., 2023; Ranaldi and Freitas, 2024), incorporating ratio- nales, or enriching responses with additional in- formation (Zhang et al., 2024; Kang et al., 2023; Li et al., 2022).However, recent studies (Ren et al., 2024; Yang et al., 2024) suggest that more details or converting responses to step by step style do not always improve performance and that alignment with the LLM\u2019s linguistic style is crucial. From our experiment, we observe that, no sin- gle response generation strategy works universally across tasks. Thus, we need to creating a method to find out the most effective way to generate response for each task, rather than use a single method for all of the tasks. Some concurrent works (Xu et al., 2024; Kim et al., 2024) attempt to predict the effectiveness of response generation methods by evaluating the entire training dataset. They generate full train- ing datasets using each method and then estimate training effectiveness based on scores computed via algorithms or reward models. However, these approaches are computationally expensive and not scalable. However, can we predict the effectiveness of each data generation methods efficiently? We observe an interesting phenomenon that each re- sponse generation method produces responses with a consistent style, meaning that a small subset of generated examples can effectively represent the entire dataset. Based on this assumption, we pro- pose an efficient ranking pipeline that evaluates a 1 arXiv:2502.11779v1 [cs.CL] 17 Feb 2025"
}