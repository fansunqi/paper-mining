{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Neural Network Pruning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "CoNNect Regularizer"
  ],
  "results": [
    "Improved pruning strategies",
    "Enhanced one-shot pruners"
  ],
  "title": "CoNNect A Swiss-Army-Knife Regularizer for Pruning of Neural Networks.pdf",
  "abstract": "Pruning encompasses a range of techniques aimed at increasing the sparsity of neural networks (NNs). These techniques can generally be framed as minimizing a loss function subject to an L0- norm constraint. This paper introduces CoNNect, a novel differentiable regularizer for sparse NN training that ensures connectivity between input and output layers. CoNNect integrates with estab- lished pruning strategies and supports both struc- tured and unstructured pruning. We proof that CoNNect approximates L0-regularization, guar- anteeing maximally connected network structures while avoiding issues like layer collapse. Numer- ical experiments demonstrate that CoNNect im- proves classical pruning strategies and enhances state-of-the-art one-shot pruners, such as Dep- Graph and LLM-pruner. 1. Introduction Machine learning models, such as neural networks (NNs), have seen rapid growth in recent years, leading to significant increases in their footprint (Patterson et al., 2021). With the increasing footprint of these models, there is a growing need to develop more energy-efficient approaches to machine learning that can balance computational performance with environmental sustainability. An effective technique for reducing a model\u2019s computa- tional effort and memory burden is neural network pruning. Pruning refers to the process of systematically eliminating parameters that contribute little to network performance, effectively simplifying the model. The resulting sparse NNs have attracted significant interest in recent years due to their ability to boost computational efficiency and minimize memory consumption while preserving or even improving 1Department of Operations Analytics, VU Amsterdam, Amsterdam, The Netherlands 2Guanghua School of Man- agement, Peking University, Beijing, China. Correspon- dence to: Christian Franssen <c.p.c.franssen@vu.nl>, Jinyang Jiang <jinyang.jiang@stu.pku.edu.cn>, Yijie Peng <pengyi- jie@pku.edu.cn>, Bernd Heidergott <b.f.heidergott@vu.nl>. model performance (LeCun et al., 1989; Hassibi et al., 1993; Frankle & Carbin, 2018). To achieve sparsity in neural networks, various techniques have been proposed, such as weight pruning, commonly re- ferred to as unstructured pruning, which involves selectively removing individual weights from the network. Pruning neural network weights based on absolute values is a clas- sic example of unstructured pruning (LeCun et al., 1989; Hassibi et al., 1993; Hagiwara, 1993; Han et al., 2015). Un- structured pruning can lead to highly sparse networks, but often results in irregular memory access patterns, which can be difficult to optimize in hardware implementations. Re- cently, semi-structured pruning has emerged as an approach to balance granularity and efficiency by removing weights within predefined patterns or groups (Frantar & Alistarh, 2023; Sun et al., 2023; Fang et al., 2024). While this is a promising approach for realizing high accuracy at various sparsity ratios, this approach presents nuanced trade-offs in inference speed and therefore the computational efficiency of the model. Finally, structured pruning (e.g., see (Yuan & Lin, 2006; Huang & Wang, 2017; Anwar et al., 2017)) offers a systematic method to remove entire groups or channels of neurons. Techniques like Group Lasso (Yuan & Lin, 2006; Hoefler et al., 2021) and other structured sparsity learning (Wen et al., 2016; Zhuang et al., 2020) fall into this category; see He & Xiao (2023) for a review. Structured pruning is widely used because it is a more hardware-friendly pruning technique. Moreover, the structured removal of parameters generally leads to an almost equal reduction in computa- tional complexity and inference speed, thus immediately improving computational efficiency. We believe that pruning should obey the following two axioms (where we identify a NN with a directed, weighted graph): Axiom 1 (Delete Weights to Improve Computational Effi- ciency). The graph should be \u2019small\u2019: pruning must signif- icantly reduce the number of weights while minimally im- pacting accuracy and maximizing computational efficiency. Axiom 2 (Preserve Neural Network Connectivity). The pruning process must prevent disruptions in the connectivity of the neural network and preserve the flow of information from input to output. The extensive research on pruning neural networks, as more 1 arXiv:2502.00744v1 [cs.LG] 2 Feb 2025"
}