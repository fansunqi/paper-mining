{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Cooperative multi-agent reinforcement learning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Distributed value decomposition networks (DVDN)",
    "Consensus updates",
    "Gradient tracking"
  ],
  "results": [
    "DVDN matches or outperforms VDN in six of ten scenarios.",
    "DVDN (GT) outperforms IQL in five of ten scenarios."
  ],
  "title": "Distributed Value Decomposition Networks with Networked Agents.pdf",
  "abstract": "We investigate the problem of distributed training under partial ob- servability, whereby cooperative multi-agent reinforcement learn- ing agents (MARL) maximize the expected cumulative joint reward. We propose distributed value decomposition networks (DVDN) that generate a joint Q-function that factorizes into agent-wise Q-functions. Whereas the original value decomposition networks rely on centralized training, our approach is suitable for domains where centralized training is not possible and agents must learn by interacting with the physical environment in a decentralized manner while communicating with their peers. DVDN overcomes the need for centralized training by locally estimating the shared objective. We contribute with two innovative algorithms, DVDN and DVDN (GT), for the heterogeneous and homogeneous agents settings respectively. Empirically, both algorithms approximate the performance of value decomposition networks, in spite of the infor- mation loss during communication, as demonstrated in ten MARL tasks in three standard environments. KEYWORDS Artificial Intelligence, Machine Learning, Multi-Agent Systems, Reinforcement Learning, Deep Learning ACM Reference Format: Guilherme S. Varela, Alberto Sardinha, and Francisco S. Melo. 2025. Dis- tributed Value Decomposition Networks with Networked Agents. In ACM Conference, Washington, DC, USA, July 2017, IFAAMAS, 21 pages. 1 INTRODUCTION Cooperative multi-agent reinforcement learning addresses the prob- lem of designing utility-maximizing agents that learn by interacting with a shared environment. Representing utility functions and ap- plying them for decision-making is challenging because of the large combined joint observation and joint action spaces. Value decomposition network (VDN) [30] avoid this combinatorial trap by considering the family of \ud835\udc44-functions that factorize agent-wise. The method offers a viable solution to the scalability of MARL systems under the premise of centralized training with decentral- ized execution, where the outputs of the individual \ud835\udc44-functions are added to form a joint \ud835\udc44-function. However, in many real-world domains, the premise of centralized training is too restrictive. For instance, in reinforcement learning This work is licensed under a Creative Commons Attribution Inter- national 4.0 License. ACM Conference, , July 2017, Washington, DC, USA. \u00a9 2025 Association for Computing Machinery. ...$ACM ISBN 978-x-xxxx-xxxx-x/YY/MM ...$15.00 based distributed1 load balancing [40] intelligent switches act as agents to distribute multiple types of requests to a fleet of servers in a data center. Agents assign the incoming load to the servers, resolving requests at low latencies under quality-of-service con- straints. In this domain, there is no simulator\u2014agents must learn online by observing queues at links and selecting the links to route the requests. In robotic teams, there might be only simulators with a considerable gap between the simulated environment and the real-world. In real-world situations where communication is re- stricted and actions can fail in unpredictable ways, also benefit from this approach. In RoboCup [1] tournament, soccer robots ac- tuate as agents and are endowed with sensors and computation onboard. Although communication has delays and link failures, agents should cooperate as a team to score goals. The straightforward alternative to centralized training is the fully decentralized training approach, employing independent \ud835\udc44- learners (IQL) [31]. As IQL agents are oblivious to the presence of their teammates, they cannot account for the joint action, and from the perspective of any single agent the perceived transition probabilities of the environment are non-stationary. This approach violates the reinforcement learning assumption that the transition probabilities are fixed and unknown. Since individual learners do not communicate, fully decentralized IQL precludes parameter shar- ing, where agents update the same policy parameters. For many MARL tasks, parameter sharing improves sample efficiency but requires that every agent updates the same weights. Hence in a decentralized training setting, every agent would need one-to-one communication to broadcast its weights and experiences to all other agents, before performing the updates locally. We propose a novel algorithm that combines the decentralized training with value decomposition networks\u2019 \ud835\udc44-function decompo- sition. Starting from the loss function used in VDN, in centralized training setting, we show that information back-propagated to agents\u2019 neutral network is the joint temporal difference (JTD). Our algorithm operates in the decentralized training and decentralized execution (DTDE) setting where agents communicate with their closest neighbors to improve their local JTD estimations. And each agent locally minimizes JTD. When agents are homogeneous, i.e., having the same individual observation and action space, we in- centivize them to align their \ud835\udc44-function\u2019s parameters and their gradients; This mechanism called gradient tracking [26] enables agents to minimize a common loss function. To the best of our knowledge, this is the first application of gradient tracking for policy iteration in reinforcement learning. 1We use distributed and decentralized interchangeably. arXiv:2502.07635v1 [cs.LG] 11 Feb 2025"
}