{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Hard Constrained Optimization"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Homeomorphic Polar Learning (HoP)",
    "Neural Networks",
    "Bounded Activation Functions",
    "Homeomorphic Mapping"
  ],
  "results": [
    "Zero violation rate",
    "Superior optimality",
    "Efficient computation"
  ],
  "title": "HoP Homeomorphic Polar Learning for Hard Constrained Optimization.pdf",
  "abstract": "Constrained optimization demands highly effi- cient solvers which promotes the development of learn-to-optimize (L2O) approaches. As a data- driven method, L2O leverages neural networks to efficiently produce approximate solutions. How- ever, a significant challenge remains in ensuring both optimality and feasibility of neural networks\u2019 output. To tackle this issue, we introduce Home- omorphic Polar Learning (HoP) to solve the star- convex hard-constrained optimization by embed- ding homeomorphic mapping in neural networks. The bijective structure enables end-to-end train- ing without extra penalty or correction. For per- formance evaluation, we evaluate HoP\u2019s perfor- mance across a variety of synthetic optimization tasks and real-world applications in wireless com- munications. In all cases, HoP achieves solutions closer to the optimum than existing L2O methods while strictly maintaining feasibility. 1. Introduction Optimization takes a fundamental role in numerous scien- tific and engineering applications (Liu et al., 2024; Abido, 2002; Rockafellar & Uryasev, 2013). However, these algo- rithms often require significant computation time to solve complex problems, particularly when dealing with non- convex or high-dimensional cases. To address these limita- tions, the paradigm of learn-to-optimize (L2O) has emerged as a promising alternative (Hounie et al., 2024; Ding et al., 2024). As a learning-based scheme, L2O takes optimization parameters as the inputs of neural network (NN) which can efficiently obtain approximate solutions from the outputs. *These authors contributed equally to this work. Ke Deng was responsible for the main theoretical framework, part of the exper- iments, and part of the writing. Hanwen Zhang was responsible for the main writing, part of the experiments, and part of the theo- retical framework. 1School of Electrical & Computer Engineering University of Georgia, Athens, USA 2School of Computing, Uni- versity of Georgia, Athens, USA. Correspondence to: Ke Deng <ke.deng@uga.edu>, Hanwen Zhang <hanwen.zhang@uga.edu>, Jin Lu <jin.lu@uga.edu>, Haijian Sun <hsun@uga.edu>. , Input \u021a to Neural Networks \u021b = [\u01dd1, \u01dd2, . . . , \u01dd\u01c8] T Cartesian Coordinates Homeomorphic Mapping Neural Networks [\u0218\u066c, \u01der] T Polar Sphere Vector Homeomorphic Mapping ... ... ... \u01de\u0634,1 \u01de\u0634,2 \u01de\u0634,d \u01der ... Bounded Activation Function Input \u021a Backward Propagation Loss \u01ca\u021a(\u021b) Bounded Activation Function Figure 1. HoP is structured as follows: the gray box defines the problem considered in this paper, where the problem variable is expressed as y, where x denotes problem parameters, and the objective function, respectively. The parameters x are fed into a NN, which contains a bounded activation function and produces a polar sphere vector comprising the direction vector v\u03b8 and the length scale \u00afzr. Using a homeomorphic mapping, the polar sphere vector is then transformed into Cartesian coordinates while strictly adhering to the original constraints. The warm-colored space on the right side represents the polar space corresponding to the polar sphere vector, while the green space is the Euclidean space for Cartesian coordinates. The loss function fx(y) which can be trained end-to-end without requiring additional penalties or corrections. This data-driven approach enhances the speed and reduces the computational cost of achieving satisfactory solutions (Donti et al., 2021; Park & Van Hentenryck, 2023). Despite its advantages, L2O faces significant challenges when applied to constrained optimization. A major issue is the lack of guarantees for NN to ensure that solutions strictly remain within the feasible region defined by the constraints (Donti et al., 2021). Current works have attempted to ad- dress this limitation through various approaches, including supervised learning (Zamzam & Baker, 2020), incorporating constrained violations into the loss function (Zhang et al., 2024; Xu et al., 2018), post-correction methods (Donti et al., 2021), implicit differentiation (Amos & Kolter, 2017), and other techniques (Zhong et al., 2023; Misra et al., 2022). However, these methods often exhibit limitations in optimal solution searching and hard constraints violation control. We propose the homeomorphic polar learning (HoP) to ad- dress the non-convex problem with star-convex hard con- 1 arXiv:2502.00304v1 [cs.LG] 1 Feb 2025"
}