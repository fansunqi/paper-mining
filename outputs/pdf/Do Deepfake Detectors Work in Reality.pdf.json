{
  "code_links": "None",
  "tasks": [
    "Deepfake Detection"
  ],
  "datasets": [
    "Real-World Faceswap Dataset",
    "FF++",
    "CelebDF"
  ],
  "methods": [
    "Super-resolution",
    "Deepfake Detection Models",
    "Self-swap"
  ],
  "results": [
    "Deepfake detectors' accuracy approaches the level of random guessing on real-world data",
    "Performance degradation of deepfake detectors when subjected to super-resolution techniques"
  ],
  "title": "Do Deepfake Detectors Work in Reality.pdf",
  "abstract": "Deepfakes, particularly those involving faceswap-based manipulations, have sparked significant societal concern due to their increasing realism and potential for misuse. Despite rapid advancements in generative models, detection methods have not kept pace, creating a critical gap in defense strategies. This disparity is further amplified by the disconnect between academic research and real- world applications, which often prioritize different objectives and evaluation criteria. In this study, we take a pivotal step toward bridging this gap by presenting a novel observation: the post-processing step of super-resolution, commonly employed in real-world scenarios, substantially undermines the effectiveness of existing deepfake detection methods. To substantiate this claim, we introduce and publish the first real-world faceswap dataset, collected from popular online faceswap platforms. We then qualitatively evaluate the performance of state-of-the-art deepfake detectors on real-world deepfakes, revealing that their accuracy approaches the level of random guessing. Furthermore, we quantitatively demonstrate the significant performance degradation caused by common post- processing techniques. By addressing this overlooked challenge, our study underscores a critical avenue for enhancing the robustness and practical applicability of deepfake detection methods in real-world settings. 1 INTRODUCTION The rise of artificial intelligence has benefited various fields from material design to energy[17, 15, 16, 12]. However, among those achievements, face-swap technology has emerged as a double-edged sword, showcasing remarkable advancements in artificial intelligence while simultaneously posing significant ethical and societal challenges [23]. By seamlessly superimposing one individual\u2019s face onto another\u2019s body in videos, this technology blurs the line between reality and fabrication, undermining the trust that forms the foundation of modern society. The malicious use of deepfake face-swapping to create deceptive media\u2014ranging from non-consensual explicit content [5] to fraudulent political campaigns [14]\u2014has eroded public confidence in the authenticity of digital content. As these forgeries become increasingly indistinguishable from genuine media, they foster skepticism and paranoia, threatening interpersonal relationships, organizational credibility, and democratic processes. Despite the recognized dangers of deepfake face-swap technology, current detection mechanisms face significant limitations, particularly in real-world applications. While many detection algorithms achieve high accuracy in controlled laboratory conditions [24], their performance often degrades when applied to real-world data. Beautification filters and post-processing techniques, commonly applied to media in practical scenarios, exacerbate this challenge by obscuring the subtle artifacts that detection systems rely on. This gap between theoretical robustness and practical reliability \u2020\u2020 Correspondence author. ** Equal contributions, order randomly generated. arXiv:2502.10920v1 [cs.CV] 15 Feb 2025"
}