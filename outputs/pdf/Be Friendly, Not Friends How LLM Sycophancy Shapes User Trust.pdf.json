{
  "code_links": [
    "None"
  ],
  "tasks": [
    "User-LLM agent interaction",
    "LLM sycophancy",
    "User trust"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Between-subjects experiment",
    "Qualtrics survey",
    "Chatbase platform",
    "GPT-4o"
  ],
  "results": [
    "Sycophancy reduces perceived authenticity and trust",
    "Friendliness increases perceived authenticity and trust",
    "Interaction effects between sycophancy and friendliness"
  ],
  "title": "Be Friendly, Not Friends How LLM Sycophancy Shapes User Trust.pdf",
  "abstract": "Recent studies have revealed that large language model (LLM)-powered conversational agents often exhibit \u2018sycophancy\u2019, a tendency to adapt their responses to align with user perspectives, even at the expense of factual accuracy. However, users\u2019 perceptions of LLM sycophancy and its interplay with other anthropomorphic features (e.g., friendliness) in shaping user trust remains understudied. To bridge this gap, we conducted a 2 (Sycophancy: presence vs. absence) \u00d7 2 (Friendliness: high vs. low) between- subjects experiment (N = 224). Our study uncovered, for the first time, the intricate dynamics between LLM sycophancy and friendliness: When an LLM agent already exhibits a friendly demeanor, being sycophantic reduces perceived authenticity, thereby lowering user trust; Conversely, when the agent is less friendly, aligning its responses with user opinions makes it appear more genuine, leading to higher user trust. Our findings entail profound implications for AI persuasion through exploiting human psychological tendencies and highlight the imperative for responsible designs in user-LLM agent interactions. 1 Introduction Eliciting positive feedback and gaining user trust has been one primary goal of developing conversational agents [29]. The HCI community has explored numerous strategies to achieve this, mainly focusing on en- hancing the agent\u2019s human-likeness, including employing human-like physical cues (e.g., profile pictures [29]), psychological cues (e.g., exhibiting personality [85, 31]), dynamic social cues (e.g., reciprocal behavior [79]), and language cues (e.g., using words of praise [26]). Among these strategies, demonstrating \u2018friendliness\u2019 \u2013 incorporating politeness and warm language cues in the agent\u2019s responses \u2013 has proven to be one of the most effective approaches [7, 44, 38]. The advances in large language models (LLMs) have led to the surging popularity of LLM-powered con- versational agents1 (e.g., OpenAI\u2019s ChatGPT, Anthropic\u2019s Claude, and Google\u2019s Gemini). In contrast to their conventional counterparts, due to their unprecedented capabilities of incorporating contextual informa- tion and generating adaptive responses [15], LLM agents not only demonstrate \u2018friendliness\u2019 but also exhibit \u2018sycophancy\u2019, the tendency to adapt and align their responses with users\u2019 perspectives, even if users provide factually incorrect opinions [70, 88, 62]. As an example, Figure 1 illustrates a concrete case: in the initial interaction, the LLM agent presents a balanced perspective on autonomous vehicles, objectively discussing both pros and cons; however, when the user expresses a predominantly negative view, the agent quickly aligns its response with the user\u2019s opinions, emphasizing the concerns and potential risks of this technology. Notably, unlike the flattery [26] or agreeableness features [86, 31, 68] of conventional agents, LLM sycophancy leverages LLMs\u2019 unprecedented capabilities for contextual understanding and response gen- eration [15], differing in three key aspects: i ) adaptive \u2013 the agents dynamically learn and adapt to user perspectives rather than expressing constant agreement; ii ) contextualized \u2013 their responses meaningfully incorporate or complement user viewpoints rather than simply agreeing; and iii ) non-factual \u2013 the responses often align with user preferences at the expense of factual accuracy [62, 88]. For instance, LLM agents would incorrectly admit to mistakes when challenged by users, even when their original responses were ac- 1In the following, we use the term \u201cLLM agent\u201d for short when the context is clear. 1 arXiv:2502.10844v1 [cs.HC] 15 Feb 2025"
}