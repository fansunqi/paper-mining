{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Differentially Private Reinforcement Learning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Trust-Region Policy Optimization (TRPO)",
    "Proximal Policy Optimization (PPO)",
    "Differentially Private Policy Gradient (DPPG)"
  ],
  "results": [
    "None"
  ],
  "title": "Differentially Private Policy Gradient.pdf",
  "abstract": "Motivated by the increasing deployment of rein- forcement learning in the real world, involving a large consumption of personal data, we intro- duce a differentially private (DP) policy gradient algorithm. We show that, in this setting, the intro- duction of Differential Privacy can be reduced to the computation of appropriate trust regions, thus avoiding the sacrifice of theoretical properties of the DP-less methods. Therefore, we show that it is possible to find the right trade-off between privacy noise and trust-region size to obtain a performant differentially private policy gradient algorithm. We then outline its performance empir- ically on various benchmarks. Our results and the complexity of the tasks addressed represent a sig- nificant improvement over existing DP algorithms in online RL. 1. Introduction In the past years, Reinforcement Learning (RL) agents have been increasingly deployed in the real world, especially to provide personalized recommendations, spanning various domains such as healthcare (Liu et al., 2022), recommen- dation engines (Afsar et al., 2023), and finance (Hambly et al., 2021), or to control advanced systems, for instance autonomous vehicles (Wang et al., 2023) and robotics (Tang et al., 2024). RL has also greatly contributed to the huge success of Large Language Models (LLMs), being used to align these models with human preferences (i.e., Rein- forcement Learning from Human Feedback (RLHF), see Kaufmann et al. (2023)) and to enhance their reasoning capabilities (DeepSeek-AI, 2025). To succeed, these RL agents are typically trained on a large amount of personal data. As a consequence, there has been growing concern over privacy leakage from RL agents. Indeed, empirical evidence suggest that RL is no more immune to privacy attacks than 1Noah\u2019s Ark Lab Paris, Huawei Technologies France, Paris 2T\u00b4el\u00b4ecom Paris, France. Correspondence to: Alexandre Rio <alexandre.rio2@huawei.com>. Preliminary work. other areas of ML. In particular, Gomrokchi et al. (2023) exploit temporal data correlation in RL to design powerful membership inference attacks (MIAs) against RL policies in the black-box setting, being able to infer the membership of full trajectories to the training data. Pan et al. (2019) and Prakash et al. (2022) have also designed privacy attacks against the transition dynamics and the reward function of the underlying Markov decision process (MDP). Moreover, several studies have shown evidence that privacy leakage can happen during the fine-tuning training phase of LLMs (Carlini et al., 2021; Fu et al., 2023), suggesting that RLHF on private data may be unsafe. The existence of such pri- vacy threats is a huge issue as the targeted data can be highly sensitive in some scenarios. For instance, in RL-based per- sonalized treatment recommendation, states and rewards can reveal information about a patient\u2019s health history, while systems for online advertising or news recommendation are trained on users\u2019 browsing history that can reveal the users\u2019 preferences on various topics (e.g., politics). Designing privacy-preserving reinforcement learning meth- ods, notably based on differential privacy (DP) \u2014 the cur- rent gold standard in terms of privacy protection \u2014- is therefore of great importance. Yet, private reinforcement learning has received little attention compared to other areas of machine learning. A handful of works have introduced DP RL methods (e.g., Vietri et al. (2020), Chowdhury & Zhou (2021), Qiao & Wang (2023)), but these works remain largely theoretical. Indeed, they are limited to episodic tab- ular and linear MDPs, only evaluated on low-dimensional simulations, and cannot, intrinsically, scale to real-world problems. Crucially, no work has convincingly tackled deep RL problems with differential privacy guarantees, even in relatively low-dimensional settings, and there currently is no DP RL method matching the versatility, scalability and empirical effectiveness of DP-SGD (Abadi et al., 2016) for (deep) supervised learning. We therefore believe that there is a pressing need to steer the DP RL field towards more practical, scalable approaches, which starts by tackling deep RL problems with differential privacy guarantees. We further observe that policy gradient methods (Sutton et al., 1999), although widely popular and achieving state-of-the-art results on various deep RL tasks, have not been studied in the private setting. Yet, directly optimizing the policy through gradient ascent, policy gra- 1 arXiv:2501.19080v1 [cs.LG] 31 Jan 2025"
}