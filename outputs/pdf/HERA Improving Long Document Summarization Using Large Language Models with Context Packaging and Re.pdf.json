{
  "code_links": "None",
  "tasks": [
    "Long document summarization"
  ],
  "datasets": [
    "arXiv",
    "PubMed"
  ],
  "methods": [
    "Context packaging",
    "Reordering"
  ],
  "results": [
    "ROUGE, BERTScore, FactCC, SummaC scores improved"
  ],
  "title": "HERA Improving Long Document Summarization Using Large Language Models with Context Packaging and Re.pdf",
  "abstract": "Despite the rapid growth of context length of large language models (LLMs) , LLMs still per- form poorly in long document summarization. An important reason for this is that relevant information about an event is scattered through- out long documents, and the messy narrative order impairs the accurate understanding and utilization of LLMs for long documents. To ad- dress these issues, we propose a novel summary generation framework, called HERA. Specifi- cally, we first segment a long document by its semantic structure and retrieve text segments about the same event, and finally reorder them to form the input context. We evaluate our approach on two long document summariza- tion datasets. The experimental results show that HERA outperforms foundation models in ROUGE, BERTScore and faithfulness metrics, while HERA does not require additional fine- tuning and resources. 1 Introduction Long document summarization aims to generate fluent, concise and faithful summaries of long texts such as government documents, scientific papers and books. Benefiting from the development of LLMs, long document summarization is no longer a difficult task. Many newly released LLMs, such as Claude 3 and Gemini 1.5, are already able to process millions of tokens at a time. However, the performance of LLMs significantly decreases as the length of context grows (Dong et al., 2024; Hsieh et al., 2024). More seriously, the summaries generated by LLMs often contain a lot of content that is irrelevant to or contradicts the original doc- ument (Li et al., 2023; Tam et al., 2023), which undermines the reliability and usability of the text summarization. Many studies explore the mechanism why the performance of LLMs degrades in long-context sce- *Corresponding Author: Yin Zhang. Article: 1 The 2024 UEFA Champions League final was held at Wembley Stadium in London, England, on 1 June 2024, between German club Borussia Dort- mund and Spanish club Real Madrid. ... 6 For Borussia Dortmund, this was their third UEFA Champions League final appearance, and Real Madrid played in a record-extending 18th Eu- ropean Cup/UEFA Champions League final, and their second in three years. They previously won 14 finals and lost three. ... 8 Real Madrid defeated Dortmund 2-0 to become the team with the most European Cup/UEFA Cham- pions League titles of all time. ... Summaries: On June 1, 2024, Real Madrid defeated Dortmund 2-0 at Wembley Stadium to win a record-extending 15th title. Table 1: The information in the summary is spread out in paragraphs far apart. narios. Liu et al. (2024) observe that LLMs prefer to extract information at the beginning or end of the context and ignore the content in the middle. The Needle-in-a-Haystack (Zhao et al., 2024) shows that LLMs have difficulty finding the required in- formation in massive texts. Wu et al. (2024) and Du et al. (2024) demonstrates that LLMs can be easily distracted by these irrelevant yet misleading contents. In addition, many works (Kumar and Talukdar, 2021; Lu et al., 2022; Wu et al., 2023; Zhang et al., 2023) have revealed that the reading order of LLMs has a significant impact on their understanding and utilization of context. There- fore, as shown in Table 1, the key to improving the performance of LLMs in long document summa- rization is how to extract useful information and arrange it in the correct narrative order. To address these problems, we propose HERA, a long document summary generation framework via context packaging and reordering. The document 1 arXiv:2502.00448v1 [cs.CL] 1 Feb 2025"
}