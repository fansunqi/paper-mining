{
  "code_links": [
    "https://github.com/XXX"
  ],
  "tasks": [
    "Medical Image Classification"
  ],
  "datasets": [
    "LIDC-IDRI",
    "derm7pt"
  ],
  "methods": [
    "Vision Transformer",
    "Hierarchical Prediction",
    "Prototype Learning"
  ],
  "results": [
    "Within-1-Accuracy: 96.3% on LIDC-IDRI",
    "Accuracy: 65.3% on derm7pt"
  ],
  "title": "Hierarchical Vision Transformer with Prototypes for Interpretable Medical Image Classification.pdf",
  "abstract": ". Explainability is a highly demanded requirement for appli- cations in high-risk areas such as medicine. Vision Transformers have mainly been limited to attention extraction to provide insight into the model\u2019s reasoning. Our approach combines the high performance of Vi- sion Transformers with the introduction of new explainability capabil- ities. We present HierViT, a Vision Transformer that is inherently in- terpretable and adapts its reasoning to that of humans. A hierarchical structure is used to process domain-specific features for prediction. It is interpretable by design, as it derives the target output with human- defined features that are visualized by exemplary images (prototypes). By incorporating domain knowledge about these decisive features, the reasoning is semantically similar to human reasoning and therefore in- tuitive. Moreover, attention heatmaps visualize the crucial regions for identifying each feature, thereby providing HierViT with a versatile tool for validating predictions. Evaluated on two medical benchmark datasets, LIDC-IDRI for lung nodule assessment and derm7pt for skin lesion classi- fication, HierViT achieves superior and comparable prediction accuracy, respectively, while offering explanations that align with human reason- ing. Keywords: Explainable AI \u00b7 Hierarchical Prediction \u00b7 Prototype Learn- ing \u00b7 Vision Transformer. 1 Introduction Since their adaptation from NLP to computer vision in 2021, Vision Transform- ers (ViTs) have revolutionized image processing, excelling in areas like image segmentation and self-supervised learning [1,2,3,4,5]. In the field of explainable AI, ViTs inherently provide an initial insight into the model\u2019s logic through atten- tion extraction [1]. However, in high-risk domains like medicine, interpretability arXiv:2502.08997v1 [cs.CV] 13 Feb 2025"
}