{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Multilingual Foundation Model"
  ],
  "datasets": [
    "Indic Language Dataset",
    "Web Data"
  ],
  "methods": [
    "Indic Tokenizer",
    "Grouped Query Attention",
    "AliBi",
    "Continual Pre-training",
    "Instruction Tuning",
    "Direct Preference Optimization"
  ],
  "results": [
    "Indic Language Benchmarks",
    "English Benchmarks",
    "Human Evaluations"
  ],
  "title": "Krutrim LLM Multilingual Foundational Model for over a Billion People.pdf",
  "abstract": "India is one of the most vibrant and culturally diverse societies. Developing a general-purpose artificial intelligence system tailored for the Indian market presents unique challenges. These include accounting for the nation\u2019s cultural nuances, accommodating its linguistic diversity with numerous regional languages, adapting to the prominence of oral traditions, ensuring accessibility to relevant data sets, and achieving scalability to serve the vast population effectively. Careful consideration and innovative approaches are necessary to navigate these complexities successfully. Existing foundation models for natural language tasks are predominantly trained on English data, limiting their effectiveness for languages native to India\u2019s over 1 billion citizens. Thousands of regional languages, dialects, language or code mixing pose representation challenges, exacerbated by sparse training data; Indic languages comprise just 1% of Common Crawl corpora despite India representing 18% of the global population. Consequently, lack of Indic language relevance and context representation leads current models to exhibit cultural and linguistic biases oriented towards Western contexts. We present Krutrim Large Language Model (LLM), a 2 trillion token multilingual foundation model designed to serve Indian demographic needs through equitable representation of the country\u2019s array of native tongues. Training data incorporates the largest known Indic language dataset, mitigating associated data scarcity obstacles that encumber model parity across dialects. Evaluations demonstrate Krutrim\u2019s strong performance on Indic language benchmarks, surpassing or at par with state-of-the-art models despite being significantly smaller in training flops. Krutrim LLM also matches or exceeds standards set on English benchmarks by models trained on comparable flops (e.g. vs LLAMA-2 on 10 out of 16 tasks with average score of 0.57 vs 0.55 of LLAMA- 2), evidencing flexible multilingual fluency. We further integrated search to deliver real-time and more factually relevant information via Krutrim LLM conversational app, working to make next-generation AI widely accessible for a diverse set of over 1 billion worldwide users. Through intentional design choices that redress endemic data imbalances, Krutrim LLM signifies meaningful progress in the pursuit of ethical, globally representative AI foundation models. \u2217Additional Contributors: Sanket Shah, Sulabh Katiyar, Sindhu Pawar, Soham Pendurkar, Pranav Raveendran, Bidyapathi Ray, Daud Ibrahim, Divyansh Rajput, Pidathala Sowjanya, Rahul Kumar, Rishabh Nahata, Pranav Raveendran, Bidyapathi Ray, Prateek Shrivastava, Yogendra Mishra, Azhagiri S, Priyanka Nayak, Sandesh Bafna, Aniruddha Uttam Tammewar, Vivek Dahiya, Ali Faraz, Ayush Tarun, Shaharukh Khan, Debanjana Biswas, Ashish Anand Kulkarni, Rajkiran Panuganti, Hareesh Kumar Acknowledgement: Ravi Jain, Bhavish Aggarwal arXiv:2502.09642v1 [cs.CL] 10 Feb 2025"
}