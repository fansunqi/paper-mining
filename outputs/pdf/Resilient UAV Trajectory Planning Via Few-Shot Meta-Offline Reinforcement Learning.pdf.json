{
  "code_links": [
    "None"
  ],
  "tasks": [
    "UAV trajectory planning",
    "scheduling policy optimization",
    "minimize AoI and transmission power"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Offline RL (CQL)",
    "Meta-learning (MAML)",
    "Deep RL (DQN)"
  ],
  "results": [
    "Converges faster than baseline schemes",
    "Achieves minimum AoI and transmission power",
    "Resilient to network failures"
  ],
  "title": "Resilient UAV Trajectory Planning Via Few-Shot Meta-Offline Reinforcement Learning.pdf",
  "abstract": "\u2014Reinforcement learning (RL) has been a promising essence in future 5G-beyond and 6G systems. Its main advantage lies in its robust model-free decision-making in complex and large-dimension wireless environments. However, most existing RL frameworks rely on online interaction with the environment, which might not be feasible due to safety and cost concerns. Another problem with online RL is the lack of scalability of the designed algorithm with dynamic or new environments. This work proposes a novel, resilient, few-shot meta-offline RL algorithm combining offline RL using conservative Q-learning (CQL) and meta-learning using model-agnostic meta-learning (MAML). The proposed algorithm can train RL models using static offline datasets without any online interaction with the environments. In addition, with the aid of MAML, the proposed model can be scaled up to new unseen environments. We showcase the proposed algorithm for optimizing an unmanned aerial vehicle (UAV) \u2019s trajectory and scheduling policy to minimize the age-of-information (AoI) and transmission power of limited- power devices. Numerical results show that the proposed few- shot meta-offline RL algorithm converges faster than baseline schemes, such as deep Q-networks and CQL. In addition, it is the only algorithm that can achieve optimal joint AoI and transmission power using an offline dataset with few shots of data points and is resilient to network failures due to unprecedented environmental changes. Index Terms\u2014Age-of-information, meta-learning, offline re- inforcement learning, precise agriculture, resilience, unmanned aerial vehicles I. INTRODUCTION A. Context and Motivation Recent progress towards intelligent wireless networks em- braces efficient and fast decision-making algorithms. Machine learning / artificial intelligence (ML/AI) has gained further interest in 5G-beyond and 6G systems due to their powerful decision-making algorithms that can adapt to large and com- plex wireless networks [1], [2]. Reinforcement learning (RL) is one family of ML/AI that is known as the algorithm of decision-making. In RL, an agent observes the environment, makes decisions, and receives an award that evaluates how good the decision is in the current environment observation. A policy in RL describes what decisions to select at each ob- servation. RL aims to find the optimum policy that maximizes the received awards [3]. To this end, RL has shown great promise in a wide range of applications, such as radio resource management (RRM) [4], Eslam Eldeeb and Hirley Alves are with the Centre for Wireless Commu- nications (CWC), University of Oulu, Finland. (e-mail: eslam.eldeeb@oulu.fi; hirley.alves@oulu.fi). This work was supported by 6G Flagship (Grant Number 369116) funded by the Research Council of Finland. network slicing [5], unmanned aerial vehicle (UAV) [6] networks, connected and autonomous vehicle (CAV) net- works [7]. RL\u2019s power relies on RL algorithms\u2019 ability to handle model-free systems, where it is tough to formulate an efficient closed-form model to the system [8]. This applies to 5G-beyond and 6G systems, which are often very complex, have large dimensions, and are full of uncertainties. These characteristics of the wireless systems make RL algorithms fit most of the problems the wireless systems face [9]. In addition, the breakthrough in deep RL enables solving extraordinarily complex and large systems by combining deep neural networks (DNNs) with traditional RL algorithms [10]. One example where RL and deep RL show superior benefits is autonomous UAVs\u2019 trajectory optimization. UAVs provide remarkable flexibility for gathering data from remote sensors and enhance communication by flying closer to the sensors, thereby increasing the likelihood of line-of-sight (LoS) com- munication [11]. Moreover, remote sensors, such as those used in smart agriculture networks, often have limited power supplies and are difficult to access for battery replacement, par- ticularly during adverse weather conditions [12]\u2013[14]. UAVs play a crucial role in conserving sensor power by reducing the distance between them. Scalable RL algorithms can optimize UAV trajectory and scheduling policies, even in dynamic and rapidly changing network environments [15], [16]. Despite its high applicability to the wireless environment, RL and deep RL still face significant difficulties in real- world wireless systems [17]. First, almost all RL and deep RL algorithms designed for wireless communication applications are online. Online RL counts on continuous interaction with the environment to update the learned policies until converging to the optimum policy. However, online interactions might not be feasible in real-world scenarios. For instance, online interactions might be unsafe in some applications, such as UAV and CAV networks, where bad decisions can lead to hazardous consequences. In addition, online interactions might be costly and time-consuming in some applications, such as RRM and network slicing, where the algorithm spends large time intervals through a massive amount of online interaction to reach the optimum policy. Second, RL algorithms are not scalable to multiple problems and dynamic environments. For example, optimizing an RL algorithm in a network with a specific number of devices can not be utilized in another network with a different number of devices [18]. Similarly, changing the characteristics of the environment, such as channel model, number of access points, and environment dimension, requires retraining the RL model from scratch, which wastes time and resources. Therefore, arXiv:2502.01268v1 [cs.RO] 3 Feb 2025"
}