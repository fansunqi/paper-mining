{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Latent Stochastic Differential Equations (SDEs) Training"
  ],
  "datasets": [
    "Synthetic Data",
    "Motion Capture Data"
  ],
  "methods": [
    "SDE Matching"
  ],
  "results": [
    "Faster convergence compared to adjoint sensitivity method",
    "Significant reduction in computational complexity"
  ],
  "title": "SDE Matching Scalable and Simulation-Free Training of Latent Stochastic Differential Equations.pdf",
  "abstract": "The Latent Stochastic Differential Equation (SDE) is a powerful tool for time series and se- quence modeling. However, training Latent SDEs typically relies on adjoint sensitivity methods, which depend on simulation and backpropaga- tion through approximate SDE solutions, which limit scalability. In this work, we propose SDE Matching, a new simulation-free method for train- ing Latent SDEs. Inspired by modern Score- and Flow Matching algorithms for learning generative dynamics, we extend these ideas to the domain of stochastic dynamics for time series and sequence modeling, eliminating the need for costly numer- ical simulations. Our results demonstrate that SDE Matching achieves performance comparable to adjoint sensitivity methods while drastically reducing computational complexity. 1. Introduction Differential equations are a natural choice for modeling continuous-time dynamical systems and have recently re- ceived significant interest in machine learning. Since Chen et al. (2018) proposed the adjoint sensitivity method for learning Ordinary Differential Equations (ODEs) in a memory-efficient manner, ODE-based approaches became popular in deep learning for density estimation (Grathwohl et al., 2019) and to model unevenly observed time series (Yildiz et al., 2019; Rubanova et al., 2019)]. However, ODEs describe deterministic systems and encode all uncertainty into their initial conditions. This limits the applicability of ODE-based approaches when modeling stochastic and chaotic processes. To address this limita- tion, Li et al. (2020) extended the adjoint sensitivity method to Latent Stochastic Differential Equations (SDEs). 1University of Amsterdam 2Constructor University, Bre- men. Correspondence to: Grigory Bartosh <g.bartosh@uva.nl>, Dmitry Vetrov <dvetrov@constructor.university>, Christian A. Naesseth <c.a.naesseth@uva.nl>. Table 1. Asymptotic complexity comparison. L and R are num- ber of sequential and parallel evaluations of drift/diffusion terms, respectively, and D is the number of parameters/states. Method Memory Time Forward Pathwise (Yang & Kushner, 1991) (Gobet & Munos, 2005) O(1) O(LD) Backprop through Solver (Giles & Glasserman, 2006) O(L) O(L) Stochastic Adjoint (Li et al., 2020) O(1) O(L log L) Amortized Reparameterization (Course & Nair, 2023) O(R) O(R) SDE Matching (this paper) O(1) O(1) Despite these advancements, training of ODE and SDE models is simulation-based, it relies on costly numerical integration of differential equations and backpropagation through the solutions. With training algorithms that are difficult to parallelize on modern hardware, Latent SDEs have resisted truly scaling. In parallel, Score Matching (Ho et al., 2020; Song et al., 2021c) and Flow Matching methods (Lipman et al., 2023; Albergo et al., 2023; Liu et al., 2023) have demonstrated that continuous-time dynamics for generative modeling can be learned efficiently in a simulation-free manner\u2014without requiring numerical integration. These techniques have proven computationally efficient and scalable for high- dimensional problems. Inspired by these developments, we develop a simulation-free approach for learning SDEs. We introduce SDE Matching\u2014a simulation-free framework for learning Latent SDE models. The key idea we adopt from matching-based approaches is direct access to latent posterior samples at any time step. This eliminates the need to integrate SDEs during training. The SDE Matching objec- tive is estimated using the Monte Carlo method, achieving O(1) memory and time complexity. We summarize the complexity comparisons in Table 1. 1 arXiv:2502.02472v1 [stat.ML] 4 Feb 2025"
}