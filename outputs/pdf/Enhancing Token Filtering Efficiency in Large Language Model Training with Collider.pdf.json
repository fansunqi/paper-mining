{
  "code_links": [
    "https://github.com/chai321/Collider"
  ],
  "tasks": [
    "LLM training efficiency"
  ],
  "datasets": [
    "open-web-math"
  ],
  "methods": [
    "Token filtering",
    "Sparse GEMM",
    "Dimension-reduced dense GEMM",
    "Automatic graph updating"
  ],
  "results": [
    "Model utility improvement: 16.3%",
    "Training time reduction: 22%",
    "Backpropagation time reduction: 35.1%"
  ],
  "title": "Enhancing Token Filtering Efficiency in Large Language Model Training with Collider.pdf",
  "abstract": "Token filtering has been proposed to enhance utility of large language models (LLMs) by eliminating inconsequential to- kens during training. While using fewer tokens should reduce computational workloads, existing studies have not succeeded in achieving higher efficiency. This is primarily due to the insufficient sparsity caused by filtering tokens only in the output layers, as well as inefficient sparse GEMM (General Matrix Multiplication), even when having sufficient sparsity. This paper presents Collider1, a system unleashing the full efficiency of token filtering in LLM training. At its core, Collider filters activations of inconsequential tokens across all layers to maintain sparsity. Additionally, it features an automatic workflow that transforms sparse GEMM into dimension-reduced dense GEMM for optimized efficiency. Evaluations on three LLMs\u2014TinyLlama-1.1B [38], Qwen2.5- 1.5B [34], and Phi1.5-1.4B [17]\u2014demonstrate that Collider reduces backpropagation time by up to 35.1% and end-to-end training time by up to 22.0% when filtering 40% of tokens. Utility assessments of training TinyLlama on 15B tokens in- dicate that Collider sustains the utility advancements of token filtering by relatively improving model utility by 16.3% com- paring to regular training, and reduces training time from 4.7 days to 3.5 days using 8 GPUs. Collider is designed for easy integration into existing LLM training frameworks, allowing systems already using token filtering to accelerate training with just one line of code. 1 Introduction Training high-quality large language models (LLMs) is no- tably resource-intensive, requiring substantial investments in both data and computational power. For example, the training of foundational models such as LLaMA 3-70B necessitates approximately 7 million GPU hours and over 15 trillion high- quality tokens [9]. Token filtering represents an emerging \u2020Junxue Zhang and Kai Chen are the corresponding authors. 1A collider is a modern system in physics used for accelerating and colliding particles to uncover deeper insights into the fundamental nature of matter. paradigm aimed at enhancing the cost-efficiency of LLM training by systematically discarding less significant tokens early in the training process2. This methodology enables the model to concentrate on the most pertinent tokens, resulting in up to 30% absolute improvement in model utility across various tasks [20]. While the effectiveness of token filtering in enhancing model utility is well recognized within the AI community3, its potential to improve computational efficiency in train- ing remains largely unexplored. In principle, by significantly reducing the number of tokens processed in the computa- tional pipeline, token filtering should decrease computational demands and expedite training. However, our experiments, which combine token filtering with existing LLM training systems, demonstrate only a modest 1.2% speedup in train- ing time, even when 40% of the tokens are eliminated (\u00a72.2). This limited enhancement in training efficiency constrains the broader advantages of token filtering for large-scale LLM training. Therefore, we pose the question: Can we fully unlock the efficiency of token filtering while simultaneously achieving greater utility than conventional training? To address this question, we first investigate the key fac- tors limiting the efficiency gains of existing token filtering system: (1) Insufficient sparsity after token filtering: Existing approaches fail to create true sparsity following token filter- ing [20]. These methods drop tokens only at the first stage of backpropagation\u2014the loss computation layer\u2014along with their corresponding loss values, without modifying the gen- erated gradients. As a result, in subsequent backpropagation steps, the hidden states of the dropped tokens are still up- dated because they contribute to the computed gradients (via attention). This leads to dense matrix computations, under- mining the efficiency of token filtering. (2) Inefficiency of sparse matrix implementations: Even when sparse matrices are employed, existing sparse GEMM (General Matrix Mul- 2This paper primarily focuses on backward filtering, as it demonstrates superior performance in enhancing the capabilities of LLMs. Further details can be found in \u00a72.1 3Rho-1 [20] received the Best Paper Runner-up Award at NeurIPS 2024. 1 arXiv:2502.00340v1 [cs.LG] 1 Feb 2025"
}