{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Verilog code generation"
  ],
  "datasets": [
    "VerilogEval-Human"
  ],
  "methods": [
    "LLMs",
    "self-consistency",
    "Cluster ranking",
    "Chain-of-Thought (CoT) reasoning"
  ],
  "results": [
    "10.5% average increase in functional correctness (pass@1)"
  ],
  "title": "VRank Enhancing Verilog Code Generation from Large Language Models Via Self-Consistency.pdf",
  "abstract": "\u2014Large Language Models (LLMs) have demonstrated promising capabilities in generating Verilog code from module specifications. To improve the quality of such generated Verilog codes, previous methods require either time-consuming manual inspection or generation of multiple Verilog codes, from which the one with the highest quality is selected with manually designed testbenches. To enhance the generation efficiency while maintaining the quality of the generated codes, we propose VRank, an automatic framework that generates Verilog codes with LLMs. In our framework, multiple code candidates are generated with LLMs by leveraging their probabilistic nature. Afterwards, we group Verilog code candidates into clusters based on identical outputs when tested against the same testbench, which is also generated by LLMs. Clusters are ranked based on the consistency they show on testbench. To determine the best candidate, Chain- of-Thought is further applied to select the best candidate from the top-ranked clusters. By systematically analyzing diverse outputs of generated codes, VRank reduces errors and enhances the overall quality of the generated Verilog code. Experimental results on the VerilogEval-Human benchmark demonstrate a significant 10.5% average increase in functional correctness (pass@1) across multi- ple LLMs, demonstrating VRank\u2019s effectiveness in improving the accuracy of automated hardware description language generation for complex design tasks. Index Terms\u2014Large Language Model, Verilog code generation I. INTRODUCTION As chip design complexity escalates alongside the demand for enhanced computational performance, hardware engineers are increasingly confronted with intricate design challenges. Large Language Models (LLMs) have made notable break- throughs in software code generation [1], [2] and are gradu- ally being applied to hardware code generation [3], [4]. For example, [5], [6] employs LLM to generate Verilog modules from module specifications and test benches, while [7] utilizes LLM as copilot to assist in writing Verilog code for CPU. These efforts highlight LLMs\u2019 potential in translating natural language specifications into hardware description languages such as Verilog. Despite the promising automation of Verilog generations, the quality of the Verilog codes generated by LLMs is still low due to the limited amount of hardware description data. The issue persists even as researchers gather more Verilog data to train models like [8]. To further enhance the quality of such codes, previous work retried generation for several times till a good enough Verilog is recognized by human engineer [7], or used iterative feedback given by human-designed testbench [9]. Although LLMs are capable of generating correct Verilog codes for specific tasks within several retries, previous work is unable to differentiate which Verilog code sample is better without human effort. To address the limitations in the previous work, we introduce VRank, an automatic framework designed to improve the efficiency and accuracy of Verilog code generation using LLMs. VRank leverages the probabilistic nature of LLMs to gener- ate multiple code candidates and automatically clusters them based on their functional outputs when tested against LLM- generated testbenches. By ranking these clusters according to their consistency and applying a chain-of-thought reasoning process, VRank selects the code candidate with the highest quality from the top-ranked clusters. Our approach not only enhances the quality of the generated Verilog code but also enables an automatic generation flow without the need for any human intervention. The contribution of our paper is summarized as follows: \u2022 We propose an automated Verilog generation framework that enhances both the quality of the generated code and the efficiency of the generation process, eliminating the need for human intervention. \u2022 The proposed framework takes advantage of the prob- abilistic characteristics of LLMs by generating multiple Verilog codes from the same module specification. Such codes are clustered based on their functionalities for subsequent code selection. \u2022 Inspired by self-consistency in machine translation, the clusters are ranked based on the consistency of their simulation outputs. This clustering and ranking strategy can effectively identify the top candidates that are most likely functionally correct. \u2022 We further deploy chain-of-thought to enhance the selec- tion of codes. By reasoning and summarizing from LLMs, this process automatically differentiates between the top two clusters, improving the accuracy of the final selection. \u2022 Experimental results demonstrate the effectiveness of VRank, showing a significant improvement in pass@1 arXiv:2502.00028v1 [cs.AR] 22 Jan 2025"
}