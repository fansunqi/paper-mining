{
  "code_links": "None",
  "tasks": [
    "Realistic long-tailed semi-supervised learning"
  ],
  "datasets": [
    "CIFAR-10",
    "CIFAR-100",
    "STL-10",
    "Imagenet-127"
  ],
  "methods": [
    "Doubly robust estimation",
    "Label shift Expectation Maximization",
    "SimPro",
    "BOAT"
  ],
  "results": [
    "Improved accuracy on CIFAR-10, CIFAR-100, STL-10, and Imagenet-127",
    "SimPro+ and BOAT+ improve over original versions",
    "No degradation in accuracy in CIFAR-10 with inaccurate class label distributions"
  ],
  "title": "Improving Realistic Semi-Supervised Learning with Doubly Robust Estimation.pdf",
  "abstract": "A major challenge in Semi-Supervised Learning (SSL) is the limited information available about the class distribution in the unlabeled data. In many real-world applications this arises from the prevalence of long-tailed distributions, where the standard pseudo-label approach to SSL is biased towards the labeled class distribution and thus per- forms poorly on unlabeled data. Existing methods typically assume that the unlabeled class distribu- tion is either known a priori, which is unrealistic in most situations, or estimate it on-the-fly us- ing the pseudo-labels themselves. We propose to explicitly estimate the unlabeled class distribu- tion, which is a finite-dimensional parameter, as an initial step, using a doubly robust estimator with a strong theoretical guarantee; this estimate can then be integrated into existing methods to pseudo-label the unlabeled data during training more accurately. Experimental results demon- strate that incorporating our techniques into com- mon pseudo-labeling approaches improves their performance. 1. Introduction Semi-supervised learning (SSL) aims to augment the small labeled set of data with a large unlabeled set of data (Chapelle et al., 2009). This is of considerable practical significance since in many applications unlabeled data is easily available but the labeling effort is very costly. Many semi-supervised learning methods have proven successful, even given very small amounts of labeled data. However there is very limited information about the unlabeled data, since we only have access to their features and not the unla- beled class distribution. We will write the labeled and unla- beled class distributions as P(Y |A = 1) and P(Y |A = 0), respectively. In some situations P(Y |A = 0) is known a priori, but in many practical applications it is unknown 1Cornell Tech 2Google DeepMind. Correspondence to: Khiem Pham <dkp45@cornell.edu>. Preliminary work. Under review by the International Conference on Machine Learning (ICML). Do not distribute. and difficult to estimate from P(Y |A = 1). In particu- lar, the distribution P(Y |A = 0) is frequently long-tailed. This SSL variant, where the unlabeled class distribution P(Y |A = 0) is unknown and differs from P(Y |A = 1), is sometimes called realistic long-tailed semi-supervised learning (RTSSL). This topic has drawn considerable recent interest (see for example (Du et al., 2024; Kim et al., 2020; Wei et al., 2021; Oh et al., 2022; Wei & Gan, 2023; Ma et al., 2024)) since it reflects realistic assumptions in many applications. In this paper, we explicitly estimate the unlabeled class dis- tribution P(Y |A = 0) as a separate first step. We note that existing methods that estimate and use this distribution dur- ing training produce biased estimate. In particular, SimPro (Du et al., 2024) tends to significantly overestimate the head classes as shown in Figure 1 in 4 out of 5 unlabeled class dis- tributions studied. In contrast, our proposed doubly-robust estimator are more accurate. Our technique derives from semi-parametric efficiency theory predominantly studied in causal inference and has well-understood and strong theo- retical guarantee (Chernozhukov et al., 2018). Leveraging this improvement, we plug this first-stage estimate into a second stage algorithm for training the final classifier. We also adapt a maximum likelihood framework for semi- supervised learning with label shift. The framework uses and estimates a missingness mechanism which encodes the tendency of a label to be in the labeled (A = 1) or un- labeled (A = 0) set, and allows learning from both sets from one missing-data likelihood, which we address with an Expectation-Maximization (EM) algorithm. The basic idea dates as far back as (Ibrahim & Lipsitz, 1996), yet despite its simplicity, it is often overlooked in the label shift and vision community. We show that it naturally generalizes and extends FixMatch (Sohn et al., 2020). The recent work of (Du et al., 2024) can be seen as the same algorithm but with different parameterization (see Section 3.2). In summary, we propose a 2-stage algorithm for RTSSL (see Figure 2). The first stage uses maximum likelihood and EM to initially learn about the data. The first-stage estimates are used for a meta doubly-robust estimator which significantly improves on the initial estimate of P(Y |A = 0). We then plug this estimate into existing pseudo-labeling-based tech- niques to learn the final classifier. Experiments demonstrate 1 arXiv:2502.00279v1 [cs.LG] 1 Feb 2025"
}