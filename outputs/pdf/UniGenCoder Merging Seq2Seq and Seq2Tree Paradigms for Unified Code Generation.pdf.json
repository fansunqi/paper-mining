{
  "code_links": [
    "https://github.com/DeepLearnXMU/UniGenCoder"
  ],
  "tasks": [
    "Code Generation",
    "Text-to-Code Generation",
    "Code-to-Code Generation"
  ],
  "datasets": [
    "CONCODE",
    "CodeXGLUE"
  ],
  "methods": [
    "UniGenCoder",
    "Sequence-to-Sequence (Seq2Seq)",
    "Sequence-to-Tree (Seq2Tree)",
    "Multi-task Learning",
    "Knowledge Distillation",
    "Contrastive Learning"
  ],
  "results": [
    "BLEU: 44.36",
    "EM: 85.69"
  ],
  "title": "UniGenCoder Merging Seq2Seq and Seq2Tree Paradigms for Unified Code Generation.pdf",
  "abstract": "\u2014Deep learning-based code generation has completely transformed the way developers write programs today. Existing approaches to code generation have focused either on the Sequence- to-Sequence paradigm, which generates target code as a sequence of tokens, or the Sequence-to-Tree paradigm, which outputs code as a sequence of actions. While these two paradigms are intuitively complementary, their combination has not been previously explored. By comparing the code generated under these two paradigms, we find that integrating them holds significant potential. In this paper, we propose UniGenCoder for code-related generation tasks, which consists of a shared encoder, a shared decoder with a minimal set of additional parameters to unify two paradigms, and a selector that dynamically chooses optimal paradigm for each instance. Also, during the model training, we first perform the multi-task learning and distillation strategies to facilitate knowledge transfer between two paradigms, and then leverage contrastive learning to train the selector. Experimental results on the text-to-code and code-to-code generation tasks demonstrate the effectiveness of our proposed model. We release our code at https://github.com/DeepLearnXMU/UniGenCoder. Index Terms\u2014Code Generation, Sequence-to-Sequence, Sequence-to-Tree I. INTRODUCTION Code understanding and generation are fundamental tasks in software development, impacting a wide range of activities such as debugging, refactoring, program synthesis, etc.. As modern software systems grow increasingly complex, automating these tasks can significantly enhance developer productivity and streamline the overall software development lifecycle. Pre- trained Transformer-based [1] language models like BERT [2] and T5 [3] have achieved significant success in natural language (NL) understanding and generation. Given the sim- ilarities between NL and programming languages (PLs), it is common practice to adapt these models for PLs (such as CodeBERT [4] and CodeT5 [5]), treating code as a sequence of words to automate various SE tasks [6]\u2013[9]. Generally, pre-trained models are categorized into three types: encoder- only (e.g., CodeBERT), decoder-only (e.g., CodeGPT [10]), and encoder-decoder models (e.g., CodeT5). Encoder-only models, optimized for understanding tasks, are less effective for generation, while decoder-only models, designed for generation, are suboptimal for understanding [5]. Encoder-decoder models can seamlessly support both code understanding and generation. \u00a7Equal contribution. \u00a3Corresponding author. Despite the similarity between NL and PL, code is more strongly structured, with its semantics heavily relying on the structural dependency between code elements. As a result, researchers have started encoding structural information (e.g., data flow [11] abstract syntax trees (ASTs) [12]) into input code sequences for the encoder training, in order to learn structure- aware code representations through various pre-training tasks. On the decoder side, most approaches [13]\u2013[15] output a continuous token sequence based on the Sequence-to-Sequence (Seq2Seq) paradigm, while the others [16]\u2013[20] leverage syntactic information and decode code as an action sequence corresponding to the pre-order traversal of an AST, following the Sequence-to-Tree (Seq2Tree) paradigm. Both of these two paradigms mainly target code-related generation tasks, such as text-to-code and code-to-code generation, and code repair. However, the combination of Seq2Seq and Seq2Tree paradigms has not yet been explored. Intuitively, Seq2Seq- based code models align with the paradigm used by NL models, enabling them to fully leverage the prior knowledge learned from large pre-train corpora. In contrast, while Seq2Tree-based code models can also be fine-tuned from NL models, their focus shifts toward capturing syntactic structure when generating ASTs. Despite considering the crucial syntax in code, this shift to a different paradigm may lead to the loss of some valuable knowledge from NL corpora. Therefore, combining two paradigms presents an opportunity to benefit from both. To verify our hypothesis, we compare the performance of CodeT5 under the Seq2Seq paradigm \u2013 CodeT5(Seq2Seq), and under the Seq2Tree paradigm \u2013 CodeT5(Seq2Tree)1, using BLEU scores on the CONCODE dev set, a common text-to-code generation dataset. Specifically, CodeT5 adapts the text-to- text transformer (i.e., T5) model for code by incorporating token type information, and the BLEU score is widely-used to measure the similarity between the generated and ground-truth code. Our results show that even though CodeT5(Seq2Seq) is more effective than CodeT5(Seq2Tree) regarding the overall performance (37.06 vs. 35.57), a more fine-grained analysis reveals that CodeT5(Seq2Seq) outperforms CodeT5(Seq2Tree) on 34.95% of instances, while CodeT5(Seq2Tree) excels in 40.00% of instances. Additionally, CodeT5(Seq2Tree) predicts 1The CodeT5 was originally designed under the Seq2Seq paradigm, so we implemented the CodeT5(Seq2Tree) ourselves to ensure a fair comparison. arXiv:2502.12490v1 [cs.CL] 18 Feb 2025"
}