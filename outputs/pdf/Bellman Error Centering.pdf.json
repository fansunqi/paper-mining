{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Reinforcement Learning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Bellman Error Centering",
    "Centered Temporal Difference Learning",
    "Centered TDC"
  ],
  "results": [
    "On-policy CTD and off-policy CTDC algorithms converge with probability one to the centered TD fixpoint under standard assumptions"
  ],
  "title": "Bellman Error Centering.pdf",
  "abstract": "This paper revisits the recently proposed reward centering algorithms including simple reward cen- tering (SRC) and value-based reward centering (VRC), and points out that SRC is indeed the re- ward centering, while VRC is essentially Bellman error centering (BEC). Based on BEC, we provide the centered fixpoint for tabular value functions, as well as the centered TD fixpoint for linear value function approximation. We design the on-policy CTD algorithm and the off-policy CTDC algo- rithm, and prove the convergence of both algo- rithms. Finally, we experimentally validate the stability of our proposed algorithms. Bellman er- ror centering facilitates the extension to various reinforcement learning algorithms. 1. Introduction Reinforcement learning (RL) has driven transformative ad- vances in strategic decision-making (e.g., AlphaGo (Silver et al., 2016)) and human-aligned large language systems (e.g., ChatGPT via RLHF (Ouyang et al., 2022; Carta et al., 2023; Dai et al., 2024; Guo et al., 2025)). However, these breakthroughs incur prohibitive computational costs: Alp- haZero requires billions of environment interactions, while RL-based training of state-of-the-art language models de- mands millions of GPU hours (Patterson et al., 2021). Such challenges necessitate urgent innovations in RL efficiency to enable scalable AI development. To tackle long-term and continuous reinforcement learn- ing problems, two primary maximization objectives have been proposed: the average reward criterion and the dis- counted reward criterion. In the context of average reward, Schwartz (1993) introduced R-Learning, employing an adap- tive method to estimate average rewards. Das et al. (1999) proposed SMART, which focuses on estimating average rewards directly. Abounadi et al. (2001) introduced RVI *Equal contribution 1Nanjing University of Posts and Telecom- munications, Nanjing, China 2National University of Defense Technology, Hefei, China. Correspondence to: Wenhao Wang <wangwenhao11@nudt.edu.cn>. Q-learning, utilizing the value of a reference state to en- hance the learning process. Yang et al. (2016) proposed CSV-learning, which employs a constant shifting values to improve convergence. Wan et al. (2021) removed reference state of RVI Q-learning, proposed differential Q-learning and differential TD learning. Regarding discounted rewards, Perotto & Vercouter (2018); Grand-Cl\u00b4ement & Petrik (2024) highlighted that, under cer- tain conditions, such as with a large discount factor, Black- well optimality can be achieved. Sun et al. (2022) demon- strated that reward shifting can effectively accelerate conver- gence in deep reinforcement learning. Schneckenreither & Moser (2025) introduced near-Blackwell-optimal Average Reward Adjusted Discounted Reinforcement Learning us- ing Laurent Series expansion of the discounted reward value function. Naik et al. (2024); Naik (2024) proposed the con- cept of reward centering, designing simple reward centering and value-based reward centering, and proved the conver- gence of tabular Q-learning with reward centering. Apply- ing reward centering in tabular Q-learning, Q-learning with linear function approximation and Deep Q-Networks (DQN) have produced outstanding experimental results across all these approaches (Naik et al., 2024). However, three issues remain unresolved: (1) Naik et al. (2024); Naik (2024) pointed out that while reward centering can be combined with other reinforcement learning (RL) algorithms, the specific methods for such integration are not straightforward or trivial. The underlying mechanisms of re- ward centering warrant further investigation. (2) Currently, there is only a convergence proof for tabular Q-learning with reward centering, leaving the convergence properties of reward centering in large state spaces with function ap- proximation still unknown. (3) If the algorithm converges, what solution it will converge to is also an open question. In response to these three issues, the contributions of this paper are as follows: (1) We demonstrated that value-based reward centering is essentially Bellman error centering. (2) Under linear function approximation, the solution of Bell- man error centering converges to the centered TD fixpoint. (3) Building on Bellman error centering, we designed cen- tered temporal difference learning algorithms, referred to as on-policy CTD and off-policy CTDC, respectively. (4) We provide convergence proofs under standard assumptions. 1 arXiv:2502.03104v1 [cs.LG] 5 Feb 2025"
}