{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Measuring Inaccuracies in the Proportional Hazard Rate Model"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Length-Biased Weighted Residual approach",
    "Extropy",
    "Kernel density estimation",
    "Empirical distribution function"
  ],
  "results": [
    "None"
  ],
  "title": "Measuring Inaccuracies in the Proportional Hazard Rate Model Based on Extropy Using a Length-Biased .pdf",
  "abstract": "In this paper, we consider the concept of the residual inaccuracy measure and extend it to its weighted version based on extropy. Properties of this measure are studied and the discrimination principle is applied in the class of proportional hazard rate (PHR) models. A characterization problem for the proposed weighted extropy-inaccuracy measure is stud- ied. We propose some alternative expressions of weighted residual measure of inaccuracy. Additionally, we establish upper and lower limits and various inequalities related to the weighted residual inaccuracy measure using extropy. Non-parametric estimators based on the kernel density estimation method and empirical distribution function for the proposed measure are obtained and the performance of the estimators are also discussed using some simulation studies. Finally, a real dataset is applied for illustrating our new proposed mea- sure. In general, our study highlights the potential of the weighted residual inaccuracy measure based on extropy as a powerful tool for improving the quality and reliability of data analysis and modelling across various disciplines. Researchers and practitioners can bene\ufb01t from incorporating this measure into their analytical toolkit to enhance the accuracy and e\ufb00ectiveness of their work. Keywords and Phrases: Extropy, Weighted measure of inaccuracy, Proportional hazard rate model, Kernel density estimation, Residual lifetime. AMS 2000 Subject Classi\ufb01cation: 62F10; 62N05. 1 Introduction The concept of entropy was \ufb01rst proposed by the physicist Lebowitz [20] to express the degree of chaos in the physical system. Later, Shannon [36] proposed and extended entropy to the \ufb01eld of information as a measure of information uncertainty. Shannon entropy represents the absolute limit on the best possible lossless compression of any communication. For additional details on this concept, refer to Cover and Thomas [6]. Suppose we have two non-negative continuous random variables X and Y , which represent the time to failure of two systems. These variables have probability density functions (PDFs) f(x) and g(x) respectively. Additionally, let F(x) and G(x) be the cumulative distribution \u2217kazemimr88@gmail.com 1"
}