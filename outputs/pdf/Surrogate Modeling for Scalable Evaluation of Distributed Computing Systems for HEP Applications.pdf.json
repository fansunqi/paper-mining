{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Scalable Evaluation of Distributed Computing Systems for HEP Applications"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Surrogate Modeling",
    "Machine Learning",
    "BiGRU",
    "BiLSTM",
    "Transformer"
  ],
  "results": [
    "Orders of magnitude faster execution times compared to DCSim",
    "Inaccuracies observed in predicting some observables"
  ],
  "title": "Surrogate Modeling for Scalable Evaluation of Distributed Computing Systems for HEP Applications.pdf",
  "abstract": ". The Worldwide LHC Computing Grid (WLCG) provides the ro- bust computing infrastructure essential for the LHC experiments by integrating global computing resources into a cohesive entity. Simulations of different com- pute models present a feasible approach for evaluating future adaptations that are able to cope with future increased demands. However, running these simu- lations incurs a trade-off between accuracy and scalability. For example, while the simulator DCSim can provide accurate results, it falls short on scaling with the size of the simulated platform. Using Generative Machine Learning as a surrogate presents a candidate for overcoming this challenge. In this work, we evaluate the usage of three different Machine Learning models for the simulation of distributed computing systems and assess their ability to generalize to unseen situations. We show that those models can predict central observables derived from execution traces of compute jobs with approximate accuracy but with orders of magnitude faster execution times. Furthermore, we identify potentials for improving the predictions towards better accuracy and generalizability. 1 Introduction High energy physics at the LHC relies on the global-scale federated computing infrastruc- ture provided by the Worldwide LHC Computing Grid (WLCG) for processing and storing the sheer amounts of scientific data gathered by the LHC experiments. To achieve the high- throughput demands with limited financial resources, good performance of the computing system has to be constantly ensured. With increasing demands expected from future LHC op- erations and technical and strategic changes in the federation, the best design for the WLCG infrastructure still needs to be identified. Due to the size and complexity of such infrastructures, it is not feasible to build alterna- tive infrastructures for mere testing. In addition, due to constant uptime requirements, they cannot be reserved for test and evaluation purposes. Instead, accurate simulation of workflow executions on such infrastructures and subsequent analysis of the simulated results can be performed without disturbing the operation of the real reference infrastructure, and a variety of alternative designs can be tested with just a fraction of the original investment. \u2217e-mail: larissa.schmid@kit.edu \u2217\u2217e-mail: maximilian.horzela@cern.ch arXiv:2502.12741v1 [cs.DC] 18 Feb 2025"
}