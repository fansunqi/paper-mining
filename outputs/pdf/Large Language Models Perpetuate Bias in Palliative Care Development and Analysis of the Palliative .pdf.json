{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Bias in LLM-generated responses in palliative and end-of-life care"
  ],
  "datasets": [
    "Palliative Care Adversarial Dataset (PCAD)-Direct",
    "Palliative Care Adversarial Dataset (PCAD)-Counterfactual"
  ],
  "methods": [
    "Adversarial testing",
    "Counterfactual testing",
    "Bias grading rubrics"
  ],
  "results": [
    "High rates of bias in LLM-generated responses",
    "Bias rates ranged from a quarter to more than half of responses",
    "Bias rates were similar across axes of identity and dimensions of care"
  ],
  "title": "Large Language Models Perpetuate Bias in Palliative Care Development and Analysis of the Palliative .pdf",
  "abstract": "King's College London, Cicely Saunders Institute of Palliative Care, Policy and Rehabilitation, London, UK. 2. Palliative Medicine Service, CIUSSS Nord-de-l'\u00cele de Montr\u00e9al, Montreal, Quebec, Canada. 3. Institute of Ophthalmology, University College London, London, UK. 4. The CHUM School of Artificial Intelligence in Healthcare, Montreal, Quebec, Canada. 5. Cole Eye Institute, Cleveland Clinic, Cleveland, USA. Corresponding author: Dr Naomi Akhras. King's College London, Cicely Saunders Institute of Palliative Care, Policy and Rehabilitation, London, UK. naomi.akhras@kcl.ac.uk _______________________________________________________________ Aim: Bias and inequity in palliative care disproportionately affect marginalised groups. Large language models (LLMs), such as GPT-4o, hold potential to enhance care but risk perpetuating biases present in their training data. This study aimed to systematically evaluate whether GPT-4o propagates biases in palliative care responses using adversarially designed datasets. Design: In July 2024, GPT-4o was probed using the Palliative Care Adversarial Dataset (PCAD), and responses were evaluated by three palliative care experts in Canada and the United Kingdom using validated bias rubrics. Settings/participants: The PCAD comprised PCAD-Direct (100 adversarial questions) and PCAD-Counterfactual (84 paired scenarios). These datasets targeted four care dimensions (access to care, pain management, advance care planning, and place of death preferences) and three identity axes (ethnicity, age, and diagnosis). Results: Bias was detected in a substantial proportion of responses. For adversarial questions, the pooled bias rate was 0.33 (95% confidence interval [CI]: 0.28, 0.38); \u201callows biased premise\u201d was the most frequently identified source of bias (0.47; 95% CI: 0.39, 0.55), such as failing to challenge stereotypes. For counterfactual scenarios, the pooled bias rate was 0.26 (95% CI: 0.20, 0.31), with \u201cpotential for withholding\u201d as the most frequently identified source of bias (0.25; 95% CI: 0.18, 0.34), such as withholding interventions based on identity. Bias rates were consistent across care dimensions and identity axes. Conclusions: GPT-4o perpetuates biases in palliative care, with implications for clinical decision-making and equity. The PCAD datasets provide novel tools to assess and address LLM bias in palliative care. Keywords: palliative care, end-of-life care, bias, artificial intelligence, large language models"
}