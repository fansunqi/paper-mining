{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Exploration-Exploitation Tradeoff",
    "In-context Reinforcement Learning",
    "Multi-armed Bandits",
    "Contextual Bandits"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Large Language Models",
    "GPT-4",
    "GPT-4o",
    "GPT-3.5",
    "LLM-based Exploration Oracles",
    "Mitigation Techniques",
    "Linear Regression Baseline"
  ],
  "results": [
    "LLMs perform poorly in exploitation tasks, particularly for larger or more complex settings",
    "LLMs can effectively suggest candidate actions in large action spaces for exploration",
    "LLMs outperform random guessing and are comparable to some mitigation techniques in exploration tasks"
  ],
  "title": "Should You Use Your Large Language Model to Explore or Exploit.pdf",
  "abstract": "We evaluate the ability of the current generation of large language models (LLMs) to help a decision-making agent facing an exploration-exploitation tradeoff. We use LLMs to explore and exploit in silos in various (contextual) bandit tasks. We find that while the current LLMs often struggle to exploit, in-context mitigations may be used to substantially improve performance for small-scale tasks. However even then, LLMs perform worse than a simple linear regression. On the other hand, we find that LLMs do help at exploring large action spaces with inherent semantics, by suggesting suitable candidates to explore. 1 Introduction There has been significant interest in the machine learning community to apply recent advances in generative AI and large language models (LLMs) to solve important decision-making problems. Early work in this direction has already produced impressive agentic behavior in both virtual [e.g., 41, 32] and physical-world environments [e.g., 7]. However, significant challenges remain. Beyond generalization (needed for supervised learning), decision-making under uncertainty requires two additional capabilities: exploitation (making the best decision given the current data) and exploration (trying new options for long-term benefit). Balancing the two has been a subject of an enormous literature, see books [e.g., see books 38, 25, 3]. A recent line of work [e.g., 23, 31] evaluates the ability of LLMs to balance exploration and exploitation entirely in-context, i.e., by specifying the problem description, parameters, and history in the LLM prompt. Focused on simple tasks in reinforcement learning, these results have been mixed. Both papers show that LLMs fail to solve these tasks adequately out-of-the-box, but they can be prompted to do so, e.g. by providing various summary statistics in-context, or by fine-tuning on data from algorithmic baselines on similar problem instances. Unfortunately, such mitigations do not readily extend beyond simple settings. For example, succinct summary statistics do not exist for many decision-making tasks, and fine-tuning for specific tasks may be prohibitive due to cost or lack of sufficient training data. Motivated by these observations, we study the ability of Gpt-4 [2], Gpt-4o [18], and Gpt-3.5 [8] to explore and exploit in-context in silos, with an eye towards leveraging a pre- trained LLM (and the inductive bias therein) as a part of a larger decision-making agent. We focus on (contextual) bandits, as a standard abstraction for the explore-exploit tradeoff. In Section 3, we evaluate LLMs as exploitation oracles for contextual bandits. Given a history of (context, action, reward) tuples, the LLM is tasked with identifying the best action \u2217Some of the results were obtained while the author was an intern at Microsoft Research. 1 arXiv:2502.00225v1 [cs.LG] 31 Jan 2025"
}