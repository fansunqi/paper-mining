{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Driving Scenario Interactivity Assessment"
  ],
  "datasets": [
    "nuScenes"
  ],
  "methods": [
    "Surprise Potential Estimation",
    "Counterfactual Reasoning",
    "Motion Prediction Models",
    "Wasserstein Distance"
  ],
  "results": [
    "Correlation: >0.82 with human labels",
    "Improved planner performance on interactive datasets"
  ],
  "title": "Surprise Potential As a Measure of Interactivity in Driving Scenarios.pdf",
  "abstract": "\u2014Validating the safety and performance of an au- tonomous vehicle (AV) requires benchmarking on real-world driving logs. However, typical driving logs contain mostly un- eventful scenarios with minimal interactions between road users. Identifying interactive scenarios in real-world driving logs en- ables the curation of datasets that amplify critical signals and provide a more accurate assessment of an AV\u2019s performance. In this paper, we present a novel metric that identifies interactive scenarios by measuring an AV\u2019s surprise potential on others. First, we identify three dimensions of the design space to describe a family of surprise potential measures. Second, we exhaustively evaluate and compare different instantiations of the surprise potential measure within this design space on the nuScenes dataset. To determine how well a surprise potential measure correctly identifies an interactive scenario, we use a reward model learned from human preferences to assess alignment with human intuition. Our proposed surprise potential, arising from this exhaustive comparative study, achieves a correlation of more than 0.82 with the human-aligned reward function, outperforming ex- isting approaches. Lastly, we validate motion planners on curated interactive scenarios to demonstrate downstream applications. I. INTRODUCTION Autonomous vehicles (AV) are highly dependent on real- world driving logs for software development, verification, and validation. However, the majority of recorded driving data consists of uneventful driving with limited interactions [1], of- fering insufficiently challenging benchmarks. For example, the nuScenes dataset [2] exhibits a highly imbalanced distribution of interactive scenarios, as illustrated in Figure 1, where less than 10% of the scenarios involve significant interactivity. This imbalance can distort performance metrics, thus obscuring a clear assessment of the AV stack\u2019s safety and capabilities [3]. To effectively test the AV stack and improve its ability to handle critical driving situations, it is essential to curate and evaluate interactive driving scenarios. Indeed, the challenge of identifying highly interactive sce- narios has attracted significant interest, leading to various approaches. Broadly, these methods can be categorized into rules-based [4], supervised learning [5], and unsupervised learning [6] approaches. Rule-based methods have limited expressivity to describe complex interactions, while supervised learning methods require substantial human effort. Therefore, in this paper, we lean towards unsupervised learning methods. In particular, we explore the curation of interactive scenarios by quantifying their surprise potential1, which is generally 1Prior literature refers to this as surprise while omitting the term \u201cpotential.\u201d However, the methods discussed in this paper and prior works, such as [6], estimate the potential for surprise rather than the actual surprise of a scenario. 90% of data 1% of data Number of sample 9% of data No interaction Weak interaction Strong interaction Straight lane-keeping Crossing intersection Cutting in Fig. 1. The distribution of the interactive score obtained by surprise potential on the nuScenes dataset [2]. We notice that most scenarios in the dataset are not considered interactive. aligned with human experience [6, 7, 8]. In the context of driv- ing, when an agent is surprised by the behavior of another, the surprise manifests itself in the form of a sudden and significant deviation from the expected nominal behavior, for example, sudden braking or swerving. Motivated by this observable outcome of being surprised, a widely adopted approach \u2013 also employed in this paper \u2013 for estimating surprise potential involves counterfactual reasoning. The trajectory of an agent in the scene is perturbed and the distribution shift in the behavior of other agents is measured [6, 7, 8] \u2013 the larger the distribu- tion shift, the more surprising a scenario can be. The surprise potential of a scenario can be likened to potential energy, which, when unleashed, can provoke surprising reactions from other traffic agents, potentially compromising safety. In this paper, we present a detailed comparative study on the class of surprise potential estimation methods that leverage counterfactual reasoning [9]. We present insights into the design choices underlying these methods from three perspectives: (i) the method for counterfactual generation; (ii) the architecture of the future prediction model; and (iii) the distance metric for evaluating distribution shift. This com- prehensive investigation enables us to develop new instances that modify history trajectories to predict counterfactual future scenarios, and employ the Wasserstein distance to measure the distribution shift between these scenarios. The absence of ground-truth interactivity labels presents a significant challenge in evaluating the performance of different surprise potential methods. Given the difficulty of assigning absolute interactivity scores, we developed a tool to collect human preference labels and employed ranking-based correla-"
}