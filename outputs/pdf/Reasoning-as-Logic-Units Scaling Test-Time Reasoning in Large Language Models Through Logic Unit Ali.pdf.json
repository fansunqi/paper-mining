{
  "code_links": [
    "https://github.com/acceptallgood/RaLU"
  ],
  "tasks": [
    "Reasoning"
  ],
  "datasets": [
    "GSM8K",
    "MATH",
    "HumanEval",
    "MBPP"
  ],
  "methods": [
    "Reasoning-as-Logic-Units (RaLU)",
    "Control Flow Graph (CFG)",
    "Logic Unit Alignment"
  ],
  "results": [
    "Improved accuracy in mathematical reasoning (GSM8K, MATH)",
    "Improved accuracy in code reasoning (HumanEval, MBPP)"
  ],
  "title": "Reasoning-as-Logic-Units Scaling Test-Time Reasoning in Large Language Models Through Logic Unit Ali.pdf",
  "abstract": "Chain-of-Thought (CoT) prompting has shown promise in enhancing the reasoning capabilities of large language models (LLMs) by generating natural language (NL) rationales that lead to the fi- nal answer. However, it struggles with numerical computation, which has somehow led to the de- velopment of program-aided techniques. Despite their potential, a persistent challenge remains: in- consistencies between LLM-reported reasoning steps and the logic in generated programs, which we term \u201creasoning hallucinations.\u201d This stems from the inherent ambiguities of NL and the sta- tistical nature of LLMs, which often lack rigor- ous logical coherence. To address this challenge, we propose a novel test-time scaling framework, Reasoning-as-Logic-Units (RaLU), which con- structs a more reliable reasoning path by aligning logical units between the generated program and their corresponding NL descriptions. By decom- posing the initially generated program into dis- crete units using static analysis, RaLU engages in an iterative dialogue with the LLM to judge, re- fine, and explain each unit. A rewind-and-correct mechanism ensures alignment between code state- ments and task requirements in each unit, ulti- mately forming a cohesive reasoning path un- der the program\u2019s logic, from which the model reaches a final solution. Our experiments demon- strate that RaLU significantly outperforms exist- ing baselines in mathematical reasoning (GSM8K, MATH) and algorithmic reasoning (HumanEval+, MBPP+), underscoring its potential to advance LLM reasoning and programming by offering en- hanced accuracy and interpretability. 1Independent Researcher, Beijing, China 2Peking Univer- sity, Beijing, China. Correspondence to: Yiwen Guo <guoyi- wen89@gmail.com>. Under Review, Copyright 2025 by the author(s). 1. Introduction Extensive studies have shown that Chain-of-Thought (CoT) (Wei et al., 2022) prompting can improve the reasoning ca- pabilities of large language models (LLMs) (Mondorf & Plank, 2024; Fu et al., 2023; Liu et al., 2023a) by requir- ing LLMs to generate a rationale before its final decision. Complementary to CoT and its variants, program-aided tech- niques like Program-of-Thought (PoT) (Chen et al., 2023b) have emerged, which decompose complex reasoning and numerical computation by prompting LLMs to generate pro- grams and use external interpreters to solve mathematical problems. When combined with test-time scaling, which dynamically allocates more computational resources during inference, these methods optimize LLM performance in rea- soning tasks, particularly in mathematical and algorithmic domains (Zhong et al., 2024). Despite these advancements, both CoT and PoT face signifi- cant challenges. The inherent ambiguity and imprecision in natural language (NL) impede precise calculations in CoT- like methods (Yu et al., 2024; Gao et al., 2023). Meanwhile, PoTs simply replace NL reasoning with programs, so they cannot improve LLM in code generation, yet solving algo- rithmic problems is an important aspect of LLM reasoning. Plus, crafting accurate programs in a single attempt remains challenging (Chen et al., 2024a), so PoT can even introduce more errors than CoT sometimes (Li et al., 2024). Furthermore, simple combinations of CoT and PoT cannot yield satisfactory outcomes. Research indicates that guiding an LLM to generate step-by-step analysis in NL before deriv- ing programs may not outperform direct prompting (Huang et al., 2024a). This underperformance can be traced to inconsistencies between reasoning steps and the logic in generated programs, which we term \u201cReasoning Halluci- nations.\u201d The hallucinations manifest as: 1) accurate NL step descriptions but logical errors in individual code state- ments; 2) missing key steps or inclusion of irrelevant ones; and 3) correct steps misordered or improperly connected. Examples of these three types are presented in Figure 1. These reasoning hallucinations arise from the statistical na- ture of LLMs, which generate responses based on token predictions rather than true reasoning. LLMs mimic reason- 1 arXiv:2502.07803v1 [cs.AI] 5 Feb 2025"
}