{
  "code_links": [
    "https://github.com/zhengzaiyi/RotationSymmetry"
  ],
  "tasks": [
    "Model Fusion"
  ],
  "datasets": [
    "Emotion",
    "NER",
    "CoNLL",
    "STS-B",
    "SST-2",
    "CoLA"
  ],
  "methods": [
    "Rotation Symmetry",
    "Parameter Matching",
    "Optimal Transport"
  ],
  "results": [
    "Improved model fusion performance",
    "Reduced distance between end models",
    "Efficient parameter matching"
  ],
  "title": "Beyond the Permutation Symmetry of Transformers the Role of Rotation for Model Fusion.pdf",
  "abstract": "Symmetry in the parameter space of deep neural networks (DNNs) has proven beneficial for var- ious deep learning applications. A well-known example is the permutation symmetry in Multi- Layer Perceptrons (MLPs), where permuting the rows of weight matrices in one layer and applying the inverse permutation to adjacent layers yields a functionally equivalent model. While permutation symmetry fully characterizes the equivalence set for MLPs, its discrete nature limits its utility for transformers. In this paper, we introduce rotation symmetry, a novel form of parameter space sym- metry for transformers that generalizes permuta- tion symmetry by rotating parameter matrices in self-attention layers. Unlike permutation symme- try, rotation symmetry operates in a continuous do- main, thereby significantly expanding the equiv- alence set for transformers. Based on this prop- erty, we propose a theoretically optimal parameter matching algorithm as a plug-and-play module to enhance model fusion. We evaluate our approach using pre-trained transformers across diverse nat- ural language and vision tasks. Experimental results demonstrate that our rotation symmetry- based matching algorithm substantially improves model fusion, highlighting the potential of param- eter space symmetry to facilitate model fusion. Our code is available on https://github. com/zhengzaiyi/RotationSymmetry. 1. Introduction Parameter space symmetry is an intriguing property of neu- ral networks that has garnered increasing attention in recent years (Du et al., 2018; Armenta & Jodoin, 2021; Kunin et al., 2021; Simsek et al., 2021; Entezari et al., 2022; Grigsby et al., 2023). One of the most studied forms of parameter space symmetry is permutation symmetry (Ainsworth et al., *Equal contribution 1University of Virginia 2NEC Labo- ratories America. Correspondence to: Jundong Li <jun- dong@virginia.edu>. 2023; Entezari et al., 2022). For instance, in a two-layer MLP, permuting the rows of the weight matrix in the first layer and applying the corresponding inverse permutation to the second layer results in a functionally equivalent model, i.e., the outputs of the original and permuted models re- main identical for any given input (Ainsworth et al., 2023). All functionally equivalent models corresponding to weight permutations form an equivalence set, which provides the- oretical insights into neural network optimization, such as the linear mode connectivity of loss landscapes (Entezari et al., 2022; Zhou et al., 2023). In addition, permutation symmetry has also proven helpful in advancing neural net- work applications, such as model fusion (Singh & Jaggi, 2020; Ainsworth et al., 2023) and optimization (Zhao et al., 2024). Although parameter space symmetry has been extensively studied in classical neural network architectures, such as MLPs and CNNs, the understanding of its application in transformers (Vaswani et al., 2017) remains limited. Trans- formers have seen rapid advancements in recent years, achieving remarkable success in a wide range of applica- tions (Yun et al., 2019; Lewis et al., 2020; Raffel et al., 2020; Clark et al., 2020; Liu et al., 2021; Dosovitskiy et al., 2021; Zhou et al., 2021; He et al., 2024; Zhu et al., 2024; Zheng et al., 2024). The transformer architecture is built upon two primary submodules: feedforward networks and self- attention layers. The feedforward network, which is struc- turally similar to MLPs, naturally inherits the permutation symmetry that has been extensively studied in the existing literature. Self-attention layers, on the other hand, involve a unique attention mechanism powered by matrix products of queries, keys, and values, which introduce additional potentials for symmetry beyond permutations. Permutation symmetries limit the equivalence set of neural networks to discrete operations, which aligns well with MLPs due to their element-wise activations (e.g., ReLU (Glorot et al., 2011)). In contrast, the continuous nature of the matrix operations in self-attention layers necessitates more flexible operations to fully characterize their equivalence set. In this paper, we introduce rotation symmetry, a novel form of parameter space symmetry for self-attention layers in transformers. Specifically, we analyze the query and key matrices jointly and demonstrate that applying a rotation 1 arXiv:2502.00264v1 [cs.LG] 1 Feb 2025"
}