{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Text-to-image generation",
    "Style and category discrimination"
  ],
  "datasets": [
    "ImageNet-R",
    "PACS",
    "VLCS",
    "OfficeHome",
    "multi-style cartoon dataset"
  ],
  "methods": [
    "Control-CLIP",
    "Adapter module",
    "Cross-attention mechanism"
  ],
  "results": [
    "Category and style classification accuracy improved",
    "Average Human Ranking (AHR) improved"
  ],
  "title": "Control-CLIP Decoupling Category and Style Guidance in CLIP for Specific-Domain Generation.pdf",
  "abstract": "\u2014Text-to-image diffusion models have shown remark- able capabilities of generating high-quality images closely aligned with textual inputs. However, the effectiveness of text guidance heavily relies on the CLIP text encoder, which is trained to pay more attention to general content but struggles to capture semantics in specific domains like styles. As a result, generation models tend to fail on prompts like \u201da photo of a cat in Pokemon style\u201d in terms of simply producing images depicting \u201da photo of a cat\u201d. To fill this gap, we propose Control-CLIP, a novel decoupled CLIP fine-tuning framework that enables the CLIP model to learn the meaning of category and style in a complement manner. With specially designed fine-tuning tasks on minimal data and a modified cross-attention mechanism, Control-CLIP can precisely guide the diffusion model to a specific domain. Moreover, the parameters of the diffusion model remain unchanged at all, preserving the original generation performance and diversity. Experiments across multiple domains confirm the effectiveness of our approach, particularly highlighting its robust plug-and- play capability in generating content with various specific styles. Index Terms\u2014component, formatting, style, styling, insert I. INTRODUCTION In recent years, there have been significant advancements in text-controlled image generation. With the emergence of text-to-image models [1]\u2013[5], we can now create visually stunning images by providing textual prompts. Apart from generating high quality images, accurately controlling and adapting the style of generated images via text is also a crucial requirement in AI-based artistic creation. A challenge lies in leveraging domain-specific text-image data to help pre-trained generative models better understand the stylistic nuances of a particular domain and accurately generate images based on textual descriptions. To tackle this problem, methods have been proposed to fine-tune the diffusion generation model [6] or incorporating various control conditions [7], as illustrated in Fig. 1 (a) and (b). Contrastive Language-Image Pre-training (CLIP) [8] ef- fectively bridges the gap between the language and image domains and serves as the foundational network for most diffusion-based image generation models. However, despite the growing use of CLIP models in gener- ative tasks like text-to-image and text-to-video generation, the need for domain adaptability in CLIP models and its impact on generative models has been under-explored. We propose that domain adaptation of text-to-image diffusion models can be effectively achieved by fine-tuning only the CLIP component on a small amount of in-domain data, thus preserving the general-domain generation capability of pre-trained diffusion models. In this paper, we consider style features as domain-specific and object categories as domain-invariant. As noted in domain adaptation [9], each domain has unique characteristics, while all domains share invariant features. For instance, we can differentiate images like cartoons, sketches, and real photos based on color strokes and texture, which are domain-specific features. However, all styles of images can depict the same object, such as a cat or a dog, determined by category features independent of the domain. Previous research on fine-tuning the CLIP model for domain adaptation mainly focused on learning category features to improve classification accuracy within specific domains, as shown in Fig. 1(c). However, these methods do not explicitly separate style features from category features in their loss functions. While effective in some cases, they inadvertently reduce the model\u2019s ability to recognize stylistic nuances. In the application of text-to-image generation, a depth under- standing of nuanced domain-specific styles is demanded. Tra- ditional fine-tuning methods of CLIP models focus on learning domain-invariant features for classification has inadvertently suppressed the model\u2019s sensitivity to style-specific attributes. This tendency has impeded the fine-tuned CLIP models in recognizing various stylistic forms and has constrained their ability to accurately interpret text descriptions about domain- specific styles. This paper introduces a novel approach called Control- CLIP, designed to explicitly disentangle style and category features to enhance the representation capacity of CLIP. Our framework uses two distinct text encoders\u2014style and category encoders\u2014fine-tuned independently. As shown in Fig. 1(d), this integration improves the CLIP model\u2019s ability to inter- pret textual descriptions of object categories and styles. By decoupling domain features, the CLIP model gains enhanced resilience in style and category perception, excelling in precise few-shot classification and style discrimination tasks within specific domains. This improved semantic understanding also allows the CLIP model to better control image generation, producing images that align more accurately with language descriptions. We develop two variations of the Control-CLIP model to cater to diverse datasets. One variant uses cross- entropy-based loss functions for datasets with style labels, while the other employs triplet loss for datasets with only text description labels. This adaptability demonstrates the method\u2019s arXiv:2502.11532v1 [cs.CV] 17 Feb 2025"
}