{
  "code_links": [
    "https://github.com/swatikar/FighterJetDRL"
  ],
  "tasks": [
    "Fighter Jet Navigation",
    "Combat Decision Making"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Deep Reinforcement Learning (DRL)",
    "Double Deep Q-Learning (DDQN)",
    "Reward Function Analysis"
  ],
  "results": [
    "80% Task Completion Rate",
    "High Success Rate",
    "Effective Decision Making"
  ],
  "title": "Fighter Jet Navigation and Combat Using Deep Reinforcement Learning with Explainable AI.pdf",
  "abstract": "\u2014This paper presents the development of an Artificial Intelligence (AI) based fighter jet agent within a customized Pygame simulation environment, designed to solve multi-objective tasks via deep reinforcement learning (DRL). The jet\u2019s primary objectives include efficiently navigating the environment, reach- ing a target, and selectively engaging or evading an enemy. A re- ward function balances these goals while optimized hyperparam- eters enhance learning efficiency. Results show more than 80% task completion rate, demonstrating effective decision-making. To enhance transparency, the jet\u2019s action choices are analyzed by comparing the rewards of the actual chosen action (factual action) with those of alternate actions (counterfactual actions), providing insights into the decision-making rationale. This study illustrates DRL\u2019s potential for multi-objective problem-solving with explainable AI. Project page is available at: Project GitHub Link. Index Terms\u2014Artificial Intelligence, Deep Reinforcement Learning, Multi-Objective, Pygame Simulation, Explainability, Reward Function Analysis I. INTRODUCTION In recent years, rapid advancements in technology have positioned AI as a transformative force across various fields. AI\u2019s ability to emulate human intelligence has led to ground- breaking developments, reshaping industries and redefining how to approach complex tasks. Traditionally, many critical tasks relied heavily on skilled human intervention, especially in high-risk situations. However, AI now has the potential to autonomously undertake these tasks, reducing risk to human life and improving operational efficiency. Starting with notable achievements, such as surpassing human capabilities in chess in 1997, AI has expanded to tackling the highly intricate board game Go [1], [2]. This shift highlights a new era where AI not only matches but often surpasses human abilities in executing high-stakes, strategic tasks. RL, a subset of AI, enables an agent to learn effective actions within its environment through trial-and-error interactions, eliminating the need for human expert data to identify robust Courses of Action (CoAs) [3]. In the field of jet navigation and combat, several previous works have been conducted. The simulation model, as ex- plored in [4], primarily focuses on air combat scenarios rather than on reinforcement learning strategies or explainability. It lacks the detailed DRL approach and reward function design, and consequently, the emphasis is more on simulating combat than on optimizing learning strategies. Reinforcement learning is covered in [5], though it does not delve into explainability through factual versus counterfactual analysis. Instead, the focus lies on the technical aspects, such as network architecture and training processes, rather than on the decision- making transparency of the agent. Additionally, [5] lacks a detailed account of how agents improve their efficiency in task completion over time.The simulation-based pilot training systems discussed in [6] focus more on training scenarios and the roles of agents rather than providing an in-depth DRL approach. The reward systems discussed in [6] align more with training goals than the complex balance of efficiency and resource management featured in our proposed approach. Simpler reward functions, as noted in [7], typically concentrate on immediate task outcomes such as shooting down targets or avoiding crashes. While these reward functions effectively guide the agent\u2019s learning, they lack the complexity and balance necessary for encouraging nuanced decision-making processes, which require balancing long-term efficiency with short-term actions. In this research, all of these shortcomings are addressed. Therefore, the main contributions of this paper are: 1) A reward schema that balances efficiency, resource man- agement, and intelligent decision-making to address a multi-objective problem. 2) Enhanced explainability through factual and counterfac- tual analysis, providing insights into the agent\u2019s deci- sions and improving transparency. Our study is organized into the following key components: first, we develop a custom simulation environment. Next, we train a fighter jet agent to make strategic engagement decisions using the double deep q-learning (DDQN) algorithm. We then focus on optimizing mission resources and explaining the agent\u2019s decision-making process through factual and counter- factual scenarios. By addressing challenges such as prioritiza- tion, adaptive behavior, and risk assessment, this research aims to advance the development of intelligent, autonomous systems for complex, multi-objective scenarios, ultimately enhancing AI\u2019s role in high-stakes environments. II. FIGHTER JET PROBLEM FORMULATION A. Environment Design The high level simulation environment with reinforcement learning agent is depicted in Figure 1. This is a continuation of previous research [8] The simulation environment is designed using Python\u2019s Pygame package. In the visualization, the blue arXiv:2502.13373v1 [cs.AI] 19 Feb 2025"
}