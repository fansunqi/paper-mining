{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Comparing the Feedback Quality of LLMs and Teachers on Experimentation Protocols"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "LLM-based feedback generation",
    "Manual feedback collection",
    "Rating scheme for feedback quality"
  ],
  "results": [
    "No significant difference in overall feedback quality between LLMs and human experts/teachers",
    "LLMs underperform in the 'Feed Back' dimension of feedback"
  ],
  "title": "Towards Adaptive Feedback with AI Comparing the Feedback Quality of LLMs and Teachers on Experimenta.pdf",
  "abstract": "Effective feedback is essential for fostering students\u2019 success in scientific inquiry. With advancements in artificial intelligence, large language models (LLMs) offer new possibilities for delivering instant and adaptive feedback. However, this feed- back often lacks the pedagogical validation provided by real-world practitioners. To address this limitation, our study evaluates and compares the feedback qual- ity of LLM agents with that of human teachers and science education experts on student-written experimentation protocols. Four blinded raters, all professionals in scientific inquiry and science education, evaluated the feedback texts gener- ated by 1) the LLM agent, 2) the teachers and 3) the science education experts using a five-point Likert scale based on six criteria of effective feedback: Feed Up, Feed Back, Feed Forward, Constructive Tone, Linguistic Clarity, and Tech- nical Terminology. Our results indicate that LLM-generated feedback shows no significant difference to that of teachers and experts in overall quality. However, the LLM agent\u2019s performance lags in the Feed Back dimension, which involves identifying and explaining errors within the student\u2019s work context. Qualitative analysis highlighted the LLM agent\u2019s limitations in contextual understanding and in the clear communication of specific errors. Our findings suggest that com- bining LLM-generated feedback with human expertise can enhance educational practices by leveraging the efficiency of LLMs and the nuanced understanding of educators. Keywords: Artificial Intelligence, Feedback, Experimentation, Scientific Inquiry, Large Lanugage Models 1 arXiv:2502.12842v1 [cs.AI] 18 Feb 2025"
}