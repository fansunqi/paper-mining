{
  "code_links": [
    "None"
  ],
  "tasks": [
    "None"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "None"
  ],
  "results": [
    "None"
  ],
  "title": "Some Things to Know about Achieving Artificial General Intelligence.pdf",
  "abstract": "Current and foreseeable GenAI models are not capable of achieving artificial general intelligence because they are burdened with anthropogenic debt. They depend heavily on human input to provide well-structured problems, architecture, and training data. They cast every problem as a language pattern learning problem and are thus not capable of the kind of autonomy needed to achieve artificial general intelligence. Current models succeed at their tasks because people solve most of the problems to which these models are directed, leaving only simple computations for the model to perform, such as gradient descent. Another barrier is the need to recognize that there are multiple kinds of problems, some of which cannot be solved by available computational methods (for example, \u201cinsight problems\u201d). Current methods for evaluating models (benchmarks and tests) are not adequate to identify the generality of the solutions, because it is impossible to infer the means by which a problem was solved from the fact of its solution. A test could be passed, for example, by a test-specific or a test-general method. It is a logical fallacy (affirming the consequent) to infer a method of solution from the observation of success. Predictions that artificial intelligence is on the verge of achieving general intelligence keep on coming (e.g., Altman, 2025; Leike & Sutskever, 2023; Modei, 2024). For example, one estimate is that 88% of the necessary capabilities (Thompson, 2025) have been achieved. The AI safety clock (International Institute for Management Development, 2024) is a symbolic representation to how close the world is to not just artificial general intelligence, but uncontrolled AGI. OpenAI recently announced a \u201cbreakthrough\u201d on the ARC-AGI benchmark (Chollet, 2019). These predictions have also raised widespread concerns. The Future of Life Institute (2023) currently has over 33,000 signatures on its open letter to temporarily halt the development of large language models. The Center for AI Safety (2023) released a statement, signed by hundreds of individuals, that \u201cMitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.\u201d The Artificial Intelligence Action Summit convened in Paris in February 2025. On the other hand, Lu et al. (2024) argue that large language models only follow instructions and have no potential to autonomously master new skills without some explicit instruction. Governments around the world are considering regulations based on these threat assessments. For example, in 2024, the California legislature passed and the governor vetoed SB-1047 Safe and Secure Innovation for Frontier Artificial Intelligence Models Act. (2023-2024). Among other things, it required \u201cimplementing the capability to promptly enact a full shutdown\u201d of any covered AI model\u2014a \u201ckill switch\u201d. According to a study commissioned by the US State Department (Harris, Harris, & Beal, 2024) (The Gladstone Report),"
}