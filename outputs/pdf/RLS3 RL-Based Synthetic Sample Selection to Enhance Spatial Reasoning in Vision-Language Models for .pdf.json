{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Enhancing spatial reasoning in Vision-Language Models",
    "Synthetic data generation for VLM fine-tuning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Reinforcement Learning",
    "Synthetic data generation",
    "VLM fine-tuning"
  ],
  "results": [
    "None"
  ],
  "title": "RLS3 RL-Based Synthetic Sample Selection to Enhance Spatial Reasoning in Vision-Language Models for .pdf",
  "abstract": "Vision-language model (VLM) fine-tuning for application-specific visual grounding based on natural language instructions has be- come one of the most popular approaches for learning-enabled autonomous systems. However, such fine-tuning relies heavily on high-quality datasets to achieve successful performance in various downstream tasks. Additionally, VLMs often encounter limitations due to insufficient and imbalanced fine-tuning data. To address these issues, we propose a new generalizable framework to improve VLM fine-tuning by integrating it with a reinforcement learning (RL) agent. Our method utilizes the RL agent to manipulate objects within an indoor setting to create synthetic data for fine-tuning to address certain vulnerabilities of the VLM. Specifically, we use the performance of the VLM to provide feedback to the RL agent to generate informative data that efficiently fine-tune the VLM over the targeted task (e.g. spatial reasoning). The key contribution of this work is developing a framework where the RL agent serves as an informative data sampling tool and assists the VLM in order to enhance performance and address task-specific vulnerabilities. By targeting the data sampling process to address the weaknesses of the VLM, we can effectively train a more context-aware model. In addition, generating synthetic data allows us to have precise control over each scene and generate granular ground truth captions. Our results show that the proposed data generation approach improves the spatial reasoning performance of VLMs, which demonstrates the benefits of using RL-guided data generation in vision-language tasks. CCS Concepts \u2022 Computing methodologies \u2192Scene understanding; Rein- forcement learning; Spatial and physical reasoning; Active learning settings. Keywords spatial reasoning, synthetic data generation, self-improving sam- pling, vision-language models 1 Introduction Autonomous perception has progressed significantly with the ad- vent of deep learning, enabling its deployment in a wide range of cyber-physical systems applications. Vision-language models (VLMs) especially have proven to be powerful tools due to enhanced understanding of real-world scenes, which is critical for precisely interacting with environments in robotics [10]. This characteristic also makes VLMs well-suited for distracted driving detection [13] and fully autonomous self-driving cars [31, 41]. However, many challenges remain, and different models have various strengths and weaknesses. In particular, CLIP-type models, which perform contrastive vision- language fine-tuning, have shown that natural language model fine- tuning can lead to very high performance on a particular scenario arXiv:2501.18880v1 [cs.CV] 31 Jan 2025"
}