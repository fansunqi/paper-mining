{
  "code_links": "None",
  "tasks": [
    "Hallucination Detection in LVLMs"
  ],
  "datasets": [
    "Visual Genome"
  ],
  "methods": [
    "Scene Graph Parser",
    "Object Detection",
    "Visual Verification",
    "VQA",
    "Visual Similarity",
    "Scaling Factor"
  ],
  "results": [
    "Accuracy: 94.67% on POPE Adversarial Split",
    "F1 Score: 94.77% on POPE Adversarial Split",
    "Accuracy: 79.00% on R-Bench Image Level",
    "F1 Score: 80.73% on R-Bench Image Level"
  ],
  "title": "CutPaste Find Efficient Multimodal Hallucination Detector with Visual-aid Knowledge Base.pdf",
  "abstract": "Large Vision-Language Models (LVLMs) have demonstrated impressive multimodal reason- ing capabilities, but they remain susceptible to hallucination, particularly object hallucina- tion where non-existent objects or incorrect attributes are fabricated in generated descrip- tions. Existing detection methods achieve strong performance but rely heavily on expen- sive API calls and iterative LVLM-based vali- dation, making them impractical for large-scale or offline use. To address these limitations, we propose CutPaste&Find, a lightweight and training-free framework for detecting halluci- nations in LVLM-generated outputs. Our ap- proach leverages off-the-shelf visual and lin- guistic modules to perform multi-step verifi- cation efficiently without requiring LVLM in- ference. At the core of our framework is a Visual-aid Knowledge Base that encodes rich entity-attribute relationships and associated im- age representations. We introduce a scaling factor to refine similarity scores, mitigating the issue of suboptimal alignment values even for ground-truth image-text pairs. Comprehen- sive evaluations on benchmark datasets, includ- ing POPE and R-Bench, demonstrate that Cut- Paste&Find achieves competitive hallucination detection performance while being significantly more efficient and cost-effective than previous methods. 1 Introduction Large Vision Language Models (LVLMs) (Liu et al., 2023a; Ye et al., 2023; Zhu et al., 2023; Li et al., 2023a; Bai et al., 2023a; Dai et al., 2023b) have exhibited impressive multimodal understand- ing and reasoning abilities. They excel at var- ious vision-language tasks, such as image cap- tioning, visual question answering (VQA), and image-based dialogue, where they generate de- scriptive text based on visual inputs. For exam- ple, models such as LLaVA (Liu et al., 2023a) *Corresponding Authors. Number of time calling OpenAI api for 1 sentence Module usage Woodpecker 4 GroundDINO, BLIP2, QA2Claim LogicCheckGPT 6 LVLM (MPlug) Our 0 SGP, GroundDINO, BLIP2, CLIP Table 1: Statistics of Calling OpenAI API and Module usage of LVLM hallucination detections. and MiniGPT-4 (Zhu et al., 2023) can generate de- tailed captions from an image, answering complex questions about visual content. However, LVLMs face one critical challenge: object hallucination. This means LVLMs generate inconsistent or fab- ricated descriptions of a given image, such as in- venting non-existent objects. This challenge un- dermines the reliability and accuracy of LVLMs, severely limiting their broader applications. To address this challenge, some hallucination detec- tion methods have been proposed. For example, Woodpecker (Yin et al., 2023) is a training-free post-processing method that detects and corrects hallucinations in LVLMs. LogicCheckGPT (Wu et al., 2024a) is a plug-and-play framework that de- tects and mitigates object hallucinations in LVLMs by probing their logical consistency in responses. \"However, these methods have several limita- tions. First, they rely heavily on external API calls. As reported in Table 1, Woodpecker and Logic- CheckGPT make 4-6 API calls to detect each sen- tence. This dependence also limits offline usability, making deployment infeasible in scenarios with restricted API access or privacy concerns. Second, they require iterative interactions with LVLMs to detect hallucinations. For instance, LogicCheck- GPT generates multiple queries and performs con- sistency checks across responses. This iterative process not only increases latency but also ampli- fies token consumption, making it impractical for large-scale applications or real-time inference."
}