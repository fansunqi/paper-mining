{
  "code_links": [
    "https://github.com/rherdt185/concept-based-explanations-and-class-contrasting"
  ],
  "tasks": [
    "Explain deep neural network predictions",
    "Contrast between classes"
  ],
  "datasets": [
    "ImageNet1K",
    "pathology tissue images"
  ],
  "methods": [
    "Attribution methods (DeepLift with SmoothGrad)",
    "Non-negative matrix factorization (NMF)",
    "Linear classifier"
  ],
  "results": [
    "Model predicts desired class in 56.3% of cases (ResNet50, layer3.5)",
    "Model predicts desired class in 71.0% of cases (ResNet50, layer4.2)",
    "Model predicts desired class in 94.1% of cases (ResNet50, layer4.2, excluding images where the model predicts class 'A')"
  ],
  "title": "Concept Based Explanations and Class Contrasting.pdf",
  "abstract": "Explaining deep neural networks is challenging, due to their large size and non-linearity. In this paper, we introduce a concept-based explanation method, in order to explain the prediction for an individual class, as well as contrasting any two classes, i.e. explain why the model predicts one class over the other. We test it on several openly available classification models trained on Ima- geNet1K, as well as on a segmentation model trained to detect tumor in stained tissue samples. We perform both qualitative and quantitative tests. For example, for a ResNet50 model from pytorch model zoo, we can use the explanation for why the model predicts a class \u2019A\u2019 to automatically select six dataset crops where the model does not predict class \u2019A\u2019. The model then predicts class \u2019A\u2019 again for the newly combined image in 71% of the cases (works for 710 out of the 1000 classes).1 1. Introduction Interpreting deep neural networks is challenging. At the same time, the range of their use cases increases, also into more critical areas like medicine (Jansen et al., 2023). As the stakes of the application become higher, the need for interpretability increases. There is a wide range of existing work that focuses on the challenge of explaining such deep neural networks. Earlier methods employ heat maps, where the idea is to highlight pixels in the input that are important for the prediction of the model. Those are usually gradient based (like DeepLift (Shrikumar et al., 2019), Integrated Gradients (Sundarara- jan et al., 2017), Smooth Grad (Smilkov et al., 2017) or GradCAM (Selvaraju et al., 2017)), or pertubation based (like RISE (Petsiuk et al., 2018)). There are some problems 1Center for Industrial Mathematics, University of Bremen, Germany. Correspondence to: Rudolf Herdt <rherdt@uni- bremen.de>. 1The code including an .ipynb example is available on git: https://github.com/rherdt185/concept-based-explanations-and- class-contrasting with such heatmap based methods. Previous work raised concerns about the reliability of some gradient based meth- ods (Adebayo et al., 2018) and they may also not increase human understanding of the model (Kim et al., 2018). A more fundamental problem is while those heatmap methods show where the model is seeing something, they do not explain what the model is seeing there, which becomes a problem if what a human sees in the area highlighted by the heatmap does not align with what the model is seeing there (Fel et al., 2023). As a consequence, other methods emerged that focus on concept based explanation. (Olah et al., 2018) combined saliency maps used in hidden layers with visualizing the in- ternal activations at those layers (therefore combining where the model is seeing something, with what it is seeing there). TCAV (Kim et al., 2018) tests the importance of human defined concepts for the prediction of a specific class (e.g. how important the concept of stripes is for the prediction of the class zebra). ACE (Ghorbani et al., 2019) goes a step further, and extracts those concepts automatically from a dataset, removing the need for human defined concepts. CRAFT (Fel et al., 2023) takes this another step further, and allows to create concept based heatmaps explaining individ- ual images. Further CRAFT uses a human study to show the practical utility of the method. We also propose a concept based method, both for explain- ing a single class (like ACE or CRAFT), as well as con- trasting any two classes. Unlike ACE as well as CRAFT that both first extract the concepts, and later score them (ACE using TCAV and CRAFT using sobol indice), we first score the activations using attribution and afterwards extract the concepts (so we have no scoring after we extracted the concepts, we assume that all extracted concepts are relevant for the class). And we get a new set of concepts per class. Further, we do not conduct a human user study, but test the faithfulness of the explanation to the model. The idea for the test is that concept based methods extract concepts that are important for the prediction of a given class, and show them to the user in the form of one or more dataset examples (e.g. crops of dataset images). Then if we extract the n (in the following we always use six) most important concepts for a given class \u2019A\u2019, then combine the dataset examples for those n concepts and pass the resulting image into the model, it should predict class \u2019A\u2019 (i.e. we take the 1 arXiv:2502.03422v1 [cs.CV] 5 Feb 2025"
}