{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Inductive Generalization"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Model Successor Functions",
    "Peano\u2019s axioms",
    "Entropy",
    "Graceful Degradation",
    "Inductive Learnability"
  ],
  "results": [
    "Formal framework for inductive generalization",
    "Synthesis of existing learning paradigms",
    "Roadmap for future research"
  ],
  "title": "Model Successor Functions.pdf",
  "abstract": "The notion of generalization has moved away from the classical one defined in statistical learn- ing theory towards an emphasis on out-of-domain generalization (OODG). Recently, there is a grow- ing focus on inductive generalization, where a progression of difficulty implicitly governs the direction of domain shifts. In inductive general- ization, it is often assumed that the training data lie in the easier side, while the testing data lie in the harder side. The challenge is that training data are always finite, but a learner is expected to infer an inductive principle that could be ap- plied in an unbounded manner. This emerging regime has appeared in the literature under dif- ferent names, such as length/logical/algorithmic extrapolation, but a formal definition is lacking. This work provides such a formalization that cen- ters on the concept of model successors. Then we outline directions to adapt well-established tech- niques towards the learning of model successors. This work calls for restructuring of the research discussion around inductive generalization from fragmented task-centric communities to a more unified effort, focused on universal properties of learning and computation. 1. Introduction Children first learn to count one, two, three, or four objects as if they were separate instances (Sarnecka & Carey, 2008; Wynn, 1992). A transition typically occurs after a child learns counting up to four, when they begin to notice a gen- eralizable mapping from set sizes to numbers (Carey, 2011). This sharp transition corresponds to an inductive leap (Pi- antadosi et al., 2012), where a learner infers an inductive principle1 governing related tasks, and spontaneously ex- trapolates the principle. However, deep learning models do not exhibit an inductive leap and the correct extrapolation behavior in counting (Chang & Bisk, 2024). 1Language Technologies Institute, Carnegie Mellon University. 1Informally, the inductive principle of counting states that adding one object to a set increases the size by one (Rips et al., 2006; Margolis & Laurence, 2008). Apart from counting, many tasks share the same requirement for inductive generalization, on which poor extrapolation results from deep learning models have been reported. For example, compositional tasks require inferring production rules of a context-free grammar (CFG) (Kazemnejad et al., 2024; Lake & Baroni, 2018). Simulating a finite-state au- tomaton requires inferring the transition rules (Liu et al., 2022; Chi et al., 2023). Reasoning over graphs requires the induction of recursive programs. (Dziri et al., 2024; Zhang et al., 2023b; Veli\u02c7ckovi\u00b4c et al., 2022). Physical rea- soning requires uncovering physical principles that explain the relation among observations (Lerer et al., 2016; Lake et al., 2017). These problem spaces have underlying data generation rules whose output complexity can be quantified. In particular, they share a count variable N that matches the number of times the inductive step is unrolled from a base case. While the values of such count variables must be bounded given any finite training set, their range is un- bounded, so unseen (large) instances are likely to be ob- served at testing time (Xiao & Liu, 2024), which poses an extrapolation challenge. We call problems that share this characteristic inductive generalization problems. The ability to represent, infer and compute the inductive step is key to generalization, because N will easily shift out- of-domain. Attempts to tackle inductive generalization take place under different names, such as length generalization (Jelassi et al., 2023; Zhou et al., 2024; Dehghani et al., 2019; Hou et al., 2024; Xiao & Liu, 2024), iterative reasoning (Du et al., 2022), algorithmic extrapolation (Bansal et al., 2022), easy-to-hard generalization (Ding et al., 2024), deep think- ing (Schwarzschild et al., 2021; Schwarzschild, 2023) and upward generalization (Anil et al., 2022). Currently, all of these dispersed communities describe their goals generically as OODG (Hupkes et al., 2023; Ilievski et al., 2024). But there are certain aspects of OODG not well characterized by existing paradigms of generalization, including domain (Ye et al., 2021), compositional (Hupkes et al., 2020), and systematic (Bahdanau et al., 2019) generalization. What is missing is a concrete notion of difficulty progression. Empiricists have been developing bespoke deep learning models in various application areas that bear a resemblance to inductive generalization problems. This creates a need for establishing both a conceptual and a theoretical common ground that fosters discussions on sources of challenge and desiderata. This work aims precisely to bridge this gap. 1 arXiv:2502.00197v1 [cs.LG] 31 Jan 2025"
}