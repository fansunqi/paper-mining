{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Updating Survey Articles"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Qualitative Analysis",
    "Interviews",
    "Thematic Analysis"
  ],
  "results": [
    "None"
  ],
  "title": "Toward Living Narrative Reviews an Empirical Study of the Processes and Challenges in Updating Surve.pdf",
  "abstract": "Surveying prior literature to establish a foundation for new knowl- edge is essential for scholarly progress. However, survey articles are resource-intensive and challenging to create, and can quickly become outdated as new research is published, risking informa- tion staleness and inaccuracy. Keeping survey articles current with the latest evidence is therefore desirable, though there is a limited understanding of why, when, and how these surveys should be updated. Toward this end, through a series of in-depth retrospec- tive interviews with 11 researchers, we present an empirical ex- amination of the work practices in authoring and updating survey articles in computing research. We \ufb01nd that while computing re- searchers acknowledge the value in maintaining an updated sur- vey, continuous updating remains unmanageable and misaligned with academic incentives. Our \ufb01ndings suggest key leverage points within current work\ufb02ows that present opportunities for enabling technologies to facilitate more e\ufb03cient and e\ufb00ective updates. CCS Concepts \u2022 Human-centered computing \u2192Empirical studies in HCI. Keywords Living literature reviews, narrative reviews, scholarly research ACM Reference Format: Raymond Fok, Alexa Siu, and Daniel S. Weld. 2025. Toward Living Narra- tive Reviews: An Empirical Study of the Processes and Challenges in Updat- ing Survey Articles in Computing Research. In CHI Conference on Human Factors in Computing Systems (CHI \u201925), April 26-May 1, 2025, Yokohama, Japan. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3706598.3714047 1 Introduction Reviewing and building upon existing knowledge is fundamental to scholarly research. However, with the acceleration of research production and publication, keeping at the forefront of all available literature has become increasingly complex. Literature reviews of- fer tremendous value in synthesizing research, but are time-consuming and resource-intensive to create [24, 34]. Moreover, they quickly This work is licensed under a Creative Commons Attribution 4.0 International License. CHI \u201925, Yokohama, Japan \u00a9 2025 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-1394-1/25/04 https://doi.org/10.1145/3706598.3714047 become outdated as new research emerges [28]. One survival anal- ysis suggests some reviews may be outdated by the time of their publication and over 25% require updating just two years after pub- lication [29]. Such information staleness presents a threat to valid- ity, potentially under-informing researchers about research oppor- tunities or critically misleading decision-makers. In response, researchers have explored the concept of \u201cliving reviews\u201d\u2014documents that are continually updatedas new evidence emerges [8, 12, 40], typically centered around living systematic re- views. Maintaining living reviews can be challenging, however, with studies \ufb01nding many are never updated after initial publica- tion [11, 34]. Prior work on living reviews has primarily focused on accelerating literature discovery and appraisal [33, 36], but recent advances in arti\ufb01cial intelligence (AI) and large language models suggest an opportunity for supporting more complex cognitive as- pects of updating reviews, such as evidence synthesis [22, 38]. Such capabilities are particularly relevant for living narrative reviews, such as surveys in computing research, which rely more on expert interpretation and conceptual synthesis than the standardized pro- tocols and meta-analyses of systematic reviews [31]. To examine how narrative reviews are created and maintained in practice, we conductedsemi-structured, retrospective interviews with 11 survey authors across diverse areas of computing research. Authors re\ufb02ected on their processes and points of friction through- out their authoring and revision work\ufb02ows, and shared their per- spectives on when, how, and with what content these reviews should be updated. In understanding these practices, we sought to iden- tify opportunities to support the updating process, including the potential role of AI assistance. Our \ufb01ndings reveal the varied methodologies used and chal- lenges encountered when authoring and updating survey articles, especially in paper discovery, taxonomy development, and synthe- sis. We identify three key types of updates for maintaining narra- tive reviews: 1) empirical updates involving evidence and exam- ples, 2) structural updates to taxonomies and paper organization, and 3) interpretive updates to syntheses and framing. Each type o\ufb00ers distinct opportunities for AI assistance, from routine tasks like recalculating numerical values to more complex support for identifying potential biases and emerging research gaps. While au- thors saw the potential of AI to assist with routine tasks, they were skeptical of its ability to handle more nuanced interpretive tasks like constructing a compelling narrative. Moreover, the subjective, expertise-driven nature of their work\ufb02ows raises important con- siderations for future tools enabling living narrative reviews."
}