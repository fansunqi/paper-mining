{
  "code_links": [
    "None"
  ],
  "tasks": [
    "EEG-based generative AI"
  ],
  "datasets": [
    "Zuco",
    "Alice",
    "Envisioned Speech",
    "Alljoined",
    "ImageNet EEG",
    "DM-RE2I",
    "Texture Perception",
    "THINGS-EEG",
    "DCAE",
    "ThoughtViz",
    "OCED",
    "NMED-T",
    "NMED-H",
    "KARA ONE",
    "Japanese Speech EEG",
    "Phrase/Word Speech EEG"
  ],
  "methods": [
    "GANs",
    "VAEs",
    "Diffusion Models",
    "Contrastive Learning",
    "Attention Mechanism",
    "Masked Signal Modeling",
    "LSTM",
    "CNNs",
    "Transformer",
    "GRUs",
    "Bi-directional GRUs",
    "Hierarchical GRUs",
    "Masked Residual Attention Mechanism",
    "BART",
    "Mel-spectrograms",
    "CSP Filters"
  ],
  "results": [
    "None"
  ],
  "title": "A Survey on Bridging EEG Signals and Generative AI from Image and Text to Beyond.pdf",
  "abstract": "Integration of Brain-Computer Interfaces (BCIs) and Generative Artificial Intelligence (GenAI) has opened new frontiers in brain sig- nal decoding, enabling assistive communica- tion, neural representation learning, and mul- timodal integration. BCIs, particularly those leveraging Electroencephalography (EEG), pro- vide a non-invasive means of translating neu- ral activity into meaningful outputs. Recent advances in deep learning, including Gen- erative Adversarial Networks (GANs) and Transformer-based Large Language Models (LLMs), have significantly improved EEG- based generation of images, text, and speech. This paper provides a literature review of the state-of-the-art in EEG-based multimodal gen- eration, focusing on (i) EEG-to-image genera- tion through GANs, Variational Autoencoders (VAEs), and Diffusion Models, and (ii) EEG-to- text generation leveraging Transformer based language models and contrastive learning meth- ods. Additionally, we discuss the emerging do- main of EEG-to-speech synthesis, an evolving multimodal frontier. We highlight key datasets, use cases, challenges, and EEG feature en- coding methods that underpin generative ap- proaches. By providing a structured overview of EEG-based generative AI, this survey aims to equip researchers and practitioners with in- sights to advance neural decoding, enhance as- sistive technologies, and expand the frontiers of brain-computer interaction. 1 Introduction & Motivation The convergence of Brain-Computer Interfaces (BCIs) and Generative Artificial Intelligence (GenAI) is transforming human-computer interac- tion by enabling direct brain-to-device communi- cation. These advancements have enabled appli- cations in assistive communication for individuals with disabilities, cognitive neuroscience, mental health assessment, augmented reality (AR)/virtual reality (VR), and neural art generation. Electroen- cephalography (EEG), a widely used non-invasive neural recording technique, enables both passive and active Brain-Computer Interfaces (BCIs) and holds potential for applications in real-time adaptive human-computer interaction (Zander et al., 2010; Wolpaw and Boulay, 2010). Recent advancements in deep learning and generative mod- els have significantly improved the decoding of EEG signals, enabling the translation of neural ac- tivity into text, images, and speech. Specifically, Generative AI, including Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) and Transformers (Vaswani et al., 2017), has signifi- cantly advanced brain decoding, facilitating visual reconstruction, language generation, and speech synthesis (Bai et al., 2023; Srivastava and Shinde, 2020; Lee et al., 2023b). GANs improve cross- subject classification and EEG data augmentation (Song et al., 2021), while Transformer-based archi- tectures and multimodal deep learning frameworks (Liu et al., 2024a; Wang and Ji, 2022) enhance EEG-to-text translation and semantic decoding (Ali et al., 2024), pushing the boundaries of brain-signal interpretation. In light of recent breakthroughs in Generative AI, this survey provides a scope review of recent advancements in EEG-based generative AI, with a focus on two primary directions. The first explores how brain signals can be used to generate or re- construct visual stimuli, utilizing models such as GANs and Diffusion Models to decode perceptual representations. The second investigates the appli- cation of deep learning for EEG-to-text translation, where recurrent neural network and Transformers (Vaswani et al., 2017) based language models, and contrastive learning techniques play a crucial role in learning linguistic representation. The survey also examines emerging trends in speech decod- ing from EEG signals and multimodal integration considerations surrounding the use of generative AI for brain signal interpretation. Through this, we hope to provide a structured understanding of arXiv:2502.12048v1 [cs.AI] 17 Feb 2025"
}