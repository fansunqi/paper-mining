{
  "code_links": "None",
  "tasks": [
    "Adversarial Attacks on Multimodal LLMs"
  ],
  "datasets": [
    "SafeBench"
  ],
  "methods": [
    "Universal Adversarial Attack",
    "Gradient-based Optimization",
    "Cross-Model Generalization",
    "Multi-answer Attack"
  ],
  "results": [
    "Significantly higher attack success rates than existing baselines",
    "Cross-model generalization across different multimodal LLMs",
    "Multi-answer approach produces diverse but still malicious responses"
  ],
  "title": "Universal Adversarial Attack on Aligned Multimodal LLMs.pdf",
  "abstract": "We propose a universal adversarial attack on multimodal Large Language Models (LLMs) that leverages a single optimized image to over- ride alignment safeguards across diverse queries and even multiple models. By backpropagating through the vision encoder and language head, we craft a synthetic image that forces the model to respond with a targeted phrase (e.g., \u201dSure, here it is\u201d) or otherwise unsafe content\u2014even for harm- ful prompts. In experiments on the SafeBench benchmark, our method achieves significantly higher attack success rates than existing baselines, including text-only universal prompts (e.g., up to 93% on certain models). We further demonstrate cross-model transferability by training on sev- eral multimodal LLMs simultaneously and testing on unseen architectures. Additionally, a multi- answer variant of our approach produces more natural-sounding (yet still malicious) responses. These findings underscore critical vulnerabilities in current multimodal alignment and call for more robust adversarial defenses. We will release code and datasets under the Apache-2.0 license. Warning: some content generated by Multimodal LLMs in this paper may be offensive to some readers. 1. Introduction Adversarial attacks remain one of the most pressing concerns in modern artificial intelligence research. In general, an adversarial attack involves crafting mali- cious inputs\u2014often subtle, carefully designed perturba- tions\u2014capable of causing models to produce unintended or harmful outputs. Such attacks can lead to privacy breaches, 1AIRI 2MSU 3HSE University 4Skoltech. Correspondence to: Temurbek Rahmatullaev <raxtemur@gmail.com>. Proceedings of the 42 nd International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s). Figure 1. An example of a single universal adversarial image pro- ducing disallowed content. This image was originally optimized on three models (Phi, Qwen, and Llama) but is here tested on the Llava 1.5 7B demonstrating cross-model generalization. Despite safety alignment, the model yields an unsafe response for a harm- ful prompt. the generation of disallowed content, or even strategic ex- ploitation of a system\u2019s decision-making processes. Despite advances in alignment techniques (e.g., supervised fine- tuning and Reinforcement Learning from Human Feedback), Large Language Models (LLMs) still exhibit significant vul- nerability to these adversarial strategies. Extending these vulnerabilities to the multimodal setting raises additional risks. Multimodal LLMs, such as those combining vision and language capabilities, have recently achieved remarkable breakthroughs in visual-textual reason- ing and aligned content generation. However, even with robust safety measures and policy filters, these systems of- ten fail to withstand carefully crafted adversarial inputs. In particular, the mere presence of a specially optimized image can override safety filters, prompting the model to produce harmful or disallowed content. In this paper, we present a universal adversarial attack that leverages a single synthetic image to compromise mul- timodal LLMs across diverse prompts. By optimizing pixel-level perturbations through the model\u2019s entire vision- 1 arXiv:2502.07987v2 [cs.AI] 13 Feb 2025"
}