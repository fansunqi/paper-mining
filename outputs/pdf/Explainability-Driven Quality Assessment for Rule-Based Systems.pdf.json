{
  "code_links": [
    "https://github.com/brains-group/androidReasoningPunya"
  ],
  "tasks": [
    "Rule Quality",
    "Rule Debugging",
    "Knowledge-Based Reasoning",
    "Explainability in Reasoning",
    "Reasoning System Validation",
    "Human-Centered AI"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Explanation Framework",
    "Trace-Based Explanation",
    "Contextual Explanation",
    "Contrastive Explanation",
    "Counterfactual Explanation",
    "MIT App Inventor (Punya) Platform"
  ],
  "results": [
    "None"
  ],
  "title": "Explainability-Driven Quality Assessment for Rule-Based Systems.pdf",
  "abstract": "This paper introduces an explanation framework designed to en- hance the quality of rules in knowledge-based reasoning systems based on dataset-driven insights. The traditional method for rule induction from data typically requires labor-intensive labeling and data-driven learning. This framework provides an alternative and instead allows for the data-driven refinement of existing rules: it generates explanations of rule inferences and leverages human in- terpretation to refine rules. It leverages four complementary expla- nation types\u2014trace-based, contextual, contrastive, and counterfac- tual\u2014providing diverse perspectives for debugging, validating, and ultimately refining rules. By embedding explainability into the rea- soning architecture, the framework enables knowledge engineers to address inconsistencies, optimize thresholds, and ensure fairness, transparency, and interpretability in decision-making processes. Its practicality is demonstrated through a use case in finance. CCS Concepts \u2022 Computing methodologies \u2192Artificial intelligence; Learn- ing settings; Knowledge representation and reasoning; \u2022 In- formation systems; \u2022 Applied computing \u2192E-commerce infras- tructure; \u2022 Human-centered computing \u2192Human computer interaction (HCI); \u2022 Software and its engineering \u2192Software verification and validation; Keywords Rule Quality, Rule Debugging, Knowledge-Based Reasoning, Ex- plainability in Reasoning, Reasoning System Validation, Human- Centered AI 1 Introduction The reliability, utility, and trustworthiness of knowledge-based sys- tems, and, ultimately, the Web of Data, lies in the quality of the underlying logic-based rules. Rules that are inaccurate or incom- plete, or, in general, do not capture their intent, compromise the effectiveness of reasoning systems that rely on them. To improve rule quality in a data-driven way, machine learning techniques such as rule induction or decision trees can be used to extract rules from data. However, these methods typically require labeled datasets, which are labor-intensive to prepare; also, the output is a new set of rules that needs to be compared to the current ones. This paper instead introduces an explanation-driven framework for refining existing rules, which enhances reliability, transparency, and trust in reasoning systems without the effort of full-scale data labeling. To that end, the framework generates explanations of rule infer- ences, which can be interpreted by knowledge engineers to refine the existing rules. Our framework leverages four complementary explanation types (trace-based, contextual, contrastive, and coun- terfactual) to provide diverse perspectives for debugging, validating, and ultimately refining rules. These explanation types empower users to understand the derivation of conclusions, explore their immediate and upstream causes, compare alternative scenarios, and, in doing so, generate actionable insights for improvement. We integrated these explanation mechanisms into Punya [9, 10], a Semantic Web fork of the MIT App Inventor platform, which is a low-code platform for app development. This choice was made to support both technical and non-technical knowledge engineers in debugging and refining rules in a data-driven way. Punya, in particular, was chosen due to its built-in support for rule-based reasoning. The proposed explanation framework primarily benefits knowledge engineers; by enhancing rule transparency through ex- planations, the framework can also benefit end-users who interact with reasoning-driven applications. To the best of our knowledge, our work is among the first to explicitly investigate the use of explanations for refining rules in knowledge-based systems. The remainder of this paper is structured as follows. Section 2 discusses related work on rule quality and debugging, highlight- ing how our approach complements and extends prior efforts. In Section 3, we describe the reasoning architecture underpinning the explanation framework. Section 4 presents a compelling use case that benefits from our solution. Section 5 demonstrates the explanation types, illustrating their role in enhancing rule qual- ity. Section 6 outlines the integration of the framework into the MIT App Inventor (Punya) platform, emphasizing its features, light- weight implementation, and practical benefits. Finally, Section 7 and Section 8 discuss the implications of this work and conclude with future directions for improving data quality and explainability in knowledge-based systems. 2 Related Work The quality and debugging of rules in knowledge-based systems have been studied in several contexts, with efforts ranging from foundational correctness principles to advanced explanation frame- works. This section reviews key contributions and situates our work within this landscape. 2.1 Rule Quality Principles Landauer [6] propose a set of acceptability principles\u2014Consistency, Completeness, Irredundancy, Connectivity, and Distribution\u2014to guide the construction and validation of rule bases. These principles provide foundational criteria for evaluating rule systems, empha- sizing logical soundness, efficiency, and simplicity. In this vein, we focus on communicating the behavior of rules to knowledge engi- neers or end-users by employing user-centric explanations\u2014 trace- based, contextual, contrastive, and counterfactual\u2014to concretely arXiv:2502.01253v1 [cs.AI] 3 Feb 2025"
}