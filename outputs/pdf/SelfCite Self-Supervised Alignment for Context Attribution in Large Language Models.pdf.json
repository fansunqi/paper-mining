{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Generating responses with context attributions"
  ],
  "datasets": [
    "LongBench-Cite"
  ],
  "methods": [
    "Context ablation",
    "Best-of-N sampling",
    "Preference optimization"
  ],
  "results": [
    "Citation F1 score: 79.1 (best across datasets)",
    "Improvement of 5.3 points on LongBench-Cite benchmark"
  ],
  "title": "SelfCite Self-Supervised Alignment for Context Attribution in Large Language Models.pdf",
  "abstract": "Instead of only relying on costly and labor-intensive annotations, SelfCite leverages a reward signal provided by the LLM itself through context ablation: If a citation is necessary, removing the cited text from the context should prevent the same response; if sufficient, retaining the cited text alone should preserve the same response. This reward can guide the inference-time best-of-N sampling strategy to improve citation quality significantly, as well as be used in preference optimization to directly fine-tune the models for generating better citations. The effectiveness of SelfCite is demonstrated by increasing citation F1 up to 5.3 points on the LongBench-Cite benchmark across five long-form question answering tasks. Date: February 14, 2025 Correspondence: Yung-Sung Chuang yungsung@mit.edu, Shang-Wen Li shangwel@meta.com, Scott Yih scot- tyih@meta.com 1 Introduction Assistants built using large language models (LLMs) have become ubiquitous in helping users gather information and acquire knowledge (OpenAI, 2022, 2023). For instance, when asked about recent news, an assistant can read through dozens of relevant articles\u2014potentially more than a user could comb through themselves\u2014and use these articles as context to provide a clear, specific answer to the user\u2019s query. While this ability can greatly accelerate information gathering, LLMs often produce hallucinations\u2014content that sounds plausible but is actually fabricated (Ji et al., 2023). Even when provided with accurate context, models may misinterpret the data or include details that are not supported by the context (Shi et al., 2024; Chuang et al., 2024). Although completely eliminating hallucinations remains difficult, existing approaches have sought to en- hance the reliability of LLMs by providing context attributions\u2013commonly referred to as citations\u2013which are fine-grained references to relevant evidences from the context, alongside generated responses for user verification (Menick et al., 2022; Slobodkin et al., 2024; Zhang et al., 2024). While they have shown promise in generating citations, an outstanding challenge is their reliance on annotated data either from human (Menick et al., 2022; Slobodkin et al., 2024) or costly proprietary APIs (Zhang et al., 2024; Huang et al., 2024) to train models to generate citations. Collecting annotations can be time-consuming and costly, especially with long-context documents. To address this challenge, we introduce SelfCite, a novel alignment approach designed to autonomously enhance the quality of citations generated by LLMs without the need for any annotations in the alignment process. Drawing inspiration from model interpretability techniques (Lei et al., 2016; Cohen-Wang et al., 2024), SelfCite leverages the inherent capabilities of LLMs to provide feedback through context ablation\u2014a process to evaluate the necessity and sufficiency of a citation. If removing the cited text prevents the LLM from assigning high probability to the same response, we can infer that it is necessary for the LLM. Conversely, if the response remains highly probable despite removing all context other than the cited text, this indicates that the citation is sufficient for the LLM to make the claim. This self-evaluation mechanism enables SelfCite to calculate a reward signal without relying on the annotation processes. Building on this intuition, we design a reward that can be cheaply computed by the LLM itself, composed by 1"
}