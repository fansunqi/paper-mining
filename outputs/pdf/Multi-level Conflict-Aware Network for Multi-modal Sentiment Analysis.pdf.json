{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Multimodal Sentiment Analysis"
  ],
  "datasets": [
    "CMU-MOSI",
    "CMU-MOSEI"
  ],
  "methods": [
    "Micro Multi-step Interaction Network (Micro-MSIN)",
    "Macro Multi-step Interaction Network (Macro-MSIN)",
    "Micro Conflict-aware Cross-Attention (Micro-CACA)",
    "Macro Conflict-aware Cross Attention (Macro-CACA)"
  ],
  "results": [
    "MCAN significantly outperforms the current state-of-the-art methods"
  ],
  "title": "Multi-level Conflict-Aware Network for Multi-modal Sentiment Analysis.pdf",
  "abstract": "Multimodal Sentiment Analysis (MSA) aims to recognize human emotions by exploiting textual, acoustic, and visual modalities, and thus how to make full use of the interac- tions between different modalities is a central challenge of MSA. Interaction contains alignment and conflict aspects. Current works mainly emphasize alignment and the inher- ent differences between unimodal modalities, neglecting the fact that there are also potential conflicts between bimodal combinations. Additionally, multi-task learning-based con- flict modeling methods often rely on the unstable generated labels. To address these challenges, we propose a novel multi-level conflict-aware network (MCAN) for multimodal sentiment analysis, which progressively segregates alignment and conflict constituents from unimodal and bimodal repre- sentations, and further exploits the conflict constituents with the conflict modeling branch. In the conflict modeling branch, we conduct discrepancy constraints at both the representation and predicted output levels, avoiding dependence on the gen- erated labels. Experimental results on the CMU-MOSI and CMU-MOSEI datasets demonstrate the effectiveness of the proposed MCAN. Index Terms\u2014 Multimodal sentiment analysis; Multi- level alignment; Multi-level conflict modeling 1. INTRODUCTION In recent years, multimodal sentiment analysis (MSA) has attracted increasingly widespread attention [1, 2, 3, 4]. Be- cause of the heterogeneity among multimodal data, how to effectively fuse the representations of different modalities and ensure the semantic integrity of modalities is an impor- tant research topic in the community of MSA [5]. Some of the earlier works focus on the interaction between different modalities on low-level features, which results in limited fusion performance [1, 6, 7]. Inspired by the attention mech- anism\u2019s [8] high-level relationship modeling capabilities, in- creasing MSA methods introduced attention when fusing uni- modal representations. For example, Multimodal transformer (MulT) [2] employs the cross-modal attention mechanism to capture multimodal sequence interactions across different \u2217Yubo Gao and Haotian Wu contributed equally to this work \u2020Corresponding author. Email: zhlei@bjtu.edu.cn time steps. Some other works, such as Text Enhanced Trans- former Fusion Network (TETFN) [9], Fine-grained Tri-modal Interaction Model (FGTI) [4], multimodal 3D stereoscopic attention [10], etc. have also witnessed the success of the attention-based methods in MSA application. These methods fuse cross-modal features well but ignore the inherent information and potential conflicts of individual modalities, making the fused information somewhat incom- plete. Some studies have noted this problem, either mapping unimodal representations to modality-invariant and modality- specific spaces and modeling them separately subsequently for fusion [3, 11, 12], or leveraging the multi-task learning (MTL) framework to model inter-modal differences in a su- pervised learning mode through unimodal label generation [13, 14] or manual annotation [15]. However, these approaches still suffer from some short- comings. First, there is still a potential conflict between emo- tional information contained by different bimodal combina- tions. Considering only inter-unimodal differences is not suf- ficient. For example, the combination of a smiling expres- sion and a positive word is positive, whereas audio represents sarcasm. In this case, the combination of textual and visual modalities and the combination of textual and acoustic modal- ities would conflict with the emotional polarity. Secondly, for those methods based on MTL, manual annotation of unimodal labels is costly, whereas label generation methods [13, 14] rely on the quality of unimodal and cross-modal representa- tions, and binary partitioning of the representation center may suffer from insufficient granularity. To address these challenges, we propose a multi-level conflict-aware network (MCAN) that models consistency and discrepancy from different levels. Specifically, the MCAN is divided into the main branch and the conflict modeling branch. Wherein, the main branch progressively models the relationship between unimodal and bimodal representa- tions utilizing Micro Multi-step Interaction Network (Micro- MSIN) and Macro Multi-step Intersection Network (Macro- MSIN) and segregates the inter-unimodal and inter-bimodal conflict components hierarchically, then feeds them to the conflict modeling branch. The conflict modeling branch mod- els inter-unimodal and inter-bimodal conflicts through mi- cro conflict-aware cross-attention (Micro-CACA) and macro conflict-aware cross-attention (Macro-CACA), respectively. arXiv:2502.09675v1 [cs.CL] 13 Feb 2025"
}