{
  "code_links": [
    "None"
  ],
  "tasks": [
    "LLM-generated Text Detection",
    "Text Origin Classification",
    "Explanation Evaluation"
  ],
  "datasets": [
    "M4GT-Bench"
  ],
  "methods": [
    "Binary Classification",
    "Ternary Classification",
    "Human Annotation",
    "Fine-tuning"
  ],
  "results": [
    "Self-detection outperforms cross-detection",
    "Ternary classification improves detection and explanation",
    "F1 scores improved with ternary classification",
    "Explanation errors categorized into three types"
  ],
  "title": "I Know Myself Better, but Not Really Greatly Using LLMs to Detect and Explain LLM-Generated Texts.pdf",
  "abstract": "Large language models (LLMs) have demon- strated impressive capabilities in generating human-like texts, but the potential misuse of such LLM-generated texts raises the need to distinguish between human-generated and LLM-generated content. This paper explores the detection and explanation capabilities of LLM-based detectors of LLM-generated texts, in the context of a binary classification task (human-generated texts vs LLM-generated texts) and a ternary classification task (human- generated texts, LLM-generated texts, and un- decided). By evaluating on six close/open- source LLMs with different sizes, our findings reveal that while self-detection consistently out- performs cross-detection, i.e., LLMs can detect texts generated by themselves more accurately than those generated by other LLMs, the perfor- mance of self-detection is still far from ideal, in- dicating that further improvements are needed. We also show that extending the binary to the ternary classification task with a new class \u201cUn- decided\u201d can enhance both detection accuracy and explanation quality, with improvements being statistically significant and consistent across all LLMs. We finally conducted compre- hensive qualitative and quantitative analyses on the explanation errors, which are categorized into three types: reliance on inaccurate features (the most frequent error), hallucinations, and incorrect reasoning. These findings with our human-annotated dataset emphasize the need for further research into improving both self- detection and self-explanation, particularly to address overfitting issues that may hinder gen- eralization. 1 Introduction The rise of large language models (LLMs) has brought remarkable advancements in natural lan- *Equal contributions. \u2020Corresponding co-authors: ruizhe.li@abdn.ac.uk, s.j.li@kent.ac.uk guage processing (NLP) tasks (Matarazzo and Tor- lone, 2025), including text generation. Models such as GPT-4o (OpenAI, 2024), LLaMA (Tou- vron et al., 2023), and Qwen (Team, 2024) have blurred the boundaries between LLM-generated (LGTs) and human-generated texts (HGTs), pos- ing new challenges in distinguishing between the two. While these capabilities of LLMs open new possibilities, they also bring concerns in areas such as misinformation, academic dishonesty, and auto- mated content moderation (Hu, 2025). As a result, detecting LGTs has become an increasingly impor- tant research area (Dugan et al., 2024; Lee et al., 2023; Bhattacharjee and Liu, 2024a). Prior research has primarily focused on devel- oping classifiers to distinguish HGTs and LGTs, including open-source detectors (Hans et al., 2024) and online close-source detection systems (Tian et al., 2023). However, most detection sys- tems have been limited to binary classification, which has several inherent issues. Recently, some works (Lee et al., 2024b) have attempted ternary classification by introducing a \u201cmixed\u201d category, which represents texts originating from mixed sources. However, this approach does not funda- mentally resolve the issue. We further adopt the definition of an \u201cUndecided\u201d category based on other studies (Ji et al., 2024) and conduct ternary classification experiments for different LLMs, as certain texts are inherently indistinguishable be- tween LGTs and HGTs. Furthermore, many stud- ies treat the detection task as a black box, offering little insight into the decision-making process. Ex- plainability, a critical aspect of trustworthy AI, has received less attention, but it is essential for build- ing systems that users can trust (Weng et al., 2024; Zhou et al., 2024). This paper presents an analysis of LLMs in detecting LGTs and HGTs, with a par- ticular emphasis on evaluating and improving the clarity of the explanations provided by LLM-based detectors. By investigating how LLMs make pre- arXiv:2502.12743v1 [cs.CL] 18 Feb 2025"
}