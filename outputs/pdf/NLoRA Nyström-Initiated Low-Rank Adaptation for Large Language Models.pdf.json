{
  "code_links": [
    "https://github.com/TracyGuo2001/NLoRA"
  ],
  "tasks": [
    "Natural Language Generation",
    "Natural Language Understanding"
  ],
  "datasets": [
    "MetaMathQA",
    "GSM8K",
    "MATH",
    "HumanEval",
    "MBPP",
    "MT-Bench",
    "GLUE"
  ],
  "methods": [
    "StructuredLoRA (SLoRA)",
    "Nystr\u00f6mLoRA (NLoRA)",
    "IntermediateTune (IntTune)"
  ],
  "results": [
    "GSM8K: SLoRA: 56.48%, NLoRA: 57.70%",
    "NLoRA: Average NLG performance improvement of 7.45% over LoRA",
    "IntTune: 7.45% average NLG performance improvement over LoRA",
    "IntTune: 92.61% of LoRA\u2019s average performance across all NLU tasks"
  ],
  "title": "NLoRA Nystr\u00f6m-Initiated Low-Rank Adaptation for Large Language Models.pdf",
  "abstract": "Parameter-efficient fine-tuning (PEFT) is essen- tial for adapting large language models (LLMs), with low rank adaptation (LoRA) being the most popular approach. However, LoRA suf- fers from slow convergence, and some recent LoRA variants, such as PiSSA, primarily rely on Singular Value Decomposition (SVD) for initialization, leading to expensive computa- tion. To mitigate these problems, we resort to Nystr\u00f6m method, which follows a three-matrix manipulation. Therefore, we first introduce StructuredLoRA (SLoRA), investigating to in- troduce a small intermediate matrix between the low-rank matrices A and B. Secondly, we propose Nystr\u00f6mLoRA (NLoRA), which lever- ages Nystr\u00f6m-based initialization for SLoRA to improve its effectiveness and efficiency. Fi- nally, we propose IntermediateTune (IntTune) to explore fine-tuning exclusively the interme- diate matrix of NLoRA to furthermore boost LLMs\u2019 efficiency. We evaluate our methods on 5 natural language generation (NLG) tasks and 8 natural language understanding (NLU) tasks. On GSM8K, SLoRA and NLoRA achieve accuracies of 56.48% and 57.70%, surpass- ing LoRA by 33.52% and 36.41% with only 3.67M additional trainable parameters. IntTune boosts average NLG performance over LoRA by 7.45% while using only 1.25% of its param- eters. These results demonstrate the efficiency and effectiveness of our approach in enhanc- ing model performance with minimal parame- ter overhead. The code is available at https: //github.com/TracyGuo2001/NLoRA. 1 Introduction Fine-tuning large language models (LLMs) has emerged as a fundamental approach to enhancing model capabilities (Yu et al., 2023; Li et al., 2023; Xia et al., 2024) and aligning models with spe- cific application requirements (Zheng et al., 2023; Ouyang et al., 2022). However, the growing scale *Corresponding authors IntTune LoRA SLoRA NLoRA 0 100 200 300 Trainable Parameters (M) 0 20 40 60 GSM8K Accuracy (%) Trainable Params GSM8K Accuracy (%) Figure 1: The comparison among LoRA and our models of LLMs introduces significant challenges to LLM development, with fine-tuning requiring substan- tial computational and memory resources (Hu et al., 2021; Chang et al., 2024). For example, fine-tuning a LLaMA-65B model requires more than 780 GB of GPU memory (Dettmers et al., 2023), while training GPT-3 175B requires 1.2 TB of VRAM (Hu et al., 2021). Such resource-intensive pro- cesses are infeasible for many researchers and in- stitutions, driving the development of parameter- efficient fine-tuning (PEFT) methods. Among these methods, Low-Rank Adaptation (LoRA) (Hu et al., 2021) has received widespread attention due to its ability to achieve competitive performance com- pared to full parameter fine-tuning, while signifi- cantly reducing memory consumption and avoiding additional inference latency. LoRA enables the indirect training of dense lay- ers in a neural network by optimizing low-rank decomposition matrices that represent changes in the dense layers during adaptation, all while keep- ing the pre-trained weights fixed. For a pre-trained weight matrix W \u2208Rm\u00d7n, LoRA introduces a low-rank decomposition \u2206W = AB, where A \u2208 Rm\u00d7r, B \u2208Rr\u00d7n, and the rank r \u226amin(m, n). This modifies the forward pass of a layer as fol- lows: Y = X(W + \u2206W) = X(W + AB), (1) where X \u2208Rb\u00d7m, Y \u2208Rb\u00d7n, and b represents 1 arXiv:2502.14482v1 [cs.CL] 20 Feb 2025"
}