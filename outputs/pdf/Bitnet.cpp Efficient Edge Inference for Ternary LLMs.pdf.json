{
  "code_links": [
    "https://github.com/microsoft/BitNet/tree/paper"
  ],
  "tasks": [
    "Edge Inference for Ternary LLMs"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "mpGEMM",
    "Ternary Lookup Table (TL)",
    "Int2 with a Scale (I2_S)"
  ],
  "results": [
    "Up to 6.25x speedup over full-precision baselines",
    "Up to 2.32x over low-bit baselines"
  ],
  "title": "Bitnet.cpp Efficient Edge Inference for Ternary LLMs.pdf",
  "abstract": "The advent of 1-bit large language models (LLMs), led by BitNet b1.58, has spurred inter- est in ternary LLMs. Despite this, research and practical applications focusing on efficient edge inference for ternary LLMs remain scarce. To bridge this gap, we introduce Bitnet.cpp, an in- ference system optimized for BitNet b1.58 and ternary LLMs. Given that mixed-precision ma- trix multiplication (mpGEMM) constitutes the bulk of inference time in ternary LLMs, Bit- net.cpp incorporates a novel mpGEMM library to facilitate sub-2-bits-per-weight, efficient and lossless inference. The library features two core solutions: Ternary Lookup Table (TL), which addresses spatial inefficiencies of previ- ous bit-wise methods, and Int2 with a Scale (I2_S), which ensures lossless edge inference, both enabling high-speed inference. Our exper- iments show that Bitnet.cpp achieves up to a 6.25x increase in speed over full-precision base- lines and up to 2.32x over low-bit baselines, setting new benchmarks in the field. Addition- ally, we expand TL to element-wise lookup ta- ble (ELUT) for low-bit LLMs in the appendix, presenting both theoretical and empirical evi- dence of its considerable potential. Bitnet.cpp is publicly available at https://github.com/ microsoft/BitNet/tree/paper, offering a sophisticated solution for the efficient and prac- tical deployment of edge LLMs. 1 Introduction In recent years, large language models have gar- nered widespread attention due to their exceptional performance across a variety of tasks. However, the growing demand for efficient deployment on edge devices, particularly driven by data privacy concerns, poses challenges due to the limited com- putational power and bandwidth of these devices. To address these challenges, model compression techniques are frequently employed. Notable ex- amples benefiting from such techniques include llama.cpp llama.cpp T-MAC Bitnet.cpp 0 1 2 3 4 5 6 7 Inference Speed (tokens / second) Float16(b16) TQ1_0(b1.69) (b2) TL2_0(b1.67) N/A Intel i7-13700H Apple M2 Ultra Figure 1: A comparison of end-to-end inference speeds on a 100B ternary LLM. (bx) represents the bits per weight, where x denotes specific value. \"N/A\" indicates that the tested CPU cannot host the specified model size with the given kernel. Gemini-Nano (Team et al., 2024) and Phi3-mini (Abdin et al., 2024), both designed for mobile and personal devices. Furthermore, recent advance- ments by BitNet b1.58 (Wang et al., 2023; Ma et al., 2024) represent a significant development in edge LLM inference, introducing 1-bit LLMs by quantiz- ing all weights to ternary values therefore reducing the bits per weight (bpw) to 1.58, while preserving accuracy comparable to full-precision LLMs. Sub- sequent releases of ternary LLMs, including TriLM (Kaushal et al., 2024), Llama3-8B-1.58 (Mekkouri et al., 2024), and BitNet a4.8 (Wang et al., 2024a), have demonstrated the effectiveness and applicabil- ity of ternary architectures, thereby extending the boundaries of the 1-bit era. Despite the burgeoning interest in ternary LLMs, the conversion of their theoretical benefits into practical advantages during inference is still understudied. To fill this gap, we aim to enhance the perfor- mance of ternary LLMs edge inference by optimiz- ing mpGEMM (e.g., 8-bit activation and 1.58-bit weight). However, the non-integer bpw charac- 1 arXiv:2502.11880v1 [cs.LG] 17 Feb 2025"
}