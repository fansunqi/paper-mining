{
  "code_links": [
    "https://github.com/Zanette-Labs/efficient-reasoning"
  ],
  "tasks": [
    "Reasoning"
  ],
  "datasets": [
    "Numina Math"
  ],
  "methods": [
    "Reinforcement Learning",
    "Policy Gradient",
    "RLOO Estimator"
  ],
  "results": [
    "Reduction in inference cost by 16% on AIME 2024",
    "Reduction in inference cost by 30% on MATH",
    "Reduction in inference cost by 50% on GSM8K"
  ],
  "title": "Training Language Models to Reason Efficiently.pdf",
  "abstract": "Scaling model size and training data has led to great advances in the performance of Large Lan- guage Models (LLMs). However, the diminishing returns of this approach necessitate alternative methods to improve model capabilities, particu- larly in tasks requiring advanced reasoning. Large reasoning models, which leverage long chain-of- thoughts, bring unprecedented breakthroughs in problem-solving capabilities but at a substantial deployment cost associated to longer generations. Reducing inference costs is crucial for the eco- nomic feasibility, user experience, and environ- mental sustainability of these models. In this work, we propose to train large reason- ing models to reason efficiently. More precisely, we use reinforcement learning (RL) to train rea- soning models to dynamically allocate inference- time compute based on task complexity. Our method incentivizes models to minimize unnec- essary computational overhead while maintaining accuracy, thereby achieving substantial efficiency gains. It enables the derivation of a family of reasoning models with varying efficiency levels, controlled via a single hyperparameter. Experi- ments on two open-weight large reasoning mod- els demonstrate significant reductions in inference cost while preserving most of the accuracy. 1. Introduction Large language models (LLMs) have made significant ad- vancements by pre-training larger models with extensive datasets (Kaplan et al., 2020), but this approach faces di- minishing returns due to limited high-quality training data. An alternative to improve model capabilities, especially in domains involving careful reasoning, involves allowing models to \u201cthink\u201d before answering, as seen in frontier rea- soning models like OpenAI\u2019s o1, Gemini 2.0 Flash Thinking 1Carnegie Mellon University. Correspondence to: Daman Arora <damana@andrew.cmu.edu>, Andrea Zanette <zanette@cmu.edu>. Experimental, and DeepSeek-R1 (Guo et al., 2025). These models produce intermediate tokens during inference, col- lectively referred to as chain-of-thoughts (Wei et al., 2022), to perform additional computations before returning an an- swer. The process of generating a long chain of thought before answering the user query is called reasoning. More precisely, large reasoning models with chain-of-thoughts capable of performing advanced reasoning emerge from re- inforcement learning (RL) (Sutton & Barto, 2018; Guo et al., 2025) on base models using ground-truth scoring functions (e.g., correctness on math problems). These reasoning models use test-time compute in the form of very long chain-of-thoughts, an approach that commands a high inference cost due to the quadratic cost of the at- tention mechanism and linear growth of the KV cache for transformer-based architectures (Vaswani, 2017). However, effective deployment of LLMs demands models that are not only accurate but also computationally efficient to serve. Even for resource-rich organizations such as large tech com- panies that have the resources to train reasoning models, ex- cessive inference costs may mean operating at a loss rather than at a profit in order to match the competitor\u2019s offering. Furthermore, reducing inference costs often reduces latency, improves responsiveness, and therefore increases user expe- rience. Finally, lowering the inference computation has a direct impact in reducing carbon emissions, with a positive benefit to both the environment and the society. We aim to develop a procedure to train the model to use the appropriate amount of inference time compute to solve the problem at hand with reasoning. For straightforward problems, the resulting model would deliver efficient, direct solutions, while for more demanding tasks, it would invest additional computational effort to perform advanced reason- ing. Such an adaptable model, that invests the minimum amount of compute to arrive at the correct solution, would be a significant leap forward in terms of operational cost. We use reinforcement learning policy gradient methods (Sut- ton & Barto, 2018) to train the model to use the least possible amount of tokens to reach the correct solution, thereby min- imizing inference costs, ideally without compromising on accuracy. We achieve this goal by means of a modified rein- forcement learning formulation which encourages the model to produce correct answers with short chain-of-thoughts. To 1 arXiv:2502.04463v1 [cs.LG] 6 Feb 2025"
}