{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Hallucination Detection",
    "Entity-Level Hallucination Detection"
  ],
  "datasets": [
    "HALLUENTITY"
  ],
  "methods": [
    "Uncertainty Estimation",
    "Token-Level Likelihood",
    "Entropy Scores",
    "Context-Aware Approaches",
    "Claim-Conditioned Probability",
    "Shifting Attention to Relevance",
    "Focus"
  ],
  "results": [
    "Token-Level Uncertainty Estimation Tends to Over-Predict Hallucinations",
    "Context-Aware Approaches Show Better Performance",
    "Model Family and Size Have Limited Impact",
    "Linguistic Properties and Entity Types Affect Hallucination Tendencies"
  ],
  "title": "Can Your Uncertainty Scores Detect Hallucinated Entity.pdf",
  "abstract": "To mitigate the impact of hallucination nature of LLMs, many studies propose detecting hal- lucinated generation through uncertainty esti- mation. However, these approaches predom- inantly operate at the sentence or paragraph level, failing to pinpoint specific spans or enti- ties responsible for hallucinated content. This lack of granularity is especially problematic for long-form outputs that mix accurate and fabri- cated information. To address this limitation, we explore entity-level hallucination detection. We propose a new data set, HALLUENTITY, which annotates hallucination at the entity level. Based on the dataset, we comprehensively eval- uate uncertainty-based hallucination detection approaches across 17 modern LLMs. Our ex- perimental results show that uncertainty esti- mation approaches focusing on individual to- ken probabilities tend to over-predict hallucina- tions, while context-aware methods show bet- ter but still suboptimal performance. Through an in-depth qualitative study, we identify rela- tionships between hallucination tendencies and linguistic properties and highlight important directions for future research. 1 Introduction How can we trust the facts generated by large lan- guage models (LLMs) when even a single halluci- nated entity can distort an entire narrative? While LLMs have revolutionized text generation in var- ious domains, from summarization to scientific writing (Liang et al., 2024), their tendency to pro- duce hallucinations\u2014factually incorrect or unsup- ported content\u2014remains a critical challenge (Xu et al., 2024). This issue is particularly concern- ing in high-stakes applications, such as medical diagnostics (Chen et al., 2024b), legal document drafting (Lin and Cheng, 2024), and news genera- tion (Odaba\u00b8s\u0131 and Biricik, 2025), where inaccurate information can cause harm to individuals and ero- sion of public trust. Detecting hallucinations is therefore a critical step toward ensuring the respon- sible deployment of LLMs. Various approaches have been proposed to detect hallucinations (Luo et al., 2024), with uncertainty-based methods emerging as a promis- ing direction (Zhang et al., 2023a). However, cur- rent uncertainty-based hallucination detection ap- proaches mainly operate at the sentence or para- graph level, classifying the entire generation as either hallucinated or correct. While this provides a coarse-grained assessment of factuality, it lacks the granularity needed to pinpoint which specific spans or entities contribute to hallucination. This limitation is particularly problematic for long-form text, where both accurate and hallucinated informa- tion frequently coexist. For example, a generated response about a historical event might correctly state the date but fabricate details about the individ- uals involved, necessitating finer-grained detection. To address these limitations, we present a first systematic exploration of entity-level hallucina- tion detection by introducing a benchmark dataset, extensively evaluating uncertainty-based detection methods on this benchmark, and analyzing their strengths and limitations in identifying hallucinated entities. Specifically, we begin by constructing a benchmark for entity-level hallucination detection, HALLUENTITY, which fills in the critical blank for the field. Constructing such a dataset is chal- lenging due to the labor-intensive nature of entity- level annotation, which requires annotators to seg- ment meaningful entities and verify their factuality against reliable sources one by one. To overcome this, we develop a systematic pipeline that maps atomic facts from model-generated text to entity- level annotations, enabling structured hallucination detection at a finer granularity. HALLUENTITY en- compasses 18,785 annotated entities, and provides a foundation for evaluating hallucination detection methods with greater interpretability and precision. Building on HALLUENTITY, we comprehen- arXiv:2502.11948v1 [cs.CL] 17 Feb 2025"
}