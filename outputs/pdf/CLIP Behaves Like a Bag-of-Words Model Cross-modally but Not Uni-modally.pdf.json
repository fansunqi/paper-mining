{
  "code_links": [
    "https://github.com/kdariina/CLIP-not-BoW-unimodally"
  ],
  "tasks": [
    "Cross-modal alignment",
    "Attribute-object binding",
    "Compositional understanding"
  ],
  "datasets": [
    "CLEVR",
    "PUG:SPAR",
    "PUG:SPARE",
    "COCO",
    "ARO",
    "SugarCrepe"
  ],
  "methods": [
    "Linear probing",
    "Linear transformation",
    "Contrastive learning",
    "Negative samples"
  ],
  "results": [
    "Improved attribute-object binding",
    "Enhanced compositional understanding",
    "Lower modality gap"
  ],
  "title": "CLIP Behaves Like a Bag-of-Words Model Cross-modally but Not Uni-modally.pdf",
  "abstract": "CLIP (Contrastive Language-Image Pretraining) has be- come a popular choice for various downstream tasks. How- ever, recent studies have questioned its ability to repre- sent compositional concepts effectively. These works sug- gest that CLIP often acts like a bag-of-words (BoW) model, interpreting images and text as sets of individual concepts without grasping the structural relationships. In particular, CLIP struggles to correctly bind attributes to their corre- sponding objects when multiple objects are present in an image or text. In this work, we investigate why CLIP exhibits this BoW-like behavior. We find that the correct attribute- object binding information is already present in individ- ual text and image modalities. Instead, the issue lies in the cross-modal alignment, which relies on cosine similarity. To address this, we propose Linear Attribute Binding CLIP or LABCLIP. It applies a linear transformation to text em- beddings before computing cosine similarity. This approach significantly improves CLIP\u2019s ability to bind attributes to correct objects, thereby enhancing its compositional under- standing. The code is available at https://github.com/ kdariina/CLIP-not-BoW-unimodally. 1. Introduction Vision-language models (VLMs) like Contrastive Language-Image Pretraining (CLIP) [24] have achieved widespread adoption due to their shared embedding space for text and image modalities, which enables strong performance on various downstream tasks. However, a fundamental limitation has emerged: CLIP often struggles with compositionality [28], specifically the ability to bind attributes to corresponding objects in complex scenes [16, 27, 31]. Compositionality is essential for VLMs, as it allows models to generalize effectively by combining simpler concepts and understanding their relations. Recent studies [31] have shown that CLIP frequently be- haves like a bag-of-words (BoW) model, failing to bind at- tributes to corresponding objects. For instance, when pre- Figure 1. LABCLIP mitigates the BoW behavior of CLIP. (1) It has been reported that CLIP behaves like a BoW model with weak attribute-object binding. (2) We discover that embeddings of individual image and text modalities already contain the attribute- object binding information; this suggests that the cross-modal BoWness stems from the lack of alignment across the modalities. (3) A simple linear transformation of the text modality effectively mitigates the BoWness of CLIP. sented with an image of \u201can orange square and a blue tri- angle\u201d as in Figure 1, CLIP often matches the image to a caption \u201ca blue square and an orange triangle\u201d. It is often unable to distinguish the structural difference. We refer to this phenomenon as BoWness, indicating the model\u2019s treat- ment of each data point as an unordered set of concepts. The BoWness significantly limits CLIP\u2019s compositional under- standing. Previous research has evaluated this limitation by jointly considering the image and text embeddings. How- ever, there has been little investigation into the source of the inability. In particular, we do not know whether the BoW- ness arises (1) from a lack of attribute-object binding in- formation in the individual text and image embeddings or 1 arXiv:2502.03566v2 [cs.CV] 8 Feb 2025"
}