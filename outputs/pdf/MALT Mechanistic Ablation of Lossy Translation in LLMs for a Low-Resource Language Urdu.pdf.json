{
  "code_links": [
    "https://github.com/taaha/MALT"
  ],
  "tasks": [
    "Low-Resource Language Processing"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Mechanistic Ablation of Lossy Translation"
  ],
  "results": [
    "Improved LLM performance on low-resource languages",
    "Preservation of cultural nuances"
  ],
  "title": "MALT Mechanistic Ablation of Lossy Translation in LLMs for a Low-Resource Language Urdu.pdf",
  "abstract": "LLMs are predominantly trained on English data, which leads to a significant drop in per- formance on low-resource languages. Under- standing how LLMs handle these languages is crucial for improving their effectiveness. This study focuses on Urdu as a use case for explor- ing the challenges faced by LLMs in processing low-resource languages. LLMs primarily rea- son in English when prompted in another lan- guage, with the final layers acting as translators to convert the English response into the target language. This study finds that even for low- resource languages, the internal latent response of LLMs in English is quite coherent; how- ever, the translation features are lossy and re- sult in poor translations, leading to reduced per- formance. By mechanistically removing these translation features and using a separate trans- lation model to translate the LLM\u2019s internal latent response, the performance of LLMs im- proves significantly while also preserving the cultural nuances of the input in low-resource languages. 1 1 Introduction Most Large Language Models (LLMs) are trained on English-dominant corpora. Even multilingual LLMs, like Llama 3, contain only around 5% non- English data (AI@Meta, 2024), which results in a significant performance gap across different lan- guages. Furthermore, this multilingual capability is generally focused on high-resource languages such as French and German, leaving limited support for low-resource languages. This greatly restricts universal AI accessibility. For example, Urdu, a low-resource language, is spoken by 230 million people (Hussain and Hussain, 2022), yet receives 1Results along with code are available here https:// github.com/taaha/MALT minimal representation in LLM training data. Ad- dressing these gaps is important to provide fairer AI access for people who speak different languages around the world. Previous studies have suggested that, due to their English-centric training datasets, LLMs use En- glish as their latent language even when prompted in another language (Wendler et al., 2024). It is only in the final layers that LLMs translate the re- sponse from this latent language into the language of the input prompt. The capability of LLMs to process non-English languages relies mostly on a very small number of neurons located in the final and initial layers (Tang et al., 2024), which are pri- marily involved in translating the prompt to and from the latent language. This study aims to rein- force this hypothesis for low-resource languages and address the translation losses that are particu- larly significant for these languages. Mechanistically erasing a direction within a layer has been shown to alter the performance of LLMs (Arditi et al., 2024). Additionally, studies have demonstrated that by modifying language- specific neurons, it is possible to control target lan- guage generation (Kojima et al., 2024). Building on this, this study shows that for smaller LLMs, regardless of the task or prompt, low-resource lan- guage generation is primarily mediated by a single direction in the final layers, which can be easily removed mechanistically. By mechanistically removing the translation fea- tures in the final layer of the LLM, internal latent responses in English are obtained that are gener- ally more coherent than the outputs generated by the unedited LLM in the target language. These responses can then be translated into low-resource languages using state-of-the-art (SOTA) machine translation models. This highlights a key insight into how LLMs process low-resource languages: LLMs are effective at understanding these lan- guages but struggle to generate coherent responses 1 arXiv:2502.00041v1 [cs.CL] 27 Jan 2025"
}