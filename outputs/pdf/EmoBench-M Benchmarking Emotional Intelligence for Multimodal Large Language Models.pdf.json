{
  "code_links": [
    "https://github.com/fansunqi/AKeyS"
  ],
  "tasks": [
    "Emotional Intelligence",
    "Multimodal Emotion Recognition",
    "Conversational Emotion Understanding",
    "Socially Complex Emotion Analysis"
  ],
  "datasets": [
    "RAVDESS",
    "CMU-MOSI",
    "CMU-MOSEI",
    "FMSA-SC",
    "MER2023",
    "CH-SIMSv2",
    "CH-SIMS",
    "MC-EIU",
    "MELD",
    "UR-FUNNY",
    "MUStARD",
    "SMILE"
  ],
  "methods": [
    "Multimodal Large Language Models",
    "Language Agents",
    "Search Algorithms",
    "Cost Function Evaluation",
    "Termination Condition Assessment"
  ],
  "results": [
    "Accuracy",
    "Weighted Average F-score",
    "Cosine Similarity",
    "Pearson Correlation"
  ],
  "title": "EmoBench-M Benchmarking Emotional Intelligence for Multimodal Large Language Models.pdf",
  "abstract": "With the integration of Multimodal large lan- guage models (MLLMs) into robotic systems and various AI applications, embedding emo- tional intelligence (EI) capabilities into these models is essential for enabling robots to ef- fectively address human emotional needs and interact seamlessly in real-world scenarios. Ex- isting static, text-based, or text-image bench- marks overlook the multimodal complexities of real-world interactions and fail to capture the dynamic, multimodal nature of emotional expressions, making them inadequate for eval- uating MLLMs\u2019 EI. Based on established psy- chological theories of EI, we build EmoBench- M, a novel benchmark designed to evaluate the EI capability of MLLMs across 13 val- uation scenarios from three key dimensions: foundational emotion recognition, conversa- tional emotion understanding, and socially complex emotion analysis. Evaluations of both open-source and closed-source MLLMs *Equal Contribution. \u2020Corresponding Authors. on EmoBench-M reveal a significant perfor- mance gap between them and humans, high- lighting the need to further advance their EI capabilities. All benchmark resources, includ- ing code and datasets, are publicly available at https://emo-gml.github.io/. 1 Introduction Emotional Intelligence (EI), initially conceptual- ized by Salovey and Mayer (1990), emphasizes the ability to perceive, understand, regulate, and apply emotions in oneself and others. Recent ad- vancements in multimodal large language mod- els (MLLMs) have significantly improved human- computer interaction and natural language under- standing, and integrating MLLMs into robotic con- trol systems has become increasingly prevalent (Sartor and Thompson, 2024). Incorporating EI ca- pabilities within MLLMs is essential for improving robotic performance in real-world environments. It will enable robots to address human emotional needs better and ensure more effective interactions. However, there is currently no universal bench- 1 arXiv:2502.04424v1 [cs.CL] 6 Feb 2025"
}