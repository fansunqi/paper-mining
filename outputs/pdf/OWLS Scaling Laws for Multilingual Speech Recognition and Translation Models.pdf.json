{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Multilingual Speech Recognition",
    "Speech Translation"
  ],
  "datasets": [
    "FLEURS",
    "OWSM v3.2",
    "YODAS",
    "Siminchik"
  ],
  "methods": [
    "Transformer",
    "CTC/attention loss",
    "Whisper-style training",
    "in-context learning"
  ],
  "results": [
    "WER: 7.7% on FLEURS",
    "BLEU: 28.9 on English to German",
    "CER: 18.7 on Taiwanese Mandarin"
  ],
  "title": "OWLS Scaling Laws for Multilingual Speech Recognition and Translation Models.pdf",
  "abstract": "Neural scaling laws offer valuable insights for designing robust sequence processing architec- tures. While these laws have been extensively characterized in other modalities, their behavior in speech remains comparatively underexplored. In this work, we introduce OWLS, an open-access, reproducible suite of multilingual speech recog- nition and translation models spanning 0.25B to 18B parameters, with the 18B version being the largest speech model, to the best of our knowl- edge. OWLS leverages up to 360K hours of pub- lic speech data across 150 languages, enabling a systematic investigation into how data, model, and compute scaling each influence performance in multilingual speech tasks. We use OWLS to derive neural scaling laws, showing how final per- formance can be reliably predicted when scaling. One of our key findings is that scaling enhances performance on low-resource languages/dialects, helping to mitigate bias and improve the accessi- bility of speech technologies. Finally, we show how OWLS can be used to power new research directions by discovering emergent abilities in large-scale speech models. Model checkpoints will be released on huggingface for future studies. 1. Introduction Neural acoustic models have shown robust performance in processing human speech information and have demon- strated remarkable capabilities in spoken language tasks (Radford et al., 2023; Peng et al., 2023b; Barrault et al., 2023a). Powered by large-scale training (Baevski et al., 2020; Zhang et al., 2023; Chen et al., 2024; 2022; Li et al., 2021), Transformer-based (Vaswani et al., 2017) models have dominated the fields of Automatic Speech Recognition 1Carnegie Mellon University 2NVIDIA. Correspondence to: William Chen <williamchen@cmu.edu>, Chao-Han Huck Yang <hucky@nvidia.com>, Shinji Watanabe <shinjiw@ieee.org>. Preprint. Copyright 2025 by the author(s). 108 109 1010 Trainable Params. of Multilingual Speech Models 104 105 106 Total Training Hours Whisper OWSM OWSM v3.1 \"OWLS\" Canary Figure 1. Comparison of previous open models and our OWLS models (blue) by parameter count and training dataset size. Whisper (Radford et al., 2023) and Canary (Puvvada et al., 2024) are trained on undisclosed data, while OWSM (Peng et al., 2023b) and the presented OWLS use public data. (ASR) and Speech Translation (ST). The state-of-the-art (SOTA) in ASR/ST has now progressed to not only scaling in terms of model and data size, but also tasks and languages. In recent years, there has been signifi- cant interest in developing massively multilingual models that can perform ASR/ST for hundreds, if not thousands, of diverse spoken languages (Chen et al., 2023b; Pratap et al., 2023; Babu et al., 2022; Yu et al., 2023; Chen et al., 2024; Zhang et al., 2023), with the goal of having a single model that can universally convert multilingual speech into text. However, the architecture of these massively multilingual models is complex, and their scaling properties pose signifi- cant challenges for both experimental designs in advancing speech science. This challenge is further exacerbated by the multi-modal nature of spoken language systems, which must handle the complexities of both multilingual text and speech. Prior art on the scaling laws of neural models devi- ates significantly from the goal of SOTA universal systems. The majority study single-task and single-modality systems (Biderman et al., 2023; Ghorbani et al., 2022; Zheng et al., 2022), while multilingual work concentrates only on set- tings where a few languages are supported (Fernandes et al., 2023; Yang et al., 2023; Li et al., 2021). To address this, we present OWLS, a Open Whisper-style Large-scale neural model Suite for Speech Recognition and 1 arXiv:2502.10373v1 [cs.CL] 14 Feb 2025"
}