{
  "code_links": [
    "https://github.com/alaaNfissi/SigWavNet-Learning-Multiresolution-Signal-Wavelet-Network-for-Speech-Emotion-Recognition"
  ],
  "tasks": [
    "Speech Emotion Recognition"
  ],
  "datasets": [
    "IEMOCAP",
    "EMO-DB"
  ],
  "methods": [
    "Learnable FDWT",
    "Conjugate quadrature filter",
    "Learnable Asymmetric Hard Thresholding",
    "1D dilated CNN",
    "Bi-GRU",
    "Temporal attention"
  ],
  "results": [
    "Accuracy: 84.8% on IEMOCAP",
    "Accuracy: 90.1% on EMO-DB"
  ],
  "title": "SigWavNet Learning Multiresolution Signal Wavelet Network for Speech Emotion Recognition.pdf",
  "abstract": "\u2014In the field of human-computer interaction and psychological assessment, speech emotion recognition (SER) plays an important role in deciphering emotional states from speech signals. Despite advancements, challenges persist due to system complexity, feature distinctiveness issues, and noise interference. This paper introduces a new end-to-end (E2E) deep learning multi-resolution framework for SER, addressing these limitations by extracting meaningful representations directly from raw waveform speech signals. By leveraging the properties of the fast discrete wavelet transform (FDWT), including the cascade algorithm, conjugate quadrature filter, and coefficient denoising, our approach introduces a learnable model for both wavelet bases and denoising through deep learning techniques. The framework incorporates an activation function for learnable asymmetric hard thresholding of wavelet coefficients. Our approach exploits the capabilities of wavelets for effective localization in both time and frequency domains. We then combine one-dimensional dilated convolutional neural networks (1D dilated CNN) with a spatial attention layer and bidirectional gated recurrent units (Bi-GRU) with a temporal attention layer to efficiently capture the nuanced spatial and temporal characteristics of emotional features. By handling variable-length speech without segmentation and eliminating the need for pre or post-processing, the proposed model outperformed state-of-the-art methods on IEMOCAP and EMO-DB datasets. The source code of this paper is shared on the Github repository: https://github.com/alaaNfissi/SigWavNet- Learning-Multiresolution-Signal-Wavelet-Network-for-Speech- Emotion-Recognition. Index Terms\u2014Speech emotion recognition, Fast discrete wavelet transform, Conjugate quadrature filter, Cascade algo- rithm, Dilated CNN, Bi-GRU I. INTRODUCTION S PEECH Emotion Recognition (SER) plays a pivotal role in human-computer interaction, fostering more sophisti- cated dialogues between machines and humans. Its implica- tions span various domains, notably in emergency call centers, where SER systems evaluate an individual\u2019s stress or fear levels, thereby enhancing the accuracy of assessing the caller\u2019s condition [1]. This, in turn, facilitates more effective decision- making within call centers. Additionally, in healthcare, SER proves beneficial in identifying psychological disorders, poten- tially minimizing the risk of suicidal behaviors [2]. Further- more, the integration of SER into virtual AI chatbots opens avenues for providing personalized therapy through online interactions [3]. Speech represents an intricate and high-frequency signal en- compassing information about the conveyed message, speaker characteristics, gender, language, and emotional content. How- ever, challenges persist in discerning emotions from speech signals, primarily due to diverse speaking styles. Physio- logical studies underscore the significance of capturing the complete emotional trajectory, necessitating sufficiently long speech segments [4]. In the initial stages, tasks centered on high-frequency signals typically involve extracting relevant time-frequency features before integrating them into machine learning (ML) algorithms, which are commonly tailored for fixed or limited input sizes [5]. Managing speech signals in an emotional context poses a challenge of balancing high- frequency sampling with low-dimensional decision states (e.g., emotions). Consequently, many applications demand sophisti- cated approaches capable of extracting meaningful and concise information from speech signals to facilitate analysis and SER. Numerous effective applications of ML in the context of speech utilize extracted features as inputs\u2014a manually cu- rated, condensed representation of the original signals. Fre- quently, these features encompass a spectrogram [6], wavelet coefficient statistics [7], or other variations [8]. Despite their notable success, these frameworks require careful feature extraction, a potentially time-intensive task. Moreover, the ex- tracted features may exhibit sensitivity to unforeseen noise or changing conditions, posing challenges in designing domain- invariant features. The establishment of such features, whether through post-processing or ML methodologies, remains an ongoing research inquiry [9]. In light of these feature extraction challenges, it\u2019s worth noting the excellent capabilities of Fourier and wavelet trans- forms for feature extraction and data dimensionality reduction, despite the intensive labor they require based on the dataset and problem at hand. The wavelet transform, with advanta- geous properties such as a linear time algorithm, perfect recon- struction, and customizable wavelet functions [10], emerges as a favorable choice for feature representation. However, its adoption in the machine learning community is limited, with methods like the Fourier transform and its variants being more commonly employed [11]. This underutilization of the wavelet transform may stem from the challenge of designing and selecting appropriate wavelet functions, typically derived analytically using Fourier methods. Moreover, the array of available wavelet functions without a comprehensive under- arXiv:2502.00310v1 [cs.SD] 1 Feb 2025"
}