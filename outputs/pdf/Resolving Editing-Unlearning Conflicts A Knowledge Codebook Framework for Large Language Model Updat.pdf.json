{
  "code_links": "None",
  "tasks": [
    "Large Language Model Updating"
  ],
  "datasets": [
    "TOFU",
    "PKU-SafeRLHF",
    "ZsRE"
  ],
  "methods": [
    "Knowledge Codebook Framework",
    "Similarity-Aware Knowledge Mapping",
    "Learning-Based Router",
    "Conflict-Aware Training"
  ],
  "results": [
    "LOKA outperforms all baselines in unlearning and editing tasks",
    "LOKA maintains consistent performance throughout sequential updating"
  ],
  "title": "Resolving Editing-Unlearning Conflicts A Knowledge Codebook Framework for Large Language Model Updat.pdf",
  "abstract": "Large Language Models (LLMs) excel in natural language processing by encoding extensive hu- man knowledge, but their utility relies on timely updates as knowledge evolves. Updating LLMs involves two key tasks simultaneously: unlearn- ing to remove unwanted knowledge and editing to incorporate new information. Existing meth- ods face two major challenges: ineffective knowl- edge storage (either too sparse or too dense) and task conflicts between editing and unlearning, as validated through our theoretical and experimen- tal results. To address these issues, we propose LOKA, a conflict-free framework for LLM up- dating based on a knowledge codebook. During training, updated knowledge is stored in multi- ple codebook memories. To optimize knowledge storage, a similarity-aware knowledge mapping ensures that related knowledge pieces are clus- tered and allocated to the same memory. Addition- ally, LOKA resolves task conflicts by employing task-specific and multi-task memories guided by a conflict score. In the inference stage, LOKA retrieves the most relevant memory from the code- book and plugs it into the original LLM to ap- ply the updated knowledge. A learning-based router controls codebook activation to further im- prove knowledge utilization. Extensive experi- ments demonstrate the effectiveness of LOKA in LLM knowledge updating tasks. 1. Introduction Large Language Models (LLMs) have demonstrated remark- able capabilities in understanding and generating human- like text across various natural language processing tasks. These capabilities are grounded in the vast knowledge en- coded during their pre-training phase (Brown et al., 2020; Roberts et al., 2020; Hu et al., 2024; Chang et al., 2024). *Work done during an internship at NEC Laboratories America. 1University of Virginia 2NEC Laboratories America. Correspon- dence to: Zhengzhang Chen <zchen@nec-labs.com>. However, human knowledge evolves continually with new discoveries and societal changes. If LLMs cannot keep up with these changes, they risk providing outdated, inaccurate, or even misleading information (Wang et al., 2024e; Zhang et al., 2024c). For instance, a virtual assistant relying on pre- 2022 COVID-19 guidelines might suggest unsafe practices to patients. Additionally, the pre-training process may inad- vertently encode undesirable knowledge, such as harmful content or sensitive information (Chen et al., 2024; Wang et al., 2024b; Liu et al., 2024d), which must be removed promptly upon identification to ensure model reliability. Ad- dressing these issues is critical to maintaining user trust (Yao et al., 2023; Liu et al., 2024a) and mitigating significant le- gal and ethical risks, as evidenced by the lawsuit filed by The New York Times against OpenAI for the unauthorized use of copyrighted materials (Freeman et al., 2024). In response to these challenges, existing research has pro- posed two approaches for updating LLMs: unlearning un- wanted knowledge (Liu et al., 2024c; Qu et al., 2024; Gao et al., 2024; Yao et al., 2024) and editing outdated knowl- edge (Li et al., 2024b; Wang et al., 2024f;a). Unlearning methods fine-tune either the original model (Yao et al., 2023; Eldan & Russinovich, 2023; Zhang et al., 2024d; Liu et al., 2024b) or additional layers (Chen & Yang, 2023) to max- imize prediction loss on the unlearning dataset. However, directly fine-tuning the original model often yields catas- trophic forgetting (Zhai et al., 2023), degrading the model\u2019s remaining knowledge in the long term. To mitigate this issue, memory-based editing techniques freeze the original LLM and fine-tune external memo- ries (Mitchell et al., 2022b; Wang et al., 2024e;c), which are retrieved during inference and integrated with the orig- inal model. This approach enables accurate updates while preserving the model\u2019s utility and integrity. However, lever- aging knowledge memory in LLM updating introduces two critical challenges. The first challenge is the conflict be- tween unlearning and editing. Intuitively, unlearning aims to maximize the prediction loss on old knowledge, whereas editing seeks to minimize the prediction loss for new knowl- edge. When editing and unlearning data share overlapping knowledge concepts, their opposing optimization objectives can lead to gradient conflicts (Fifty et al., 2021; Chai et al., 1 arXiv:2502.00158v1 [cs.CL] 31 Jan 2025"
}