{
  "code_links": [
    "https://github.com/SongW-SW/GRM"
  ],
  "tasks": [
    "Graph Out-of-Distribution Generalization"
  ],
  "datasets": [
    "Cora",
    "Photo",
    "Twitch",
    "FB-100",
    "Elliptic",
    "Arxiv",
    "SP-Motif",
    "MNIST-75sp",
    "G-SST2",
    "Molhiv"
  ],
  "methods": [
    "Generative Risk Minimization (GRM)",
    "Variational Graph Auto-Encoder (VGAE)",
    "Graph Neural Networks (GNNs)",
    "Invariance Loss",
    "Regularization Loss",
    "Supervision Loss"
  ],
  "results": [
    "GRM consistently outperforms all other baselines on both the worst case (Min.) and average (Avg.) results",
    "GRM achieves the best results over other baselines on all four datasets"
  ],
  "title": "Generative Risk Minimization for Out-of-Distribution Generalization on Graphs.pdf",
  "abstract": "Out-of-distribution (OOD) generalization on graphs aims at dealing with scenarios where the test graph distribution differs from the training graph distributions. Compared to i.i.d. data like images, the OOD generalization problem on graph-structured data remains challenging due to the non-i.i.d. property and complex structural information on graphs. Recently, several works on graph OOD generalization have explored extracting invariant subgraphs that share crucial classification information across different distributions. Nevertheless, such a strategy could be suboptimal for entirely capturing the invariant information, as the extraction of discrete structures could potentially lead to the loss of invariant information or the involvement of spurious information. In this paper, we propose an innovative framework, named Generative Risk Minimization (GRM), designed to generate an invariant subgraph for each input graph to be classified, instead of extraction. To address the challenge of optimization in the absence of optimal invariant subgraphs (i.e., ground truths), we derive a tractable form of the proposed GRM objective by introducing a latent causal variable, and its effectiveness is validated by our theoretical analysis. We further conduct extensive experiments across a variety of real-world graph datasets for both node-level and graph-level OOD generalization, and the results demonstrate the superiority of our framework GRM. Our code is provided at https://github.com/SongW-SW/GRM. 1 Introduction In recent years, it has become increasingly crucial to develop machine learning models that can handle tasks with test data distributions differing from training data, commonly referred to as out-of-distribution (OOD) generalization (Mansour et al., 2009; Blanchard et al., 2011; Muandet et al., 2013; Beery et al., 2018; Recht et al., 2019; Su et al., 2019). Such disparities, termed as distribution shifts, can substantially undermine the 1 arXiv:2502.07968v1 [cs.LG] 11 Feb 2025"
}