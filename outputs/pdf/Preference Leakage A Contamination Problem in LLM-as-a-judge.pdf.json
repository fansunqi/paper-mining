{
  "code_links": [
    "https://github.com/David-Li0406/Preference-Leakage"
  ],
  "tasks": [
    "LLM-as-a-judge",
    "Data contamination"
  ],
  "datasets": [
    "Arena-Hard",
    "AlpacaEval 2.0",
    "MTBench"
  ],
  "methods": [
    "Preference leakage score",
    "Pairwise comparison",
    "Manual annotation",
    "BERT classification"
  ],
  "results": [
    "Preference leakage score in most model pairs indicates strong bias towards related student models",
    "Inheritance settings lead to significant bias",
    "Same series models show more significant bias",
    "Larger student models cause more bias",
    "Different data generator/judge LLMs result in varying degrees of bias",
    "Preference leakage score correlates with the degree of relatedness and proportion of synthetic data",
    "LLM judges do not show good performance in recognizing student models' generation",
    "Subjective questions and judgment dimensions tend to lead to more bias"
  ],
  "title": "Preference Leakage A Contamination Problem in LLM-as-a-judge.pdf",
  "abstract": "Large Language Models (LLMs) as judges and LLM-based data synthesis have emerged as two fundamental LLM-driven data annotation methods in model development. While their com- bination significantly enhances the efficiency of model training and evaluation, little attention has been given to the potential contamination brought by this new model development paradigm. In this work, we expose preference leakage, a contami- nation problem in LLM-as-a-judge caused by the relatedness between the synthetic data generators and LLM-based evaluators. To study this issue, we first define three common relatednesses between data generator LLM and judge LLM: being the same model, having an inheritance relationship, and belonging to the same model family. Through extensive experiments, we empirically confirm the bias of judges towards their related student models caused by preference leakage across multiple LLM baselines and benchmarks. Further analysis suggests that prefer- ence leakage is a pervasive issue that is harder to detect compared to previously identified biases in LLM-as-a-judge scenarios. All of these findings imply that preference leakage is a widespread and challenging problem in the area of LLM- as-a-judge. We release all codes and data at: https://github.com/David-Li0406/ Preference-Leakage1. 1. Introduction Recent advancements in Large Language Models (LLMs) (Achiam et al., 2023; Jaech et al., 2024; Tong et al., 2024; Zhang et al., 2024a) have empowered various *Equal contribution 1Arizona State University 2University of California, Los Angeles 3University of Notre Dame 4University of Illinois Urbana Champaign. Correspondence to: Dawei Li <daweili5@asu.edu>. 1More resources on LLM-as-a-judge are on the website: https://llm-as-a-judge.github.io/ downstream tasks and applications. However, this also poses substantial challenges to the automatic evaluation of these models. Representatively, LLM-based AI agents\u2019 focus transfer from traditional natural language processing tasks (Yang et al., 2023; Zhang et al., 2023) to real-world (Liu et al., 2023b; Huang et al., 2023), open-ended response generation (Wu et al., 2024), which greatly limits the applicability of traditional n-gram matching methods (e.g., BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004)) (Liu et al., 2016; Reiter, 2018) or model-based evaluators (Zhang et al., 2020; Zhong et al., 2022) for evaluation. To address these challenges, the paradigm of LLM-as-a- judge (Zheng et al., 2023; Li et al., 2024a; Jiang et al., 2024a; Zhong et al., 2024; Li et al., 2025) has been proposed, de- signed to leverage LLM as evaluators to assess response quality. By combining powerful LLMs with well-designed prompting strategies, LLM-as-a-judge enables human-like evaluation of long-form and open-ended generation in a more cost-efficient and scalable manner. However, recent studies point out some weaknesses of such assessment. For instance, Ye et al. (2024) explores various biases and vulner- abilities of LLM-as-a-judge, highlighting the importance of developing a reliable and fair LLM-based evaluation system. In this work, we aim to introduce another concern in LLM- as-a-Judge\u2013Preference Leakage. This issue arises when the LLMs used for data generation and evaluation are closely re- lated, as illustrated in Figure 1. Synthetic data generated by LLMs (Gan et al., 2023; Tan et al., 2024; Li et al., 2024b;c) has become a cornerstone of model training (Lee et al., 2025). When combined with LLM-as-a-Judge, they offer significant efficiency gains in model development. However, limited attention has been given to the potential contami- nation that occurs when the generator and evaluator LLMs share a close relationship. During our preliminary study, we find this issue is particularly pervasive in popular LLM- as-a-judge benchmarks (e.g., AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li et al., 2024e)) and LLM-relevant studies (more details can be found in Appendix A), due to the common reliance on the most advanced LLMs, such as GPT-4 (Achiam et al., 2023), for both data synthesis and evaluation to ensure the highest quality outputs. In our work, we reveal this relatedness\u2014akin to the overlap be- tween training data and evaluation sets in traditional data 1 arXiv:2502.01534v1 [cs.LG] 3 Feb 2025"
}