{
  "code_links": [
    "https://github.com/huanggongping/LMFCA-Net"
  ],
  "tasks": [
    "Multi-channel speech enhancement"
  ],
  "datasets": [
    "VCTK-DEMAND",
    "CHIME-3"
  ],
  "methods": [
    "Decoupled Fully-Connected Attention (FCA)",
    "T-FCA",
    "F-FCA",
    "Sandglass Unit",
    "Bottleneck Block"
  ],
  "results": [
    "Significant reduction in computational complexity and latency",
    "Maintaining comparable performance to state-of-the-art methods"
  ],
  "title": "LMFCA-Net A Lightweight Model for Multi-Channel Speech Enhancement with Efficient Narrow-Band and Cr.pdf",
  "abstract": "\u2014Deep learning based end-to-end multi-channel speech en- hancement methods have achieved impressive performance by leveraging sub-band, cross-band, and spatial information. However, these methods often demand substantial computational resources, limiting their prac- ticality on terminal devices. This paper presents a lightweight multi- channel speech enhancement network with decoupled fully connected attention (LMFCA-Net). The proposed LMFCA-Net introduces time-axis decoupled fully-connected attention (T-FCA) and frequency-axis decou- pled fully-connected attention (F-FCA) mechanisms to effectively capture long-range narrow-band and cross-band information without recurrent units. Experimental results show that LMFCA-Net performs comparably to state-of-the-art methods while significantly reducing computational complexity and latency, making it a promising solution for practical applications. Index Terms\u2014Multi-channel speech enhancement, decoupled fully connected attention (FCA), deep learning, lightweight model. I. INTRODUCTION Speech enhancement aims to improve the clarity and intelligibility of speech by suppressing noise and reverberation. It is widely employed in applications such as teleconferencing, hearing aids, and hands-free human-machine interfaces [1, 2]. Nowadays, most systems are equipped with microphone arrays for sound signal acquisition, and the strength of multi-channel speech enhancement lies in its ability to exploit spatial information [3, 4]. Traditional approaches are primarily based on signal processing, with common methods including beamforming and multi-channel filtering [5\u20138]. Recent advances in deep learning have led to a shift in mi- crophone arrays speech enhancement towards deep learning-based approaches [9]. One strategy involves estimating beamforming pa- rameters using deep neural networks (DNNs) [9, 10], while end- to-end multi-channel speech enhancement directly processes input speech signals using DNNs to leverage their nonlinear capabilities. To effectively exploit spatial information, end-to-end multi-channel speech enhancement methods often include modules that explicitly capture narrow-band and cross-band information. For instance, the FT-JNF employs long short-term memory (LSTM) units along the time and frequency axes of the spectrogram [11], while McNet [12] improves this approach by applying LSTM units across combined adjacent frames or frequency bins to analyze spectral information. More recently, the TF-GridNet [13] and SpatialNet [14] have incor- porated self-attention mechanisms to model narrow-band and cross- band information. This two-stage approach is advantageous because spatial cues remain consistent over time for stationary sources, though they tend to vary with frequency [15]. Cross-band information, in turn, enhances the integration of these features [13]. However, current DNN-based multi-channel speech enhancement methods are often computationally intensive, making them challenging to implement in terminal applications with limited processing capabilities [2]. The sequential nature of recurrent units, such as LSTMs, limits paral- lelization and hardware efficiency [16], while self-attention mech- anisms impose a substantial computational burden due to extensive matrix multiplications [17]. Existing lightweight speech enhancement methods primarily focus on mono-channel designs. For instance, PerceptNet [18] uses GRU recurrence as the DNN backbone, Deep- FilterNet [19] and GTCRN [20] reduce computational complexity by employing grouped GRUs. However, these methods still face issues such as gradient vanishing and training instability [21]. Implementing an end-to-end multi-channel speech enhancement network on terminal devices is worth exploring. According to pre- vious studies [22], simply extending single-channel speech input to multi-channel input and allowing the network to implicitly utilize spa- tial information can improve the speech enhancement performance. Currently, multi-channel speech enhancement networks, such as TF- GridNet [13] and many others [12, 23] typically have a computational cost of several to tens of billions of multiply-accumulate operations per second (MACs). These complex model structures achieve superior enhancement performance, but they are difficult to practically apply to terminal devices. We summarize the current dilemma in the following two points: \u2022 Only a few multi-channel designs consider lightweight features, for example, NICE-Beam [24] estimates statistical quantities within the beamforming framework, focusing primarily on ef- fectively estimating the covariance matrix for minimum variance distortionless response (MVDR) filters. \u2022 Modeling narrow-band and cross-band information is essential for enhancing the use of spatial cues [13], a technique widely used in multi-channel designs [12, 14, 15]. Previous methods commonly rely on recurrent units or self-attention module to achieve this, which limit computational efficiency and, in the case of recurrent units, can result in training instability [16, 21]. In this work, we propose an approach that leverages long-range narrow-band and cross-band information without relying on recurrent units. This is achieved through the introduction of time-axis decou- pled fully-connected attention (T-FCA) and frequency-axis decoupled fully-connected attention (F-FCA) mechanisms, inspired by the work of Tang et al. [17]. Building on these mechanisms, we introduce the Lightweight Multi-channel speech enhancement network with decoupled Fully Connected Attention, termed as LMFCA-Net, to address these challenges. Experimental results demonstrate that the two-stage modeling, based on T-FCA and F-FCA, is highly effec- tive. Furthermore, LMFCA-Net strikes an excellent balance between computational efficiency and enhancement performance. arXiv:2502.11462v1 [eess.AS] 17 Feb 2025"
}