{
  "code_links": [
    "https://github.com/KomeijiForce/Cuckoo"
  ],
  "tasks": [
    "Information Extraction"
  ],
  "datasets": [
    "C43",
    "TuluV3"
  ],
  "methods": [
    "Next Tokens Extraction (NTE)"
  ],
  "results": [
    "F1 score: 70.0-85.0"
  ],
  "title": "Cuckoo an IE Free Rider Hatched by Massive Nutrition in LLM's Nest.pdf",
  "abstract": "Massive high-quality data, both pre-training raw texts and post-training annotations, have been carefully prepared to incubate advanced large language models (LLMs). In contrast, for information extraction (IE), pre-training data, such as BIO-tagged sequences, are hard to scale up. We show that IE models can act as free rid- ers on LLM resources by reframing next-token prediction into extraction for tokens already present in the context. Specifically, our pro- posed next tokens extraction (NTE) paradigm learns a versatile IE model, Cuckoo1, with 102.6M extractive data converted from LLM\u2019s pre-training and post-training data. Under the few-shot setting, Cuckoo adapts effectively to traditional and complex instruction-following IE with better performance than existing pre- trained IE models. As a free rider, Cuckoo can naturally evolve with the ongoing advance- ments in LLM data preparation, benefiting from improvements in LLM training pipelines with- out additional manual effort.2 1 Introduction The biggest lesson researchers have learned from training large language models (LLMs) (Wang et al., 2023b; Touvron et al., 2023; Achiam et al., 2023; Groeneveld et al., 2024; Dubey et al., 2024; Team et al., 2024) is the power of massive and high- quality data (Kaplan et al., 2020; Hernandez et al., 2021). Although pre-training information extrac- tion (IE) models (Huang et al., 2021; Tedeschi and Navigli, 2022; Lu et al., 2022; Li et al., 2023; Bog- danov et al., 2024; Peng et al., 2024) has once been a popular topic before the rise of general LLMs, the relative scarcity of automated annotations has lim- ited the further development of this domain. Conse- quently, more and more researchers have accepted 1Cuckoo is known for laying its eggs in other birds\u2019 nests, tricking them into raising its chicks. 2Open Cuckoo: https://github.com/KomeijiForce/Cuckoo LLMs as backbone models for IE tasks (Agrawal et al., 2022; Wang et al., 2023a; Xu et al., 2024b). The primary reason for the temporary lag in IE pre-training is the stricter format requirements for data collection compared to those for LLMs. The paradigm for learning LLMs, the next token predic- tion (NTP), can utilize every token in the sentence as an annotation. In contrast, IE pre-training al- ways requires spans annotated with label names. While certain platforms provide massive annota- tions, such as Page Links in Wikipedia (Balasuriya et al., 2009; Ding et al., 2021; Han et al., 2018; Tedeschi and Navigli, 2022), they are still much less efficient than NTP. To illustrate the gap, Multi- nerd (Tedeschi and Navigli, 2022) takes multiple processing efforts to collect 164K English named entity recognition (NER) instances from Wikipedia and Wikinews, while NTP can easily gather tril- lions of tokens from raw texts as supervision. This paper proposes a frustratingly simple yet ef- fective way to scale up IE pre-training. We suggest that IE pre-training can simply be a free rider on the LLM\u2019s training resources by learning on exactly the same pre-training and post-training datasets. We modify NTP to next tokens extraction (NTE), using BIO tags for next tokens that can be extracted from the input context as shown in Figure 1. With the instruction-following ability learned in post- training, one can adjust the prompt to instruct NTE- based taggers to perform different IE tasks. Specialized for IE, NTE has three advantages over NTP. 1) Parameter Efficiency, NTP requires extra parameters to store knowledge to generate tokens not in the input context, while NTE concen- trates only on tagging input tokens. Thus, NTE- based IE taggers can have better parameter effi- ciency than NTP-based LLMs, fitting it to smaller models like RoBERTa (Liu et al., 2019). 2) Infer- ence Efficiency, NTE taggers are not only smaller because of the parameter efficiency but can also extract multiple tokens with the BIO scheme in 1 arXiv:2502.11275v1 [cs.CL] 16 Feb 2025"
}