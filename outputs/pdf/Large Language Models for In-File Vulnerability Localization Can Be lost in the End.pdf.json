{
  "code_links": [
    "None"
  ],
  "tasks": [
    "In-File Vulnerability Localization"
  ],
  "datasets": [
    "CVE catalog"
  ],
  "methods": [
    "LLMs",
    "Logistic regression",
    "Code-in-the-haystack experiment",
    "Chunking strategy"
  ],
  "results": [
    "Low accuracy scores below .4",
    "Lost-in-the-end issue",
    "Chunking strategy improves recall by over 37%"
  ],
  "title": "Large Language Models for In-File Vulnerability Localization Can Be lost in the End.pdf",
  "abstract": "However, this function-level approach may miss bugs that span multiple functions and code blocks. Recent advancements in artificial intelligence have enabled processing of larger inputs, leading everyday software developers to increasingly rely on chat-based large language models (LLMs) like GPT-3.5 and GPT-4 to detect vulnerabilities across entire files, not just within functions. This new development practice requires researchers to urgently investigate whether commonly used LLMs can effectively analyze large file-sized inputs, in order to provide timely insights for software developers and engineers about the pros and cons of this emerging technological trend. Hence, the goal of this paper is to evaluate the effectiveness of several state-of-the-art chat-based LLMs, including the GPT models, in detecting in-file vulnerabilities. We conducted a costly investigation into how the performance of LLMs varies based on vulnerability type, input size, and vulnerability location within the file. To give enough statistical power (\ud835\udefd\u2265.8) to our study, we could only focus on the three most common (as well as dangerous) vulnerabilities: XSS, SQL injection, and path traversal. Our findings indicate that the effectiveness of LLMs in detecting these vulnerabilities is strongly influenced by both the location of the vulnerability and the overall size of the input. Specifically, regardless of the vulnerability type, LLMs tend to significantly (\ud835\udc5d< .05) underperform when detecting vulnerabilities located toward the end of larger files\u2014a pattern we call the \u2018lost-in-the-end\u2019 effect. Finally, to further support software developers and practitioners, we also explored the optimal input size for these LLMs and presented a simple strategy for identifying it, which can be applied to other models and vulnerability types. Eventually, we show how adjusting the input size can lead to significant improvements in LLM-based vulnerability detection, with an average recall increase of over 37% across all models. Replication Package: https://doi.org/10.5281/zenodo.14840519 CCS Concepts: \u2022 Software and its engineering \u2192Software testing and debugging; \u2022 Computing methodologies \u2192Neural networks; \u2022 Security and privacy \u2192Software security engineering. Additional Key Words and Phrases: Large Language Models, In-File Vulnerability Detection, XSS, SQL Injection, Path Traversal, \u2018Lost-in-the-End\u2019 Issue, Code Context 1 Introduction In the rapidly evolving field of software development, the integration of large language models (LLMs) has shifted from a novel concept to a mainstream practice. As generative artificial intelligence (AI) technologies continue to advance, tools like ChatGPT, Copilot, and similar models are now embedded as first-class citizens into development environments, assisting with tasks such as code generation, bug fixing, and security analysis. According to Sonatype\u2019s 2023 report [40], 97% of developers and security professionals now rely on LLMs in their workflows, marking a significant shift in how software vulnerabilities are detected and patched. Indeed, the integration of LLMs addresses a critical challenge identified in the 2022 GitLab Survey [17], which noted that developers often fail to detect security-relevant bugs early and do not prioritize bug fixing adequately. This typically leads to frequent vulnerabilities that are Authors\u2019 Contact Information: Francesco Sovrano, Collegium Helveticum, ETH Zurich, Zurich, Switzerland and University of Zurich, Department of Informatics, Zurich, Switzerland, sovrano@collegium.ethz.ch; Adam Bauer, University of Zurich, Zurich, Switzerland, adam.bauer@uzh.ch; Alberto Bacchelli, University of Zurich, Department of Informatics, Zurich, Switzerland, bacchelli@ifi.uzh.ch. arXiv:2502.06898v1 [cs.SE] 9 Feb 2025"
}