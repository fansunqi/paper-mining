{
  "code_links": [
    "https://github.com/tungngreen/PipelineScheduler"
  ],
  "tasks": [
    "Edge Video Analytics"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Dynamic batching",
    "Workload-aware greedy algorithm",
    "Spatiotemporal scheduling"
  ],
  "results": [
    "10\u00d7 effective throughput improvement",
    "Stable end-to-end latency distribution"
  ],
  "title": "OCTOPINF Workload-Aware Inference Serving for Edge Video Analytics.pdf",
  "abstract": "\u2014Edge Video Analytics (EVA) has become a major application of pervasive computing, enabling real-time visual processing. EVA pipelines, composed of deep neural networks (DNNs), typically demand efficient inference serving under stringent latency requirements, which is challenging due to the dynamic Edge environments (e.g., workload variability and network instability). Moreover, EVA pipelines face significant resource contention due to resource (e.g., GPU) constraints at the Edge. In this paper, we introduce OCTOPINF, a novel resource- efficient and workload-aware inference serving system designed for real-time EVA. OCTOPINF tackles the unique challenges of dynamic edge environments through fine-grained resource allocation, adaptive batching, and workload balancing between edge devices and servers. Furthermore, we propose a spatiotem- poral scheduling algorithm that optimizes the co-location of inference tasks on GPUs, improving performance and ensuring service-level objectives (SLOs) compliance. Extensive evaluations on a real-world testbed demonstrate the effectiveness of our approach. It achieves an effective throughput increase of up to 10\u00d7 compared to the baselines and shows better robustness in challenging scenarios. OCTOPINF can be used for any DNN-based EVA inference task with minimal adaptation and is available at https://github.com/tungngreen/PipelineScheduler. I. INTRODUCTION Recently, Edge Video Analytics (EVA) has emerged as a major area of pervasive computing [1], offering real- time visual sensing and processing. The pervasive nature of EVA systems enables seamless integration into diverse tasks such as surveillance [2], [3], [4], [5], health care [6], and activity recognition [7], [8]. These systems can perform continuous, on-site video stream analysis with reduced de- pendency on remote cloud infrastructures. In practice, VA services are typically organized as cascading pipelines of deep neural networks (DNNs) [9]. For instance, the pipeline of [Object Detect \u2192Vehicle Classify, Plate Detect] can be em- ployed for traffic monitoring (Figure 1). The task of executing the pipeline is defined as inference serving and is subjected to stringent latency demands (e.g., 200 ms), specified by service- level objectives (SLOs). It has been studied in works such as [10], [11], [12], [13], [14], [15] with the common goal of efficiently allocating and scheduling resources to meet SLOs. Advancements in embedded computing now enable DNN models to run on both Edge servers and nearby embed- ded devices (e.g., Jetson Orin Nano) deployed next to data sources such as CCTV cameras [16]. Despite their \u2217This paper has been accepted to IEEE PerCom 2025. \u00a9 2025 IEEE. The final version will be available at [DOI link when available]. People Cars Fig. 1: Real-world footage at 2 locations shows varying number of objects leading to variations in workload for pipeline models (e.g., car classification). limited capabilities, Edge devices can execute part of the EVA pipeline, enabling server-device cooperation to share workloads [9], [14], [15] increasing efficiency and flex- ibility. However, implementing this deployment scenario to reap its full potential involves two key considerations. (1) Highly dynamic environments. At the Edge, significant variations in video content over time lead to changes in workloads for models within an EVA pipeline (Figure 1), and constant changes in network conditions (i.e., bandwidth and latency) cause fluctuations in compute time budgets for the pipeline, complicating the workload distribution task. (2) Resource contention. At the Edge, multiple DNN models run concurrently, or co-located, on the same processing unit (e.g., GPUs), leading to unpredictable performance degrada- tions in terms of latency (defined as co-location interference [17]). While the adverse effects can be lessened by grouping models with low workloads, the workload dynamicity men- tioned above makes this impractical. Several solutions have been proposed. Jellyfish [13] lever- ages dynamic batching to increase throughput and uses mul- tiple DNN versions to adapt to network latency variations. During network instability, it reduces data resolution by opting for less accurate model versions, and adjusts batch sizes to meet throughput demands. However, its centralized architec- ture, which transfers raw videos to the server and assumes ample GPU resources to avoid co-location interference, is unsuitable for resource-constrained Edge environments. Fur- thermore, Jellyfish can only adjust batch sizes for different versions of the same model, not systematically schedule the whole pipeline. Distributed architectures like Distream [14] and Rim [18] instead utilize Edge devices and balance workloads between devices and servers. Distream introduces a stochastic method to determine the \u201dsplit point,\u201d adaptively dividing EVA pipelines between local and server-based work- loads. However, to reduce the optimization space, it uses a static batch size for models, which fails to account for dynamic workloads and network conditions. This is because while batching improves throughput, it increases end-to-end latency arXiv:2502.01277v1 [cs.DC] 3 Feb 2025"
}