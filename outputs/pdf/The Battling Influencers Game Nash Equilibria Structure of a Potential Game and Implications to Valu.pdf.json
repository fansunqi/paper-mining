{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Value Alignment"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Game Theory",
    "Nash Equilibrium"
  ],
  "results": [
    "Potential Game",
    "Pure Nash Equilibria"
  ],
  "title": "The Battling Influencers Game Nash Equilibria Structure of a Potential Game and Implications to Valu.pdf",
  "abstract": "When multiple influencers attempt to compete for a receiver\u2019s attention, their influencing strategies must account for the presence of one another. We introduce the Battling Influencers Game (BIG), a multi-player simultaneous-move general-sum game, to provide a game-theoretic characteriza- tion of this social phenomenon. We prove that BIG is a potential game, that it has either one or an infinite number of pure Nash equilibria (NEs), and these pure NEs can be found by convex opti- mization. Interestingly, we also prove that at any pure NE, all (except at most one) influencers must exaggerate their actions to the maximum extent. In other words, it is rational for the influencers to be non-truthful and extreme because they antici- pate other influencers to cancel out part of their influence. We discuss the implications of BIG to value alignment. 1. Introduction Life is full of agents who want to influence others: Food truck vendors entice us with BBQ samples; Social media influencers review selective pickleball brands to persuade us; Editors publish op-eds to sway public opinions. When multiple influencers with conflicting interests battle for our attention, intuitively they would be strategic and adjust their actions to account for the presence of one another in order to be effective. This paper presents a game theoretic definition of the Bat- tling Influencers Game (BIG). We model the influencers as players in a multi-player simultaneous-move general-sum game. Our main technical result is that BIG is a potential game with special pure Nash Equilibria structures. Conse- quently, we can predict how rational influencers would ad- just their strategies in the battle: exaggeration is inevitable. This prediction may shed new computational light on the genesis of misinformation. As a use case, BIG can be applied to the AI value alignment problem. The receivers of the influence were traditionally people, but can extend to AI value alignment algorithms. However, unlike in standard machine learning, our focus is not on the value alignment algorithm itself. Instead, BIG predicts how battling alignment-data providers could be motivated to intentionally produce training data that do not truthfully reflect their values. While out of scope for the current paper, our insight can help design future value alignment algorithms to remove such incentives. 2. Related Work Our work provides a game theoretic model of the numerical example and informal theorem in section 5 of (Park et al., 2024), in particular, we also assume strategic data providers (which we call influencers) to large language models (which we call receivers), and our model leads to results consistent with their example where the data providers untruthfully report their opinions. In addition, we prove the existence of pure strategy Nash equilibria of this class of games, and we show the property that almost all influencers maximally exaggerate their preferences in every equilibrium. Our work is also closely related to (Hao & Duan, 2024), which mod- els the interaction between multiple influencers by a dy- namic Bayesian game. They described the phenomenon of strategic extreme exaggeration, which is also discussed in (Sun et al., 2024), (Soumalias et al., 2024), (Conitzer et al., 2024), and (Roughgarden & Schrijvers, 2017) for various applications, but they do not explicitly compute the equilibria of the original game or quantify the amount of exaggeration. In comparison, we use a static game with known influencer types and we are able to provide better characterizations of the set of equilibria of the game. Value alignment aims to make language models produce outputs that are more aligned with human values. Exist- ing training frameworks tailored for this purpose, such as (Ouyang et al., 2022) and (Rafailov et al., 2024), collect preference data from humans and train a large language model to follow users\u2019 intent. However, research in this direction mostly focuses on the algorithmic aspects of value alignment and does not emphasize the heterogeneity of hu- man values. There has been work that studies how to make LLMs align with diverse preferences of different demo- 1 arXiv:2502.01127v3 [cs.GT] 7 Feb 2025"
}