{
  "code_links": [
    "https://github.com/njspyx/location-inference"
  ],
  "tasks": [
    "Image Geolocation Inference"
  ],
  "datasets": [
    "Google Street View"
  ],
  "methods": [
    "VLM",
    "Agent"
  ],
  "results": [
    "Median distance error < 300 km",
    "Up to 30.6% decrease in distance error with tool access"
  ],
  "title": "Evaluating Precise Geolocation Inference Capabilities of Vision Language Models.pdf",
  "abstract": "The prevalence of Vision-Language Models (VLMs) raises important questions about privacy in an era where visual in- formation is increasingly available. While foundation VLMs demonstrate broad knowledge and learned capabilities, we specifically investigate their ability to infer geographic lo- cation from previously unseen image data. This paper intro- duces a benchmark dataset collected from Google Street View that represents its global distribution of coverage. Foundation models are evaluated on single-image geolocation inference, with many achieving median distance errors of <300 km. We further evaluate VLM \u201cagents\u201d with access to supplemental tools, observing up to a 30.6% decrease in distance error. Our findings establish that modern foundation VLMs can act as powerful image geolocation tools, without being specifically trained for this task. When coupled with increasing accessi- bility of these models, our findings have greater implications for online privacy. We discuss these risks, as well as future work in this area.1 Introduction With the proliferation of user-data collecting services on the Internet, it seems that privacy is becoming increasingly scarce (Jiang et al. 2022). This issue has only been exacer- bated by the advent of Large Language Models (LLMs) and multi-modal Vision-Language Models (VLMs). Research has demonstrated AI\u2019s ability to infer user demographics from both text (Staab et al. 2024) and social media images (T\u00a8omekc\u00b8e et al. 2024). Inspired by the popular geolocation game, GeoGuessr, we investigate whether this capability ex- tends to precise location inference of everyday images. Geolocation inference refers to the ability to deter- mine exact geographical coordinates (latitude and longitude) given an image or series of images (Haas et al. 2024). While research in this field has advanced substantially over the past decade, it still remains a challenging problem. Previ- ous works built custom datasets and architectures that focus on specific regions (cities or countries) and fail to generalize beyond distribution shifts in testing (Suresh, Chodosh, and Copyright \u00a9 2025, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. 1Datasets and code can be found at: https://github.com/njspyx/ location-inference (a) Population Distribution Class Population Range Count Small Urban 50,000\u2013200,000 560 Medium Urban 200,000\u2013500,000 539 Metropolitan 500,000\u20131.5M 323 Large Metropolitan >1.5M 141 (b) Geographic Distribution Continent Count North America 26 Asia 21 Europe 21 Africa 13 South America 5 Oceania 2 Table 1: Geographical and population distribution of full benchmark dataset. Abello 2018; Wu and Huang 2022; Berton, Masone, and Ca- puto 2022; Clark et al. 2023). A recent development is the PIGEON architecture (Haas et al. 2024), which achieved re- markable global geolocation performance. It is notable that the PIGEON authors have not released the model\u2019s train- ing data and weights, citing the ethical risks of public use. Our work demonstrates that superhuman capabilities can be achieved through foundation VLMs and limited scaffolding. While prior studies (Wang et al. 2024; Zhang et al. 2023) have demonstrated geolocation in VLMs to some extent, they limit their evaluation to country classification, rather than precise geographical coordinates. In addition, they fail to test the full potential of VLM capability. Compared to custom architectures, foundation models have shown im- pressive general reasoning and common sense (Lu et al. 2024), allowing them to use tools and act as autonomous agents. General VLM benchmarks have considered the eval- uation of these agents (Li et al. 2024), but haven\u2019t explored them in the context of geolocation. To address these research gaps, we collect a dataset of images from Google Street View to benchmark image ge- olocation. An evaluation of single-image location inference arXiv:2502.14412v1 [cs.CV] 20 Feb 2025"
}