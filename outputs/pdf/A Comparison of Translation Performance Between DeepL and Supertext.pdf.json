{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Machine Translation"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "None"
  ],
  "results": [
    "Segment-level assessments indicate no strong preference between the systems in most cases",
    "Document-level analysis reveals a preference for Supertext in three out of four language directions"
  ],
  "title": "A Comparison of Translation Performance Between DeepL and Supertext.pdf",
  "abstract": "As strong machine translation (MT) systems are increasingly based on large language mod- els (LLMs), reliable quality benchmarking re- quires methods that capture their ability to leverage extended context. This study com- pares two commercial MT systems \u2013 DeepL and Supertext \u2013 by assessing their performance on unsegmented texts. We evaluate transla- tion quality across four language directions with professional translators assessing seg- ments with full document-level context. While segment-level assessments indicate no strong preference between the systems in most cases, document-level analysis reveals a preference for Supertext in three out of four language dir- ections, suggesting superior consistency across longer texts. We advocate for more context- sensitive evaluation methodologies to ensure that MT quality assessments reflect real-world usability.1 1 Introduction After the transition from statistical to neural mod- elling roughly a decade ago (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015), the field of MT is undergoing an- other paradigm shift towards leveraging LLMs (Xu et al., 2024; Wu et al., 2024b; Kocmi et al., 2024). LLM-based translation offers the potential for sig- nificantly improved translation quality, especially with respect to consistent translation of documents. Unlike neural machine translation (NMT) systems, which typically process documents as isolated sen- tences or paragraphs (Post and Junczys-Dowmunt, 2023), many LLMs operate with context windows that can span thousands of words, allowing them to maintain consistency throughout a document \u2013 for instance, by ensuring that a word\u2019s translation in 1We release all evaluation data and scripts for further ana- lysis and reproduction at https://github.com/supertext /evaluation_deepl_supertext the final sentence matches its previous forms (Wu et al., 2024b). In the most recent shared task at the Conference on Machine Translation (WMT24) that focuses on evaluating the state of the art in general-domain translation quality, the majority of the 28 system submissions were already based on LLMs (Kocmi et al., 2024). Although without statistical signific- ance and for the language direction English to Ger- man only, one system even outranked the human reference translations as evaluated by professional human annotators. Despite this impressive achievement, findings of human-machine parity should be approached with caution. Similar claims already emerged with pre- LLM technology (Hassan et al., 2018), yet have subsequently been refuted due to limitations in the evaluation design focusing on single segments in isolation (L\u00e4ubli et al., 2018; Toral et al., 2018; Freitag et al., 2021). The WMT24 shared task also highlights that evaluations based on automatic metrics (rather than human evaluation) can lead to wrong conclusions when comparing strong MT systems (Kocmi et al., 2024). However, these insights are often overlooked in evaluations of commercial MT systems. For ex- ample, Intento\u2019s The State of Machine Translation 2024 report,2 which assesses 52 providers across 11 language pairs, serves as a valuable resource for potential users in real-world settings, but its benchmarking methodology relies on automatic scoring of sentence-level data, and the authors ac- knowledge that \u2018you may need a human linguist\u2019 to ensure greater reliability. In this paper, we evaluate two commercial trans- lation systems (Section 2) under conditions that allow for leveraging the full-text capabilities of LLMs. Source texts are translated without prior 2https://inten.to/machine-translation-repor t-2024 1 arXiv:2502.02577v2 [cs.CL] 11 Feb 2025"
}