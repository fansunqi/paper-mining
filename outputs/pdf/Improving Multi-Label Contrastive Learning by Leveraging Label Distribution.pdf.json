{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Multi-label learning"
  ],
  "datasets": [
    "MS-COCO",
    "NUS-WIDE",
    "MIRFLICKR",
    "PASCAL-VOC",
    "Mulan"
  ],
  "methods": [
    "Contrastive learning",
    "Label distribution learning"
  ],
  "results": [
    "mAP: 818/820",
    "mAP: 826/826",
    "mAP: 824/824",
    "mAP: 825/825"
  ],
  "title": "Improving Multi-Label Contrastive Learning by Leveraging Label Distribution.pdf",
  "abstract": "In multi-label learning, leveraging contrastive learning to learn better representations faces a key challenge: selecting positive and negative samples and effectively utilizing label in- formation. Previous studies selected positive and negative samples based on the overlap between labels and used them for label-wise loss balancing. However, these methods suffer from a complex selection process and fail to account for the varying importance of different labels. To address these problems, we propose a novel method that improves multi-label contrastive learning through label distribution. Specifically, when selecting positive and negative samples, we only need to consider whether there is an intersection between la- bels. To model the relationships between labels, we introduce two methods to recover label distributions from logical labels, based on Radial Basis Function (RBF) and contrastive loss, respectively. We evaluate our method on nine widely used multi-label datasets, in- cluding image and vector datasets. The results demonstrate that our method outperforms state-of-the-art methods in six evaluation metrics. Keywords: multi-label learning, contrastive learning, label distribution B. Corresponding author \u00a9 Preprint 2025 . Under review. Do not distribute. arXiv:2501.19145v1 [cs.LG] 31 Jan 2025"
}