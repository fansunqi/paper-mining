{
  "code_links": [
    "https://github.com/antoinedemathelin/obpam"
  ],
  "tasks": [
    "K-medoids Clustering"
  ],
  "datasets": [
    "MNIST",
    "CIFAR10",
    "UCI Datasets"
  ],
  "methods": [
    "OneBatchPAM"
  ],
  "results": [
    "Time complexity improved to O((p+ T )n log(n))",
    "Achieved similar performance to FasterPAM with reduced computational burden"
  ],
  "title": "OneBatchPAM A Fast and Frugal K-Medoids Algorithm.pdf",
  "abstract": "This paper proposes a novel k-medoids approximation algo- rithm to handle large-scale datasets with reasonable compu- tational time and memory complexity. We develop a local- search algorithm that iteratively improves the medoid selec- tion based on the estimation of the k-medoids objective. A single batch of size m \u226an provides the estimation, which reduces the required memory size and the number of pairwise dissimilarities computations to O(mn), instead of O(n2) compared to most k-medoids baselines. We obtain theoreti- cal results highlighting that a batch of size m = O(log(n)) is sufficient to guarantee, with strong probability, the same performance as the original local-search algorithm. Multiple experiments conducted on real datasets of various sizes and dimensions show that our algorithm provides similar perfor- mances as state-of-the-art methods such as FasterPAM and BanditPAM++ with a drastically reduced running time. Code \u2014 https://github.com/antoinedemathelin/obpam Introduction The k-medoids problem consists in choosing k medoids from a set of n points Xn, minimizing the sum of the pair- wise dissimilarities between the n points and their nearest medoid. This problem has many uses in machine learning, in particular for clustering, subset selection and active learn- ing (Bhat 2014; Wei, Iyer, and Bilmes 2015; Kaushal et al. 2019; de Mathelin et al. 2021). The k-medoids problem is related to k-medians, k-means and facility location (Schu- bert and Rousseeuw 2021). One specificity of k-medoids is to consider generic dissimilarities (non-necessarily met- ric). In machine learning applications, the dissimilarity func- tion can involve heavy computational costs, especially when computed between complex data types such as images, texts, or time series. The k-medoids problem is a discrete optimization prob- lem known to be NP-hard (Kariv and Hakimi 1979), for which a wide variety of approximation algorithms have been developed. Many k-medoids approximations are greedy or local-search algorithms, which improve a medoid selection sequentially by either adding or removing a medoid or swap- ping one medoid with another data point (Dohan, Karp, and Copyright \u00a9 2025, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Matejek 2015). The main local-search approach considered by the operations research communities is called PAM (Par- titioning Around Medoid) (Kaufman and Rousseeuw 1987; Kaufman 1990). This algorithm starts from an initial choice of k points (potentially greedily selected) and then performs a series of \u201dswaps\u201d. The state-of-the-art PAM algorithms are the FastPAM variants (Schubert and Rousseeuw 2021; Schu- bert and Lenssen 2022). A major drawback of these approximation algorithms is the computational burden encountered for large values of n. Indeed, the main algorithms require the computation and in-memory conservation of pairwise dissimilarities between the n points, resulting in a complexity of O(n2). Nowadays, with the rise of Big Data, and the focus on reducing compu- tational resources, there is a strong incentive to build algo- rithms that overcome this O(n2) limitation. Subsampling is a straightforward solution to reduce the number of dissimilarity calculations. The idea is to use an approximation algorithm (like PAM) on a subsample of size m \u226an selected among the n data points, resulting in a re- duction of the time and memory complexities from O(n2) to O(m2). Previous works have proven that this simple ap- proach yields appealing statistical guarantees over the ap- proximation error for relatively small batch size m (Mishra, Oblinger, and Pitt 2001; Thorup 2005; Mettu and Plaxton 2004; Meyerson, O\u2019callaghan, and Plotkin 2004; Huang, Jiang, and Lou 2023; Guha and Mishra 2016; Czumaj and Sohler 2007). In this category of methods, the CLARA al- gorithm (Clustering LARge Applications) (Kaufman 1986; Kaufman and Rousseeuw 2008) is the most commonly used. The main drawback of the subsampling approach is the loose approximation of considering only the medoid candidates in the m subsampled data points, resulting in worse cluster- ing quality (Tiwari et al. 2020). A recent method, Bandit- PAM, leverages Bandit algorithms to deal with this limita- tion (Tiwari et al. 2020, 2023). BanditPAM keeps the n data points as potential medoid candidates but only computes the dissimilarities for data points with high medoid potential, thus reducing the number of pairwise dissimilarity compu- tations to O(n log(n)) for one medoid selection or one swap step of the PAM algorithm. Although BanditPAM provides a medoid selection close to PAM (in terms of k-medoids ob- jective), the Bandit-based framework requires the computa- tion of new pairwise dissimilarities at each medoid selection, arXiv:2501.19285v1 [cs.LG] 31 Jan 2025"
}