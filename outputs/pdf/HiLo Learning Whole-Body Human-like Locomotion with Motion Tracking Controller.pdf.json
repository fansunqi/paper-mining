{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Humanoid robot locomotion control"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Deep Reinforcement Learning (DRL)",
    "Motion tracking controller",
    "Domain randomization",
    "Distributional value function"
  ],
  "results": [
    "Human-like locomotion",
    "Resilience to external disturbances",
    "Rapid adaptation of motion patterns",
    "Zero-shot transfer from simulation to real world"
  ],
  "title": "HiLo Learning Whole-Body Human-like Locomotion with Motion Tracking Controller.pdf",
  "abstract": "\u2014Deep Reinforcement Learning (RL) has emerged as a promising method to develop humanoid robot locomotion controllers. Despite the robust and stable locomotion demon- strated by previous RL controllers, their behavior often lacks the natural and agile motion patterns necessary for human-centric scenarios. In this work, we propose HiLo (human-like locomotion with motion tracking), an effective framework designed to learn RL policies that perform human-like locomotion. The primary challenges of human-like locomotion are complex reward engi- neering and domain randomization. HiLo overcomes these issues by developing an RL-based motion tracking controller and simple domain randomization through random force injection and action delay. Within the framework of HiLo, the whole-body control problem can be decomposed into two components: One part is solved using an open-loop control method, while the residual part is addressed with RL policies. A distributional value function is also implemented to stabilize the training process by improving the estimation of cumulative rewards under perturbed dynamics. Our experiments demonstrate that the motion tracking controller trained using HiLo can perform natural and agile human-like locomotion while exhibiting resilience to external disturbances in real-world systems. Furthermore, we show that the motion patterns of humanoid robots can be adapted through the residual mechanism without fine-tuning, allowing quick adjustments to task requirements. Index Terms - Whole-Body Control, Human-like Locomotion, Reinforcement Learning, Humanoid Robots, Sim-to-Real I. INTRODUCTION H UMANOID robotics has intrigued many research in- stitutes and companies due to its potential in general- purpose AI [7], [13] and diverse applications [14], [27]. These human-sized robots have advantages in tasks that require dex- terity and adaptability in human-centric settings. Whole-body locomotion control is a fundamental component that allows these robots to perform effectively. Traditional robot control approaches, such as model predictive control (MPC) [12], [26], have demonstrated impressive mobility through precise math- ematical models and predefined motion planning. However, these methods often struggle with robustness and general- ization in unknown or dynamically changing environments. Moreover, they require significant expertise for dynamics mod- eling and maintenance, which limits their broader applicability. Recently, Reinforcement Learning (RL) has emerged as a promising solution for legged robots [11]. RL allows robots to \u2217Equal contribution. \u2020Corresponding author. All authors are with the Department of Research and Development, Fourier Intelligence, Pudong, Shanghai 200120, China. {qiyuan.zhang, chenfan.weng, guanwu.li, fulai.he, yusheng.cai}@fftai.com This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. acquire complex motor skills through interactions with their environment, showing potential to overcome the limitations of traditional methods. Despite recent advances in RL for humanoid locomotion [9], the high dimensionality of humanoid robots poses sig- nificant challenges for RL policies to acquire human-like motion patterns required for human-centric scenarios. This is particularly evident in environments that require human interaction and cooperation [10], such as healthcare settings and collaborative tasks. Prior RL methods commonly struggle with reward engineering and domain randomization, which are critical components of learning natural and agile locomotion. Designing a reward function that accurately captures the de- sired behavior can be a laborious and time-consuming process. Given the complexity and inherent instability of humanoid dynamics, a well-designed reward function must also balance multiple objectives, including stability, energy efficiency, and natural movement. Domain randomization [23], on the other hand, requires the creation of training environments with diverse dynamics properties to ensure that the learned RL policy adapts to real-world systems. This requires a deep understanding of robot dynamics, potential discrepancy in real- world conditions, and the tasks at hand. These challenges have significantly limited the practicality and effectiveness of current RL methods to achieve truly human-like locomotion. In this work, we present HiLo (Human-like Locomo- tion with Motion Tracking), an effective RL policy learning framework for human-like locomotion. HiLo trains a motion tracking controller that optimizes a mimic reward, penalizing deviations from a reference walking motion. This approach simplifies the reward design and provides a clear signal for control. The motion tracking controller consists of two components: an open-loop control part based on the target reference and a residual component that is solved by an RL policy. This residual mechanism allows the RL policy to make corrections for deviations instead of having to learn locomotion from scratch. Additionally, the decomposition of the motion tracking controller offers flexibility for modifying motion patterns without fine-tuning. To enhance training sta- bility and efficiency, HiLo utilizes a distributional value func- tion to capture perturbed dynamics information, leading to a more accurate estimation of cumulative rewards. Furthermore, HiLo also integrates extended random force injection [25] with action delay to replace complex dynamics perturbation techniques. This integration improves policies\u2019 robustness and facilitates transfer from simulations to real-world applications. In summary, our contributions are four-fold: \u2022 Simple yet effective domain randomization for hu- arXiv:2502.03122v1 [cs.RO] 5 Feb 2025"
}