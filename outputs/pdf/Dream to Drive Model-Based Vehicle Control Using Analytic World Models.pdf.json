{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Model-based Vehicle Control",
    "World Modeling"
  ],
  "datasets": [
    "Waymo Open Motion Dataset (WOMD)"
  ],
  "methods": [
    "Analytic Policy Gradients (APG)",
    "Analytic World Models (AWMs)",
    "Model-Predictive Control (MPC)",
    "Differentiable Simulation"
  ],
  "results": [
    "12% improvement in average displacement error on WOMD",
    "Improved planning performance"
  ],
  "title": "Dream to Drive Model-Based Vehicle Control Using Analytic World Models.pdf",
  "abstract": "\u2014Differentiable simulators have recently shown great promise for training autonomous vehicle controllers. Being able to backpropagate through them, they can be placed into an end- to-end training loop where their known dynamics turn into useful priors for the policy to learn, removing the typical black box assumption of the environment. So far, these systems have only been used to train policies. However, this is not the end of the story in terms of what they can offer. Here, for the first time, we use them to train world models. Specifically, we present three new task setups that allow us to learn next state predictors, optimal planners, and optimal inverse states. Unlike analytic policy gradients (APG), which requires the gradient of the next simulator state with respect to the current actions, our proposed setups rely on the gradient of the next state with respect to the current state. We call this approach Analytic World Models (AWMs) and showcase its applications, including how to use it for planning in the Waymax simulator. Apart from pushing the limits of what is possible with such simulators, we offer an improved training recipe that increases performance on the large- scale Waymo Open Motion dataset by up to 12% compared to baselines at essentially no additional cost. I. INTRODUCTION Differentiable simulation has emerged as a powerful tool to train controllers and predictors across different domains like physics [22, 55, 31], graphics [26, 24, 65], and robotics [23, 8, 4, 50]. Within the field of autonomous vehicles (AVs), it was recently shown that differentiable motion dynamics can serve as a useful stepping stone for training robust and realistic vehicle policies [39]. The framework is straightforward and bears similarity to backpropagation through time \u2013 it involves rolling out a trajectory and supervising it with a ground- truth (GT) expert one. The process is sample-efficient because the gradients of the dynamics automatically guide the policy toward optimality and there is no search involved, unlike when the environment is treated as black box. Yet, this has only been explored for single policies, which are reactive [53] in their nature \u2013 at test-time they simply associate an action with each observation without providing any guarantee for their expected performance. Unlike them, model-based methods use planning at test time [44], which guarantees to maximize the estimated reward. They are con- sidered more interpretable compared to model-free methods, due to the simulated world dynamics, and more amenable to conditioning, which makes them potentially safer [42]. They are also sample-efficient due to the self-supervised training [5]. 1INSAIT, Sofia University \u201cSt. Kliment Ohridski\u201d, Sofia, Bulgaria 2University of Zurich, Zurich, Switzerland 3ETH Zurich, Zurich, Switzerland State Agent State Relative odometry Inverse state estimation State planning Differentiable Simulator Dynamics Action Agent Optimal Control World modeling (AWM) Control (APG) Fig. 1: Differentiable simulation allows for a variety of learning tasks. Previously, differentiable simulators have been used to train controllers using analytic policy gradients (bot- tom). Here, we propose to use them for learning relative odometry, state planning, and inverse state estimation (top). Consequently, the ability to plan at test time is a compelling requirement towards accurate and safe autonomous driving. An open question is whether model-based methods can be trained and utilized in a differentiable environment, and what would be the benefits of doing so. We tackle this question here. Naturally, planning requires learning a world model, but the concept of a world model is rather nuanced, as there are different ways to understand the effect of one\u2019s own actions. Fig. 1 shows our approach, which uses the differentiability of the simulator to formulate three novel tasks related to world modeling. First, the effect of an agent\u2019s action could be understood as the difference between the agent\u2019s next state and its current state. If a vehicle\u2019s state consists of its position, yaw, and velocity, then this setup has an odometric interpretation. Second, an agent could predict not an action, but a desired next state to visit, which is a form of state planning. Third, we can ask \u201dGiven an action in a particular state, what should the state be so that this action is optimal?\u201d, which is another form of world modeling but also an inverse problem. Thus, we are motivated to understand the kinds of tasks solvable in a differentiable simulator for vehicle motion. Policy learning with differentiable simulation is called Analytic Pol- icy Gradients (APG). Similarly, we call the proposed approach arXiv:2502.10012v1 [cs.AI] 14 Feb 2025"
}