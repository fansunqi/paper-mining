{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Zeroth-order Optimization"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "MeZO with Block Coordinate Descent (MeZO-BCD)"
  ],
  "results": [
    "Improved convergence with faster wall-clock time/iteration by up to \u00d72.09 while yielding similar or better accuracy"
  ],
  "title": "Unraveling Zeroth-Order Optimization Through the Lens of Low-Dimensional Structured Perturbations.pdf",
  "abstract": "Zeroth-order (ZO) optimization has emerged as a promising alternative to gradient-based back- propagation methods, particularly for black-box optimization and large language model (LLM) fine-tuning. However, ZO methods suffer from slow convergence due to high-variance stochastic gradient estimators. While structured perturba- tions, such as sparsity and low-rank constraints, have been explored to mitigate these issues, their effectiveness remains highly under-explored. In this work, we develop a unified theoretical frame- work that analyzes both the convergence and gen- eralization properties of ZO optimization under structured perturbations. We show that high di- mensionality is the primary bottleneck and in- troduce the notions of stable rank and effective overlap to explain how structured perturbations reduce gradient noise and accelerate convergence. Using the uniform stability under our framework, we then provide the first theoretical justification for why these perturbations enhance generaliza- tion. Additionally, through empirical analysis, we identify that block coordinate descent (BCD) to be an effective structured perturbation method. Extensive experiments show that, compared to ex- isting alternatives, memory-efficient ZO (MeZO) with BCD (MeZO-BCD) can provide improved converge with a faster wall-clock time/iteration by up to \u00d72.09 while yielding similar or better accuracy. 1. Introduction Zeroth-order (ZO) optimization (Shamir, 2013), sometimes referred to as gradient-free or derivative-free optimiza- tion, has emerged as a compelling alternative to traditional *Equal contribution 1Graduate School of AI, KAIST, Seoul, Republic of Korea 2Intel Labs, San Diego, USA 3AITRICS, Seoul, Republic of Korea. Correspondence to: Eunho Yang <eun- hoy@kaist.ac.kr>. Preprint. gradient-based methods in deep learning. Unlike first- or higher-order approaches that rely on an explicit gradient or Hessian information, ZO methods update model parame- ters solely based on the function evaluations. This property makes them particularly useful in settings where gradients are inaccessible or expensive to compute, such as black-box optimization (Cai et al., 2021; Zhang et al., 2022b), adversar- ial attacks (Chen et al., 2017; Kurakin et al., 2017; Papernot et al., 2017), hyperparameter tuning (Li et al., 2021), neu- ral architecture search (Wang et al., 2022), and efficient machine learning (Zhang et al., 2024a). Beyond these traditional applications, recently ZO optimiza- tion has gained attention for fine-tuning large language mod- els (LLMs) (Malladi et al., 2023; Guo et al., 2024; Gautam et al., 2024; Liu et al., 2024b; Zhang et al., 2024d; Chen et al., 2024; Yu et al., 2024; Park et al., 2024), primarily due to its significantly lower memory requirements com- pared to gradient-based optimizations. A pioneering study, MeZO (Malladi et al., 2023), demonstrated that zeroth-order stochastic gradient descent (SGD) (Robbins & Monro, 1951) can fine-tune language models using memory and computa- tional resources comparable to inference, making it a viable alternative for resource-constrained settings. However, despite its practicality, ZO optimization suffers from notoriously slow convergence and poor generaliza- tion, particularly in large-scale fine-tuning scenarios. Unlike first-order methods that benefit from exact gradient updates, ZO approaches rely on stochastic gradient estimates ob- tained via random perturbations, leading to inherently high variance and inefficient updates. While recent studies have attempted to address this issue by introducing structured per- turbations, such as sparse perturbations (Liu et al., 2024b; Guo et al., 2024) and low-rank perturbations (Chen et al., 2024; Yu et al., 2024), the underlying principles that govern their effectiveness remain under-explored. Specifically, no unified framework exists to explain why these structures improve ZO efficiency or which structural properties are critical. Furthermore, the generalization properties of ZO op- timization also remain elusive, as existing theoretical studies primarily focus on convergence while neglecting its impact on model generalization. In this work, we provide a unified framework to analyze 1 arXiv:2501.19099v1 [cs.LG] 31 Jan 2025"
}