{
  "code_links": [
    "https://github.com/yichao-yuan-99/Vortex"
  ],
  "tasks": [
    "GPU-accelerated large-scale data analytics"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Optimized IO primitive",
    "IO-decoupled programming model",
    "Tailored query execution",
    "Late materialization"
  ],
  "results": [
    "Performance gain of 5.7\u00d7 over state-of-the-art GPU baselines",
    "Improved price-performance of 2.5\u00d7 compared to a CPU baseline"
  ],
  "title": "Vortex Overcoming Memory Capacity Limitations in GPU-Accelerated Large-Scale Data Analytics.pdf",
  "abstract": "Despite the high computational throughput of GPUs, limited mem- ory capacity and bandwidth-limited CPU-GPU communication via PCIe links remain significant bottlenecks for accelerating large- scale data analytics workloads. This paper introduces Vortex, a GPU-accelerated framework designed for data analytics workloads that exceed GPU memory capacity. A key aspect of our framework is an optimized IO primitive that leverages all available PCIe links in multi-GPU systems for the IO demand of a single target GPU. It routes data through other GPUs to such target GPU that handles IO-intensive analytics tasks. This approach is advantageous when other GPUs are occupied with compute-bound workloads, such as popular AI applications that typically underutilize IO resources. We also introduce a novel programming model that separates GPU kernel development from IO scheduling, reducing programmer bur- den and enabling GPU code reuse. Additionally, we present the design of certain important query operators and discuss a late ma- terialization technique based on GPU\u2019s zero-copy memory access. Without caching any data in GPU memory, Vortex improves the performance of the state-of-the-art GPU baseline, Proteus, by 5.7\u00d7 on average and enhances price performance by 2.5\u00d7 compared to a CPU-based DuckDB baseline. PVLDB Reference Format: Yichao Yuan, Advait Iyer, Lin Ma, and Nishil Talati. Vortex: Overcoming Memory Capacity Limitations in GPU-Accelerated Large-Scale Data Analytics. PVLDB, 18(4): XXX-XXX, 2024. doi:XX.XX/XXX.XX PVLDB Artifact Availability: The source code, data, and/or other artifacts have been made available at https://github.com/yichao-yuan-99/Vortex. 1 INTRODUCTION GPUs, with their massively parallel architecture, offer high compu- tational power and memory throughput, making them an attractive choice for accelerating large-scale data analytics. However, a signif- icant limitation is the memory capacity of GPUs, which is typically constrained to tens or low hundreds of gigabytes in modern hard- ware. In contrast, CPU memory has reached capacities of multiple terabytes. Transferring data from CPU memory to GPU memory This work is licensed under the Creative Commons BY-NC-ND 4.0 International License. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of this license. For any use beyond those covered by this license, obtain permission by emailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights licensed to the VLDB Endowment. Proceedings of the VLDB Endowment, Vol. 18, No. 4 ISSN 2150-8097. doi:XX.XX/XXX.XX is often bottlenecked by the limited bandwidth of interconnect links such as PCIe. This combination of capacity-limited GPU mem- ory and bandwidth-limited CPU-GPU communication significantly restricts the acceleration potential of GPUs, as real-world data ana- lytics workloads often require processing large datasets that by far exceed the GPU memory capacity. Prior works addressing this issue can be broadly categorized into two approaches: (1) utilizing multiple GPUs [29, 39, 45], and (2) leveraging CPU-side memory for data processing [12, 16, 27, 30, 33, 59]. The first approach employs multiple GPUs to horizontally scale memory capacity. However, this also scales computational resources, maintaining a constant compute-to-memory ratio, and may lead to under-utilization of expensive GPU resources. In the second category, some works stream data from CPU memory to GPU [16, 27, 33], but still face the bandwidth limitations of PCIe links. Additionally, the CPU-GPU hybrid execution approach [12, 30, 59] needs to deal with the large disparity in computational power between CPUs and GPUs, risking leaving GPU resources idle when CPU falls on the critical path. This raises a critical question: how can we exploit the massively parallel GPU architecture to accelerate data analytics workloads that exceed GPU memory capacity? In this paper, we present Vortex\u2014a GPU-accelerated framework for large-scale data analytics that addresses memory capacity lim- itations from a fresh perspective. Vortex has two crucial design goals. First, it processes workload sizes that significantly exceed GPU memory capacity. It assumes no data caching in GPU memory before the execution of any query. Second, Vortex is designed to have IO scheduling independent of GPU kernel optimization, alleviating the burden on the GPU programmer. This unique programming model also maximizes the reuse of optimized GPU code. The design of Vortex is structured into three layers. With the evolution of multi-GPU systems, we identify a unique opportunity to leverage the IO resources of all GPUs on such systems to transfer data to a single GPU executing data analytics. Based on this, at a bottom design layer, we propose an optimized IO primitive that fully exploits the bandwidth available from all PCIe links and inter- GPU communication fabric to transfer data from CPU memory to the GPU at high speeds. This design is motivated by the observation that today\u2019s data processing platforms serve both data analytics and AI workloads to support intelligent workflows and diverse needs from users. AI workloads tend to be compute-bound, leaving their IO resources underutilized. We aim to co-locate compute-bound workloads and data analytics on the same multi-GPU server to process hybrid requests from users. Our primitive repurposes these idle IO resources from other GPUs to forward data to the target GPU that handles IO-intensive data analytics. arXiv:2502.09541v1 [cs.DB] 13 Feb 2025"
}