{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Brain-Computer Interface"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "SSVEP speller",
    "LLM API",
    "Multilingual Support",
    "Dynamic SSVEP interface generation",
    "Cross-domain applications"
  ],
  "results": [
    "None"
  ],
  "title": "An Innovative Brain-Computer Interface Interaction System Based on the Large Language Model.pdf",
  "abstract": "Recent advancements in large language models (LLMs) provide a more effective pathway for upgrading brain-computer interface (BCI) technology in terms of user interaction. The widespread adoption of BCIs in daily application scenarios is still limited by factors such as their single functionality, restricted paradigm design, weak multilingual support, and low levels of intelligence. In this paper, we propose an innovative BCI system that deeply integrates a steady-state visual evoked potential (SSVEP) speller with an LLM application programming interface (API). It allows natural language input through the SSVEP speller and dynamically calls large models to generate SSVEP paradigms. The command prompt, blinking frequency, and layout position are adjustable to meet the user\u2019s control requirements in various scenarios. More than ten languages are compatible with the multilingual support of LLM. A variety of task scenarios, such as home appliance control, robotic arm operation, and unmanned aerial vehicle (UAV) management are provided. The task interfaces of the system can be personalized according to the user\u2019s habits, usage scenarios, and equipment characteristics. By combining the SSVEP speller with an LLM, the system solves numerous challenges faced by current BCI systems and makes breakthroughs in functionality, intelligence, and multilingual support. The introduction of LLM not only enhances user experience but also expands the potential applications of BCI technology in real-world environments. Keywords: Brain-computer interface (BCI), steady-state visual evoked potential (SSVEP), large language model (LLM), multilingual support, cross- domain applications 1. Introduction Brain-computer interface (BCI) is a technology that directly realizes information transmission and interaction between the brain and external devices [1]. Through a BCI system, human brain activity can be read, decoded, and converted into commands that can be understood and executed by an external device. The core of BCI technology lies in the conversion of electroencephalographic (EEG) signals (usually obtained through non-invasive or invasive methods) into control signals for human-computer interaction [2], [3]. BCI has been widely used in many fields, with neurorehabilitation [4] and medical assistance [5] being one of the most mature applications. Through BIC technology, paralyzed patients, patients with movement disorders, etc. can control external devices (e.g., prosthetics [6], wheelchairs [7], computers [8], etc.) to perform daily life operations, which greatly improves their quality of life. In recent years, brain-computer interfaces (BCIs) have achieved remarkable results in stroke rehabilitation and motor recovery for patients with spinal cord injuries. In particular, there have been preliminary clinical applications in restoring patients' motor functions through BCI systems [9], [10]. In addition, BCI technology is also widely used in home control [11], drone control [12], robotic arm control [13], and other fields [14]. Through brainwave control, users can realize the control of different devices. In addition, the potential of BCI in all aspects of life is gradually being tapped. However, the popularity of BCI systems faces the following limitations: First, most of the existing BCI systems are mostly limited to some specific functions, such as simple letter spelling [15], gesture control [16], or lightweight device control [17]. Although SSVEP (Steady State Visual Evoked Potential) technology[18] performs well in spelling systems, existing systems appear to be under-functional for complex device control needs, such as smart home, drone manipulation, or complex task planning. Second, most of the existing SSVEP paradigm designs rely on pre-set flicker frequencies and position layouts, making it difficult to dynamically generate personalized control interfaces [19]. This makes users need to rely on fixed and limited interfaces to interact in different scenarios, reducing the flexibility and practicality of the system. Third, most of the existing BCI systems are limited to specific languages [20], especially non-English speaking users who often lack complete localization support when using them. Cross-language support is very limited, especially with the lack of integration with natural language understanding and generation. Last, although some BCI systems can perform relatively simple commands, such as controlling electrical switches or robot movement, the systems are unable to reason based on complex semantics, automatically generate"
}