{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Human imitation learning",
    "Robot learning",
    "Embodied intelligence",
    "Virtual reality"
  ],
  "datasets": [
    "EgoMe"
  ],
  "methods": [
    "Egocentric view",
    "Exocentric view",
    "Sensor data integration",
    "Multimodal data analysis"
  ],
  "results": [
    "None"
  ],
  "title": "EgoMe Follow Me Via Egocentric View in Real World.pdf",
  "abstract": "When interacting with the real world, human often take the egocentric (first-person) view as a benchmark, naturally transferring behaviors observed from a exocentric (third- person) view to their own. This cognitive theory provides a foundation for researching how robots can more effectively imitate human behavior. However, current research either employs multiple cameras with different views focusing on the same individual\u2019s behavior simultaneously or encoun- ters unpair ego-exo view scenarios, there is no effort to fully exploit human cognitive behavior in the real world. To fill this gap, in this paper, we introduce a novel large-scale ego- centric dataset, called EgoMe, which towards following the process of human imitation learning via egocentric view in the real world. Our dataset includes 7902 pairs of videos (15804 videos) for diverse daily behaviors in real-world scenarios. For a pair of videos, one video captures a ex- ocentric view of the imitator observing the demonstrator\u2019s actions, while the other captures a egocentric view of the imitator subsequently following those actions. Notably, our dataset also contain exo-ego eye gaze, angular velocity, ac- celeration, magnetic strength and other sensor multi-modal data for assisting in establishing correlations between ob- serving and following process. In addition, we also propose eight challenging benchmark tasks for fully leveraging this data resource and promoting the research of robot imitation learning ability. Extensive statistical analysis demonstrates significant advantages compared to existing datasets. The proposed EgoMe dataset and benchmark will be released soon. 1. Introduction In daily life, human beings can observe the behaviors of other individuals and follow them, skillfully applying the acquired knowledge and skills to their own. This pro- cess not only reflects the natural instinct of human cognition but also provides profound insights into the exploration of Figure 1: Our EgoMe dataset takes the egocentric view as a benchmark, following behaviors observed from a exocen- tric view to their own actions for exploring the process of human imitation learning in the real world. robotic learning. By endowing robots with the capability to imitate human behavior, it is significant to achieve rapid skill improvement and diversified skill accumulation, espe- cially when dealing with complex tasks such as intricate manufacturing processes, precision surgeries, or emergency response operations. It holds profound significance for the development of the next generation of AI agents including embodied intelligence [13] and virtual reality [32, 55]. To effectively acquire this ability, it is essential for the robot to directly observe and follow human behaviors from its own viewpoint, encompassing its dual roles as both an ob- server (exocentric/third-person view), meticulously record- ing and analyzing actions of demonstrator , and as a imi- tator (egocentric/first-person view), actively following and understanding the behaviors from within. In the early stage, tremendous amount of research pri- marily focus on observing and analyzing human behavior through a exocentric (third-person) viewpoint [23, 14, 5, 10, 5, 25, 29, 2, 49, 62, 33, 37] (i.e., the observer\u2019s viewpoint). Although these methods can capture the external manifesta- tions of behavior, it often struggles to deeply understand and appreciate the motivations and internal logic behind these behaviors. With the emergence of wearable devices, many research opportunities have arisen for a egocentric (first- arXiv:2501.19061v1 [cs.CV] 31 Jan 2025"
}