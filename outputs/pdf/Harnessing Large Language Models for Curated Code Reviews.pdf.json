{
  "code_links": [
    "https://zenodo.org/records/14812107",
    "https://github.com/OussamaSghaier/CuREV"
  ],
  "tasks": [
    "Code Review",
    "Automated Comment Generation",
    "Code Refinement"
  ],
  "datasets": [
    "Code Review Dataset"
  ],
  "methods": [
    "LLM-as-a-Judge",
    "Curated Pipeline",
    "Reformulation"
  ],
  "results": [
    "BLEU: 11.26 for Comment Generation",
    "CodeBLEU: 0.44 for Code Refinement"
  ],
  "title": "Harnessing Large Language Models for Curated Code Reviews.pdf",
  "abstract": "\u2014In code review, generating structured and relevant comments is crucial for identifying code issues and facilitating accurate code changes that ensure an efficient code review process. Well-crafted comments not only streamline the code review itself but are also essential for subsequent tasks like code refinement, where the code is modified to satisfy the input review comment. Although various AI-based approaches aimed to automate comment generation, their effectiveness remains limited by the quality of the training data. Existing code review datasets are often noisy and unrefined, posing limitations to the learning potential of AI models and hindering the automation process. To address these challenges, we propose a curation pipeline designed to enhance the quality of the largest publicly available code review dataset. We begin by establishing an evaluation framework, incorporating specific criteria and categories to empirically study the initial quality of the dataset. Using a large language model (LLM)-driven approach, we then apply our curation pipeline to refine the dataset. A comparative analysis of the newly curated dataset, based on the same evaluation frame- work, demonstrates substantial improvements in the clarity and conciseness of the comments. Additionally, we assess the impact of the curated dataset on automating downstream tasks, specifically comment generation and code refinement. Our findings show that the curated dataset leads to enhanced model performance in generating more accurate comments. Curated comments are also more useful as they lead to more accurate code refinement. Index Terms\u2014Code review, large language models, software maintenance, empirical software engineering. I. INTRODUTION Code review is a critical component of the software de- velopment life cycle, aimed at identifying issues, detecting suboptimal code, and uncovering bugs [1], [2], while ensuring the overall quality and maintainability of the source code [3]\u2013 [5]. This process typically involves a manual inspection of code by one or more developers, reviewing code written by their peers [6], [7]. The code review process consists of several key tasks, with the most essential being the identification and documentation of potential issues through review comments, the subsequent code refinement to resolve these concerns, and the quality assessment of the submitted code to decide if it should be accepted or needs further review. Issue identification and description (i.e., review comment generation) constitutes a foundational task in code review, focusing on the detection of defects or problems within the The data is available at https://zenodo.org/records/14812107. The replica- tion package is available at https://github.com/OussamaSghaier/CuREV. code. This phase is pivotal, as it involves not only identifying specific issues but also offers potential solutions for resolv- ing them [1], [2]. The significance of this task cannot be overstated, as subsequent stages of the review process heavily depend on its accuracy and thoroughness. Code refinement, for example, is an execution phase directly tied to insights gained from issue identification; developers revise and improve the code based on comments provided during this stage [8]. Thus, comment generation is essential to the entire code review process. Without precise execution of this task, the integrity and quality of later stages is compromised. As such, this task must be carried out meticulously, ensuring that issues and improvements are thoroughly examined and well-documented. Automating comment generation has gained increasing at- tention, with various techniques proposed to improve accuracy and efficiency [9]\u2013[13]. These methods typically use machine learning and natural language processing to analyze code and generate comments that identify defects and suggest fixes. However, current systems still fall short of matching human reviewers in detecting nuanced issues and providing context- aware suggestions. There is significant room for improvement across multiple facets of this task, from enhancing the quality of the existing datasets to developing more sophisticated gen- erative language models. Current models often struggle with producing comments that are both accurate and actionable, indicating a need for more refined approaches that can bridge the gap between human expertise and automated solutions. One key element to the success of deep learning models is the quality of training data. Language models cannot perform effectively if the training data is flawed or insufficient. Thus, high-quality data is foundational to the learning process, as models are only as good as the data they are trained on. In the case of code review, the datasets currently available [11], [14], while valuable, often suffer from several limitations. These datasets are frequently extracted in a raw form from reposito- ries and are not curated or pre-processed to ensure their quality. As a result, they may contain various forms of noise, such as uncivil comments, irrelevant or poorly structured reviews, grammatical errors, or irrelevant comments. These issues not only hinder the learning process but can also lead to models learning undesirable patterns, such as generating irrelevant or incoherent feedback, thereby undermining the effectiveness of the automated task. Consequently, improving the quality of arXiv:2502.03425v1 [cs.SE] 5 Feb 2025"
}