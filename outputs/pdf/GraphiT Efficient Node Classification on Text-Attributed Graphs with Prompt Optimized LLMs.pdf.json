{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Node classification"
  ],
  "datasets": [
    "Cora",
    "PubMed",
    "Ogbn-arxiv"
  ],
  "methods": [
    "GraphiT (Graphs in Text)",
    "DSPy framework",
    "Keyphrase extraction"
  ],
  "results": [
    "GraphiT outperforms baselines on Cora, PubMed, and Ogbn-arxiv",
    "Lower context length compared to neighbor summaries"
  ],
  "title": "GraphiT Efficient Node Classification on Text-Attributed Graphs with Prompt Optimized LLMs.pdf",
  "abstract": "The application of large language models (LLMs) to graph data has attracted a lot of attention recently. LLMs allow us to use deep contextual embeddings from pretrained models in text-attributed graphs, where shallow embeddings are often used for the text at- tributes of nodes. However, it is still challenging to efficiently en- code the graph structure and features into a sequential form for use by LLMs. In addition, the performance of an LLM alone, is highly dependent on the structure of the input prompt, which limits their effectiveness as a reliable approach and often requires iterative man- ual adjustments that could be slow, tedious and difficult to replicate programmatically. In this paper, we propose GraphiT (Graphs in Text), a framework for encoding graphs into a textual format and optimizing LLM prompts for graph prediction tasks. Here we focus on node classification for text-attributed graphs. We encode the graph data for every node and its neighborhood into a concise text to enable LLMs to better utilize the information in the graph. We then further programmatically optimize the LLM prompts us- ing the DSPy framework to automate this step and make it more efficient and reproducible. GraphiT outperforms our LLM-based baselines on three datasets and we show how the optimization step in GraphiT leads to measurably better results without manual prompt tweaking. We also demonstrated that our graph encoding approach is competitive to other graph encoding methods while being less expensive because it uses significantly less tokens for the same task. Keywords Graphs, Large language models, Node classification, DSPy ACM Reference Format: Shima Khoshraftar, Niaz Abedini and Amir Hajian. 2025. GraphiT: Efficient Node Classification on Text-Attributed Graphs with Prompt Optimized LLMs. In . ACM, Sydney, Australia, 6 pages. https://doi.org/10.1145/nnnnnnn. nnnnnnn 1 Introduction Graphs are powerful tools for representing entities and the rela- tionships between them in different applications such as social networks and citation networks. For instance, in a citation network, nodes are the articles and there is an edge between two articles if one article cites another one. In text-attributed graphs, nodes have text attributes which provide further information about the nodes. In the citation network described above, the text attributes of a node could be the content of the associated article. One of the 0Corresponding author: shima.khoshraftar@arteria.ai Conference\u201925, Sydney, Australia 2025. ACM ISBN 978-x-xxxx-xxxx-x/YYYY/MM https://doi.org/10.1145/nnnnnnn.nnnnnnn main applications of graphs is the node classification task in which a model predicts a label for the nodes in the test set. Graph Neural Nets (GNNs) [14, 32] are the state of the art in graph representation learning. They typically generate a node em- bedding by aggregating the embeddings of neighbors of the node in a message passing mechanism [15, 25]. GNNs consider the structure and the attributes of graphs in generating embeddings. The text at- tributes of nodes are often represented by shallow embeddings such as bag-of-words [12] and word2vec [18] which can not capture the contextual relationships between words in text attributes. However, large language models have demonstrated great success in gener- ating contextual text embeddings with superior performance than shallow embeddings in natural language processing (NLP) tasks. The success of the LLM models is mainly due to their pre-training on a vast amount of text corpora which gives them massive knowl- edge and semantic comprehension capabilities. Hence, many recent efforts have explored combining LLMs and GNNs [5, 10, 28, 31, 33]. While effective, this combination results in a complex system involv- ing two large models which increase the computational demands and require labeled training data. Consequently, other studies focused on evaluating the potential of LLMs to act as standalone models for both embedding generation and prediction [2, 7, 21, 30]. These methods employ various tech- niques for optimizing LLMs, which can be broadly categorized into prompt engineering [7] which relies heavily on manual adjustments or fine-tuning which require labeled training data [30]. Additionally, different approaches are explored for converting graph structures into sequential formats suitable for LLMs, including using text at- tributes, lists of a node\u2019s neighbors [30], and neighbor summaries [2] which can lead to increasing the context length of prompts making the LLM calls more expensive. In this paper, we investigate the promise and limitations of using LLMs for the node classification tasks by proposing new approaches for graph encoding and prompt optimization in terms of instruc- tion and examples using DSPy framework [13]. Specifically, we use a prompt programming approach which automates the optimiza- tion of LLMs for node classification without extra training, manual tweaks and with a small set of labeled data. Furthermore, we pro- pose using keyphrases of neighbor nodes to represent a node, which offer several advantages. First, keyphrases require significantly less of the LLMs\u2019 context window while effectively conveying the key points. Second, when neighbor summaries are lengthy, LLMs may experience the \"lost-in-the-middle\" effect [17], where critical in- formation representing a node\u2019s neighbors is overlooked. Lastly, in certain graph applications, including multi-hop neighbors is essential. However, summarizing such extended neighborhood in- formation becomes challenging and less interpretable. By using keyphrases, we can generate concise yet informative summaries arXiv:2502.10522v1 [cs.AI] 14 Feb 2025"
}