{
  "code_links": "None",
  "tasks": [
    "Efficient acceleration of high-precision deep neural networks"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Hybrid-domain floating-point compute-in-memory architecture"
  ],
  "results": [
    "Energy efficiency: 1.53x compared to fully digital baselines",
    "Minimal area overhead",
    "Negligible accuracy loss"
  ],
  "title": "A Hybrid-Domain Floating-Point Compute-in-Memory Architecture for Efficient Acceleration of High-Pre.pdf",
  "abstract": "\u2014Compute-in-memory (CIM) has shown significant potential in efficiently accelerating deep neural networks (DNNs) at the edge, particularly in speeding up quantized models for inference applications. Recently, there has been growing interest in developing floating-point-based CIM macros to improve the accuracy of high-precision DNN models, including both inference and training tasks. Yet, current implementations rely primarily on digital methods, leading to substantial power consumption. This paper introduces a hybrid domain CIM architecture that integrates analog and digital CIM within the same memory cell to efficiently accelerate high-precision DNNs. Specifically, we develop area-efficient circuits and energy-efficient analog-to-digital conversion techniques to realize this architecture. Comprehensive circuit-level simulations reveal the notable energy efficiency and lossless accuracy of the proposed design on benchmarks. I. INTRODUCTION Compute-in-memory (CIM) holds significant promise for ef- ficiently accelerating deep neural networks (DNNs) at the edge by bringing computation and memory closer together to reduce energy-intensive data movement that occurs in conventional von Neumann architecture [1]\u2013[4]. Although numerous existing works have focused on building integer-based (INT-based) CIM macros [5]\u2013[9] to improve the energy efficiency of quantized DNN models, there is growing interest in developing floating- point-based (FP-based) CIM macros to enhance the accuracy of high-precision DNN models, including both inference and training applications. Yet, most current FP CIM methods rely on digital implementation, leading to significant power con- sumption [10]\u2013[12]. Novel approaches are needed to bridge this gap, ensuring both high accuracy and energy efficiency for increasing high-precision DNN applications. This paper proposes a novel hybrid-domain FP CIM architec- ture to remarkably improve energy efficiency while maintaining lossless accuracy for high-precision DNN applications. The proposed architecture is based on a key observation that has been significantly overlooked in conventional designs of FP CIM macros. It is observed that the FP arithmetic can be in- trinsically divided into two parts: (1) the computation-intensive multiplication (sub-MUL), contributing less than 1/4 to FP products, and (2) the computation-light addition (sub-ADD), contributing more than 3/4 to FP products, as elaborated in Section III-A. We harness this insight to strategically integrate both analog CIM (for energy-efficient sub-MUL) and digital CIM (for accurate sub-ADD) on a unified hardware substrate. This hybrid-domain CIM strategy, to the best of our knowledge, optimally combines the strengths of analog and digital CIM to achieve state-of-the-art energy efficiency by maintaining equivalent accuracy compared to fully digital baselines. The key contributions of the work are listed below. \u2022 We propose a novel hybrid-domain FP CIM architecture based on static random-access memory (SRAM) that inte- grates both digital and analog CIM within a unified macro to accelerate high-precision DNN applications. \u2022 We develop detailed circuit schematics and physical lay- outs to implement this architecture. We optimize the local computing logic with minimal area overhead and minimize the energy cost of analog-to-digital conversion. \u2022 Experimental evaluations with comprehensive circuit-level simulations demonstrate the exceptional energy efficiency and accuracy of the proposed architecture compared to conventional fully digital baselines. II. BACKGROUND AND RELATED WORK A. Floating-Point Arithmetic Primitives FP arithmetic operations are crucial to high-precision DNNs. FP format: A general FP number in scientific notation is expressed as f = (\u22121)S \u00b7 2E \u00b7 1.M, where S (S = 0 or S = 1), E, and M (M \u2208(0, 1)) represent the sign, exponent, and mantissa (fraction) of the number, respectively. The \u20181\u2019 before M is a hidden bit that is not explicitly shown in the binary format of an FP number. E is the actual exponent, with an offset applied to the exponent encoded in the standard format. For example, in the IEEE 8-bit FP format (FP8, E4M3 or E5M2), S is 1-bit, E is 4(5)-bit, M is 3(2)-bit (Fig. 1(b)). FP multiplication: The multiplication of FP numbers is a straightforward process, involving exponent addition ( 1\u20dd) and mantissa multiplication ( 2\u20dd). For simplicity, the example below demonstrates this using two positive FP numbers. (2E0 \u00b7 1.M0) \u00b7 (2E1 \u00b7 1.M1) = (2E0+E1) | {z } 1 \u20dd \u00b7 (1.M0 \u00b7 1.M1) | {z } 2 \u20dd . (1) FP addition: However, the addition of floating-point (FP) numbers is more complex and can be expressed as follows: 2E0\u00b71.M0+2E1\u00b71.M1 = 2Emax | {z } 1 \u20dd \u00b7 \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 2E0\u2212Emax | {z } 2 \u20dd \u00b7 1.M0 + 2E1\u2212Emax | {z } 2 \u20dd \u00b7 1.M1 | {z } 3 \u20dd . (2) arXiv:2502.07212v1 [cs.AR] 11 Feb 2025"
}