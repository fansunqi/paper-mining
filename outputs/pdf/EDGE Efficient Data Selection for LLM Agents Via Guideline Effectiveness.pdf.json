{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Data selection for LLM agents"
  ],
  "datasets": [
    "WebShop",
    "HotpotQA"
  ],
  "methods": [
    "Guideline Effectiveness (GE) metric",
    "Active learning"
  ],
  "results": [
    "Achieves state-of-the-art performance with 75% and 50% less data",
    "Improves efficiency and outcomes of prompt engineering and fine-tuning"
  ],
  "title": "EDGE Efficient Data Selection for LLM Agents Via Guideline Effectiveness.pdf",
  "abstract": "Large Language Models (LLMs) have shown re- markable capabilities as AI agents. However, ex- isting methods for enhancing LLM-agent abilities often lack a focus on data quality, leading to ineffi- ciencies and suboptimal results in both fine-tuning and prompt engineering. To address this issue, we introduce EDGE, a novel approach for identi- fying informative samples without needing golden answers. We propose the Guideline Effectiveness (GE) metric, which selects challenging samples by measuring the impact of human-provided guide- lines in multi-turn interaction tasks. A low GE score indicates that the human expertise required for a sample is missing from the guideline, making the sample more informative. By selecting sam- ples with low GE scores, we can improve the ef- ficiency and outcomes of both prompt engineer- ing and fine-tuning processes for LLMs. Exten- sive experiments validate the performance of our method. Our method achieves competitive results on the HotpotQA and WebShop and datasets, re- quiring 75% and 50% less data, respectively, while outperforming existing methods. We also provide a fresh perspective on the data quality of LLM-agent fine-tuning. 1 Introduction In recent years, Large Language Models (LLMs) [Ouyang et al., 2022; OpenAI, 2023] have demonstrated remarkable few- shot learning and reasoning capabilities. An increasing num- ber of studies have begun exploring how to leverage LLMs as agents that can accomplish various tasks through mul- tiple interactions with the environment [Deng et al., 2023; Liu et al., 2024b; Wang et al., 2024]. For example, Web- Shop [Yao et al., 2022] provides a simulated shopping envi- ronment where agents must select products that best match user requirements. During interactions, LLMs frequently encounter complex or previously unseen scenarios, which places substantial de- \u2217Corresponding author. mands on their generalization capabilities. Numerous studies have been dedicated to mitigating this challenge. Prior work has demonstrated the importance of guidelines (or insights) in prompt-based multi-turn interaction methods. Guidelines are natural language prompts summarized from data that contain more information and cover more scenar- ios than exemplars, while typically consuming less context space. Existing approaches autonomously gather experiences from training tasks through trial and error to generate these guidelines [Zhao et al., 2024]. Another line of research focuses on Supervised Fine- Tuning (SFT) of open-source LLMs to enhance their instruction-following capabilities. Prior work has shown that the effectiveness of SFT depends more on dataset quality than quantity [Wang et al., 2023; Zhou et al., 2023]. Current data filtering approaches, including GPT-4-based scoring [Chen et al., 2024], instruction difficulty assessment [Li et al., 2024], and semantic diversity metrics [Lu et al., 2024], have demon- strated varying degrees of success. Despite these advancements, current LLM-agent ap- proaches still face several pressing challenges. In prompt- based methods, existing approaches for obtaining guidelines do not consider data quality control, instead randomly se- lecting samples from annotated data, which not only requires substantial and costly annotation efforts but also suffers from noisy data problems. Meanwhile, in SFT-based methods, cur- rent approaches heavily rely on golden answer feedback and primarily focus on single-turn instruction tuning, lacking necessary exploration of more complex multi-turn interaction scenarios that are essential for real-world applications. To address these challenges, we propose Efficient Data se- lection for LLM agents via Guideline Effectiveness, a novel framework centered around a new metric called Guideline Ef- fectiveness (GE) to select the most informative subset of sam- ples from a vast unlabeled data (query) pool. These selected samples can be utilized for both prompt engineering and SFT. Guidelines represent human understanding of tasks and serve as prior knowledge for agents, encompassing tool us- age patterns and comprehension of complex scenarios [Zhao et al., 2024; Fu et al., 2024]. The GE score essentially quanti- fies the impact of guidelines on each data sample, enabling us to identify which samples are most challenging for the model and thus select more informative ones. Beginning with an initial guideline, we employ an active learning approach to arXiv:2502.12494v1 [cs.LG] 18 Feb 2025"
}