{
  "code_links": [
    "https://github.com/inspire-group/continual_robust_training/"
  ],
  "tasks": [
    "Continual Adaptive Robustness"
  ],
  "datasets": [
    "CIFAR-10",
    "CIFAR-100",
    "ImageNette"
  ],
  "methods": [
    "Continual Robust Training",
    "Adversarial L2 Regularization",
    "Gaussian Regularization",
    "Uniform Regularization",
    "Variation Regularization"
  ],
  "results": [
    "Improved robustness on both known and unforeseen attacks",
    "Increased Union accuracy with regularization",
    "Reduced degradation on previous attacks with regularization",
    "Improved performance on unforeseen attacks with regularization"
  ],
  "title": "Adapting to Evolving Adversaries with Regularized Continual Robust Training.pdf",
  "abstract": "Robust training methods typically defend against specific attack types, such as \u2113p attacks with fixed budgets, and rarely account for the fact that de- fenders may encounter new attacks over time. A natural solution is to adapt the defended model to new adversaries as they arise via fine-tuning, a method which we call continual robust train- ing (CRT). However, when implemented naively, fine-tuning on new attacks degrades robustness on previous attacks. This raises the question: how can we improve the initial training and fine-tuning of the model to simultaneously achieve robustness against previous and new attacks? We present theoretical results which show that the gap in a model\u2019s robustness against different attacks is bounded by how far each attack perturbs a sample in the model\u2019s logit space, suggesting that regular- izing with respect to this logit space distance can help maintain robustness against previous attacks. Extensive experiments on 3 datasets (CIFAR-10, CIFAR-100, and ImageNette) and over 100 at- tack combinations demonstrate that the proposed regularization improves robust accuracy with lit- tle overhead in training time. Our findings and open-source code1 lay the groundwork for the deployment of models robust to evolving attacks. 1. Introduction For safety critical applications, it is important to defend ma- chine learning (ML) models against test-time attacks. How- ever, many existing defenses (Madry et al., 2018; Zhang et al., 2019; Croce et al., 2020) assume that the adversary is restricted to a narrow threat model such as an \u2113p ball of fixed radius around the input. When this assumption is violated, *Equal contribution 1Department of Electrical and Computer Engineering, Princeton University 2Department of Computer Sci- ence, University of Chicago 3Google Deepmind. Correspondence to: Sihui Dai <sihuid@princeton.edu>, Christian Cianfarani <crc@uchicago.edu>. Preprint. 1Our code is available at: https://github.com/ inspire-group/continual_robust_training/ No ALR ALR No ALR ALR 0 20 40 60 80 100 Robust Accuracy (%) t = 0 t = 1 Train on \u21132 Attack Fine-tune on StAdv \u21132 Attack StAdv Attack Figure 1: Impact of our proposed regularization term (ALR) in both training and fine-tuning on CIFAR-10. Adversarial \u21132 regularization (ALR) significantly improves generalization to the unforeseen StAdv attack when per- forming adversarial training for \u21132 robustness. Using ALR when subsequently fine-tuning with only StAdv attack also decreases the drop in \u21132 robustness. the robustness of adversarially trained models can signif- icantly degrade (Dai et al., 2023; Kaufmann et al., 2019). Additionally, due to rapid development of new types of at- tacks (Xiao et al., 2018; Laidlaw & Feizi, 2019; Laidlaw et al., 2021; Kaufmann et al., 2019), it is difficult to antici- pate all types of attacks in advance. This raises the question: how can we defend models as new attacks emerge? For long-term robustness, models must quickly adapt to new attacks without sacrificing robustness to previous ones, a goal known as continual adaptive robustness (CAR) (Dai et al., 2024b) (\u00a72). A natural approach is to apply adversarial training on known attacks and fine-tune when new ones emerge, a process we call continual robust training (CRT). However, adversarial training provides poor generalization to unseen attacks, leading to suboptimal starting points for fine-tuning, and fine-tuning itself can degrade robustness against past attacks (Figure 1). We theoretically show that the robustness gap between at- tacks is linked to logit-space distances between perturbed and clean inputs and regularizing these distances can im- prove generalization to new attacks and reduce drops in robustness on previous attacks. Extensive experiments con- firm these findings. Our key contributions are as follows: Regularized Continual Robust Training for Adapting to New Adversaries (\u00a73) . To enhance CRT, we analyze the difference in robust losses between attacks and show it is up- per bounded by the sum of the maximal \u21132 distance between 1 arXiv:2502.04248v1 [cs.LG] 6 Feb 2025"
}