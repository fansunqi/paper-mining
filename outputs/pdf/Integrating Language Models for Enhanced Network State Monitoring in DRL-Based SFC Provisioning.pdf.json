{
  "code_links": [
    "None"
  ],
  "tasks": [
    "SFC provisioning",
    "VNF placement",
    "Network state monitoring"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "DRL",
    "BERT",
    "DistilBERT",
    "LoRA"
  ],
  "results": [
    "BERT outperforms DistilBERT with lower test loss (0.28 vs. 0.36) and higher confidence (0.83 vs. 0.74)",
    "BERT requires approximately 46% more processing time compared to DistilBERT"
  ],
  "title": "Integrating Language Models for Enhanced Network State Monitoring in DRL-Based SFC Provisioning.pdf",
  "abstract": "\u2014Efficient Service Function Chain (SFC) provisioning and Virtual Network Function (VNF) placement are critical for enhancing network performance in modern architectures such as Software-Defined Networking (SDN) and Network Function Virtualization (NFV). While Deep Reinforcement Learning (DRL) aids decision-making in dynamic network environments, its re- liance on structured inputs and predefined rules limits adapt- ability in unforeseen scenarios. Additionally, incorrect actions by a DRL agent may require numerous training iterations to correct, potentially reinforcing suboptimal policies and degrading performance. This paper integrates DRL with Language Models (LMs), specifically Bidirectional Encoder Representations from Transformers (BERT) and DistilBERT, to enhance network man- agement. By feeding final VNF allocations from DRL into the LM, the system can process and respond to queries related to SFCs, DCs, and VNFs, enabling real-time insights into resource utilization, bottleneck detection, and future demand planning. The LMs are fine-tuned to our domain-specific dataset using Low- Rank Adaptation (LoRA). Results show that BERT outperforms DistilBERT with a lower test loss (0.28 compared to 0.36) and higher confidence (0.83 compared to 0.74), though BERT requires approximately 46% more processing time. Index Terms\u2014SFC provisioning, VNF, DRL, Language Model, BERT, Network State Monitoring, Confidence Score. I. INTRODUCTION Service Function Chain (SFC) provisioning and Virtual Network Function (VNF) placement are critical for optimizing network performance in modern architectures like Software- Defined Networking (SDN) and Network Function Virtualiza- tion (NFV). NFV improves agility while decreasing operational and capital expenditures by using virtualization to separate software from physical infrastructure and place network ser- vices on general purpose hardware, such as data centers (DCs) [1]. SFC enhances NFV\u2019s advantages by sequencing VNFs to enable services such as Cloud Gaming (CG), Augmented Reality (AR), VoIP (Voice over IP), Video Streaming (VS), Massive IoT (MIoT), and Industry 4.0 (Ind 4.0). Despite its advantages, SFC provisioning has significant challenges, in- cluding resource allocation, sequential VNF execution, extreme traffic management, and fulfilling end-to-end (E2E) latency limitations. These issues become more complex for dynamic applications with strict QoS requirements, such as CG, AR, and MIoT [2]. Researchers utilize Deep Reinforcement Learning (DRL) algorithms for optimal VNFs\u2019 placements and SFC provision- ing, as they perform well in decision-making and adapting to varying service demands [3] [4]. However, DRL models often rely on structured inputs and predefined policies, which can limit their adaptability in the presence of unexpected network conditions [5]. For instance, when a new SFC request arrives at a DC with insufficient computational resources, the DRL model applies its pre-trained policy based on historical data and structured inputs such as computational and storage capacity usage and SFC requests. A DRL model can struggle with this situation or any other unexpected situations such as sudden traffic surges or partial DC failures resulting in delays or suboptimal decisions as it requires multiple iterations to adjust its policy. Moreover, if a DRL agent takes the wrong action, it may require multiple training iterations to fix it, and if it keeps repeating the wrong action, it may reinforce the flawed policy rather than explore better alternatives, especially if the exploration-exploitation balance is not properly tuned. A weak reward function design may also fail to penalize incorrect deci- sions, allowing the model to continue misallocating resources without correction. In contrast, Language Models (LMs) can complement DRL by analyzing both structured data (e.g., resource usage) and unstructured data (e.g., logs and network configuration intent) in real-time [6]. LMs can immediately detect bottlenecks, ex- plain the issue (e.g., \u201dThe DC cannot handle the request due to insufficient computational power\u201d), and suggest solutions, such as reallocating idle VNFs or rerouting the request to a less con- gested DC. Since LMs can interpret data without retraining for every new scenario, they provide rapid, informed recommen- dations and offer interactive Q/A capabilities for network state monitoring. This makes LMs particularly effective in handling dynamic and unpredictable network environments, enhancing DRL\u2019s long-term optimization with real-time adaptability and responsiveness. This integration enhances E2E latency and service quality, allowing network operators to successfully meet the needs of dynamic applications, particularly those with variable workloads, strict latency requirements, and real-time adaptability needs, such as CG, AR, VS, and MIoT. In our previous work [2], our focus was on VNF placement using DRL, where each VNF requires a certain amount of storage and computational power. Based on SFC requests and VNF resource requirements, the goal was to allocate resources optimally to maximize the number of SFC requests handled efficiently. In this paper, we extend that approach by integrating the network state conditions obtained after DRL actions into arXiv:2502.11298v1 [cs.NI] 16 Feb 2025"
}