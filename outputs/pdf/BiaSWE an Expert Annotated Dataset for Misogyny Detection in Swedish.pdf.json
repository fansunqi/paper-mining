{
  "code_links": [
    "https://huggingface.co/datasets/AI-Sweden-Models/BiaSWE"
  ],
  "tasks": [
    "Misogyny Detection"
  ],
  "datasets": [
    "BiaSWE"
  ],
  "methods": [
    "Expert Annotated"
  ],
  "results": [
    "None"
  ],
  "title": "BiaSWE an Expert Annotated Dataset for Misogyny Detection in Swedish.pdf",
  "abstract": "In this study, we introduce the process for creating BiaSWE, an expert-annotated dataset tailored for misogyny detection in the Swedish language. To address the cul- tural and linguistic speci\ufb01city of misog- yny in Swedish, we collaborated with ex- perts from the social sciences and humani- ties. Our interdisciplinary team developed a rigorous annotation process, incorporat- ing both domain knowledge and language expertise, to capture the nuances of misog- yny in a Swedish context. This method- ology ensures that the dataset is not only culturally relevant but also aligned with broader efforts in bias detection for low- resource languages. The dataset, along with the annotation guidelines, is publicly available for further research. 1 Introduction Large Language Models (LLMs) have experi- enced immense growth over the past years due to being capable of solving diverse tasks that previ- ously required a separate model for each speci\ufb01c task (De Angelis et al., 2023). Despite their ap- parent bene\ufb01ts, it is known that the characteristics of the dataset used to train a language model play a fundamental role in determining the model\u2019s be- havior (Gebru et al., 2021). LLMs are typically trained on large amounts of data from the Internet and thus inevitably re\ufb02ect the opinions and biases of its users. For example, a 2018 survey showed that about 85% of English Wikipedia contribu- tors identi\ufb01ed as male (Oldach, 2022). As LLMs\u2019 behavior \u201cre\ufb02ects the Collective Intelligence of Western society\u201d, LLMs can perpetuate and even amplify biases and stereotypes of social minori- ties (Kotek et al., 2023). The widespread presence of misogyny online is illustrated by a study from 2020 where 65% of women reported knowing an- other woman that had been the target of online vi- olence (The Economist Intelligence Unit, 2020). The way to avoid harmful machine learning models is to ensure that the datasets used for training are responsibly curated, involving diverse stakeholders (Delgado et al., 2021). However, dataset creation alone is not suf\ufb01cient, and addi- tional approaches, such as alignment, play a role in guiding model outputs towards human values. In the context of bias detection, misogyny varies by language and culture (Zeinert et al., 2021). Therefore, we consider creating expert-annotated, language-speci\ufb01c datasets crucial for detecting bi- ases, helping to identify areas where models may risk perpetuating harmful stereotypes or undesir- able attitudes. To address these challenges, we make two key contributions1 : 1. We present BiaSWE, a small annotated dataset for misogyny detection in Swedish, annotated for hate speech, misogyny, misogyny type categories and severity. 2. We share the creation process of the BiaSWE 1Link to the dataset and annotation guidelines: https://huggingface.co/datasets/AI-Sweden-Models/BiaSWE"
}