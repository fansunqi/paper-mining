{
  "code_links": [
    "https://anonymous.4open.science/r/NN-CIFT-B065/"
  ],
  "tasks": [
    "Instruction Fine-Tuning",
    "Subset Selection"
  ],
  "datasets": [
    "MixInstruct",
    "Alpaca"
  ],
  "methods": [
    "InfluenceNetwork",
    "NN-CIFT"
  ],
  "results": [
    "Cost reduction of up to 99%",
    "Comparable performance to original influence functions",
    "Average MSE of 0.067"
  ],
  "title": "Data Valuation Using Neural Networks for Efficient Instruction Fine-Tuning.pdf",
  "abstract": "Influence functions provide crucial insights into model training, but existing methods suffer from large computational costs and limited gen- eralization. Particularly, recent works have pro- posed various metrics and algorithms to calcu- late the influence of data using language mod- els, which do not scale well with large models and datasets. This is because of the expensive forward and backward passes required for com- putation, substantial memory requirements to store large models, and poor generalization of influence estimates to new data. In this paper, we explore the use of small neural networks \u2013 which we refer to as the InfluenceNetwork \u2013 to estimate influence values, achieving up to 99% cost reduction. Our evaluation demon- strates that influence values can be estimated with models just 0.0027% the size of full lan- guage models (we use 7B and 8B versions). We apply our algorithm of estimating influence values (called NN-CIFT: Neural Networks for effiCient Instruction Fine-Tuning) to the downstream task of subset selection for gen- eral instruction fine-tuning. In our study, we include four state-of-the-art influence functions and show no compromise in performance, de- spite large speedups, between NN-CIFT and the original influence functions. We provide an in-depth hyperparameter analyses of NN- CIFT. The code for our method can be found here: https://anonymous.4open.science/r/NN- CIFT-B065/. 1 Introduction The strong instruction-following abilities of large language models (LLMs) can be attributed to in- struction fine-tuning (IFT) (Zhang et al., 2024). IFT builds on top of current language modeling capabilities and strengthens the instruction follow- ing abilities of models. Recent works have taken data efficient approaches for IFT. The goal is to Method Cost Size Pairwise DELIFT (Agarwal et al., 2025) O(MN) \u00b7 F 7-8B DELIFT (SE) (Agarwal et al., 2025) O(MN) \u00b7 F 355M LESS (Xia et al., 2024) O(M + N) \u00b7 B 7-8B NN-CIFT (ours) O(MN) \u00b7 F 205K Pointwise SelectIT (Liu et al., 2024a) O(M) \u00b7 F 7-8B NN-CIFT (ours) O(M) \u00b7 F 205K Table 1: Approximate computational complexity of data valuation in previous works measured by the cost of forward passes (F) or the cost of backward passes (B) through a model. M and N are the cardinality of DF and DT , a fine-tuning and target dataset respectively, we use for subset selection. See Appendix B.1 for more details. Size denotes the number of parameters of the corresponding model. Note: larger models have a higher cost for forward and back passes. select a small subset of samples on which to fine- tune a model (Agarwal et al., 2025; Mirzasoleiman et al., 2020; Das and Khetan, 2024; Xia et al., 2024; Renduchintala et al., 2024; Liu et al., 2024c) that emulates the full dataset. Data efficient pipelines typically consist of two stages: (1) data valuation: designing functions to estimate the influence of data points, and (2) data selection: using influence estimates to choose a bal- anced set of influential data. Usually, data selection is cheaper than valuation \u2013 for instance, DELIFT (SE)1 (Agarwal et al., 2025) computes the similar- ity of sentence embeddings between pairs of data (expensive) for valuation and selects representative data using a submodular function (cheap). Formally, influence functions estimate the value of data. For instance, brute force influence func- tions use leave-one-out (LOO) training to measure impact by omitting each data point and evaluat- ing performance (Scanlon, 1982). More recent influence functions use LLMs to estimate influence. 1Short for \"Sentence Embedding\". 1 arXiv:2502.09969v1 [cs.LG] 14 Feb 2025"
}