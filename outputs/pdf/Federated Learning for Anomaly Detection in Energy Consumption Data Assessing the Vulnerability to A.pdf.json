{
  "code_links": "None",
  "tasks": [
    "Anomaly Detection",
    "Federated Learning",
    "Energy Consumption"
  ],
  "datasets": [
    "London Hydro Residential Energy Consumption"
  ],
  "methods": [
    "LSTM",
    "Transformer",
    "FGSM",
    "PGD"
  ],
  "results": [
    "FL more sensitive to PGD attacks",
    "Transformer less sensitive than LSTM",
    "Performance degradation under attacks"
  ],
  "title": "Federated Learning for Anomaly Detection in Energy Consumption Data Assessing the Vulnerability to A.pdf",
  "abstract": "\u2014Anomaly detection is crucial in the energy sector to identify irregular patterns indicating equipment failures, energy theft, or other issues. Machine learning techniques for anomaly detection have achieved great success, but are typically centralized, involving sharing local data with a central server which raises privacy and security concerns. Federated Learning (FL) has been gaining popularity as it enables distributed learning without sharing local data. However, FL depends on neural networks, which are vulnerable to adversarial attacks that manipulate data, leading models to make erroneous predictions. While adversarial attacks have been explored in the image domain, they remain largely unexplored in time series problems, especially in the energy domain. Moreover, the effect of adversarial attacks in the FL setting is also mostly unknown. This paper assesses the vulnerability of FL-based anomaly detection in energy data to adversarial attacks. Specifically, two state-of-the-art models, Long Short Term Memory (LSTM) and Transformers, are used to detect anomalies in an FL setting, and two white-box attack methods, Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD), are employed to perturb the data. The results show that FL is more sensitive to PGD attacks than to FGSM attacks, attributed to PGD\u2019s iterative nature, resulting in an accuracy drop of over 10% even with naive, weaker attacks. Moreover, FL is more affected by these attacks than centralized learning, highlighting the need for defense mechanisms in FL. Index Terms\u2014Adversarial Attacks, Federated Learning, Time Series Classification, Energy Consumption, Anomaly Detection I. INTRODUCTION In 2022, commercial and residential buildings accounted for approximately 34% of global energy consumption while contributing to 37% of energy and process-related carbon dioxide (CO2) emissions [1]. The United Nations\u2019 Net-zero emission goal aims to reduce the adverse effects of global warming by decreasing global net CO2 emissions to 55% of 2010 levels by 2030, with the target of achieving net zero by 2050 [2]. Therefore, identifying and mitigating energy waste is essential to align with the net-zero emissions goal. Anomaly detection, the process of identifying patterns that diverge from the established normal behavior [3], plays a crucial role in determining energy waste. Example causes of anomalies in energy consumption data include wasteful human usage [4], faulty control systems [5], and energy theft [6]. These anomalies can be detected by identifying deviations from the consumer\u2019s historical energy patterns, which can then trigger energy-conserving measures, promoting efficiency and reducing environmental impact. Energy distribution companies have been transitioning to smart meters, which measure and transmit energy consumption information for improved energy management and control [6]. The potential of smart meter data for anomaly detection has been widely recognized, and several techniques have been proposed, with Machine Learning (ML) approaches providing state-of-the-art solutions. Common approaches to training ML models involve transmitting data to a central server for model training on that server [7]. However, this centralized approach exposes data to privacy and security risks. Federated Learning (FL) [8] reduces these privacy and se- curity risks by decentralizing the learning with multiple nodes collaboratively training a global model without sharing their local data. Upon receiving the global model weights from the server during an FL round, clients train independently on their local data for several iterations and subsequently send the updated model weights to the server. The server aggregates clients\u2019 weights to update the global model, which is then broadcast back to the clients for the next training round. By keeping data local, FL enhances privacy and security while facilitating compliance with regulatory requirements such as the EU/UK General Data Protection Regulation [9]. Recognizing these advantages, energy consumption studies have integrated FL to train neural networks and preserve data locality [10], [11]. Thus, our study focuses on FL-based anomaly detection in smart meter energy consumption data. While FL mitigates privacy and security risks associated with data sharing, it still raises other security concerns [12]\u2013 [14] such as the vulnerability to adversarial attacks from malicious clients [14]. Several studies [15], [16] in vision tasks showed that FL is vulnerable to white-box adversarial attacks targeting client neural networks (NNs), such as the \u00a9 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. https://www.ieee.org/publications/rights/rights-policies.html arXiv:2502.05041v1 [cs.LG] 7 Feb 2025"
}