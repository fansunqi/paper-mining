{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Autonomous driving decision-making"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Teacher LLM-Guided Deep Reinforcement Learning",
    "Policy mixing",
    "Self-attention mechanism"
  ],
  "results": [
    "TeLL-Drive outperforms existing baseline methods",
    "Significant performance gap between TeLL-Drive and traditional DRL algorithms",
    "Faster convergence and higher final rewards relative to V-PPO and A-PPO"
  ],
  "title": "TeLL-Drive Enhancing Autonomous Driving with Teacher LLM-Guided Deep Reinforcement Learning.pdf",
  "abstract": "\u2014Although Deep Reinforcement Learning (DRL) and Large Language Models (LLMs) each show promise in address- ing decision-making challenges in autonomous driving, DRL often suffers from high sample complexity, while LLMs have difficulty ensuring real-time decision making. To address these limitations, we propose TeLL-Drive, a hybrid framework that integrates an Teacher LLM to guide an attention-based Student DRL policy. By incorporating risk metrics, historical scenario retrieval, and domain heuristics into context-rich prompts, the LLM produces high-level driving strategies through chain-of- thought reasoning. A self-attention mechanism then fuses these strategies with the DRL agent\u2019s exploration, accelerating policy convergence and boosting robustness across diverse driving conditions. Our experimental results, evaluated across multiple traffic scenarios, show that TeLL-Drive outperforms existing baseline methods, including other LLM-based approaches, in terms of success rates, average returns, and real-time feasibility. Ablation studies underscore the importance of each model com- ponent, especially the synergy between the attention mechanism and LLM-driven guidance. These findings suggest that TeLL- Drive significantly enhances both the adaptability and safety of autonomous driving systems, while offering a more efficient and scalable approach for policy learning. Full validation results are available on Our Website. I. INTRODUCTION A Utonomous driving technology has made significant ad- vancements over the past decade, emerging as a trans- formative force poised to revolutionize the transportation sector [1]. By promising enhanced safety, reduced traffic congestion, and increased mobility accessibility, autonomous vehicles (AVs) are set to redefine the landscape of modern transportation. Central to the operational efficacy of AVs is their ability to perform real-time, complex decision-making that rivals or surpasses human driving capabilities. Achieving such sophisticated decision-making necessitates the integration of advanced artificial intelligence methodologies capable of perceiving, interpreting, and responding to dynamic and often unpredictable driving environments [2]. Deep Reinforcement Learning (DRL) has become a key framework for decision-making in autonomous systems [3], [4]. In autonomous driving, DRL is used to develop policies for vehicle behaviors, such as intersection navigation [5]. How- ever, traditional DRL faces challenges, including high data requirements, slow convergence, and limited environmental understanding [6], hindering its scalability and efficiency in dynamic driving scenarios that demand robust adaptability. C.Xu, J.Liu, P.Hang and J.Sun are with the Department of Traffic Engineering and Key Laboratory of Road and Traffic Engineering, Min- istry of Education, Tongji University, Shanghai 201804, China.(e-mail: xck1270157991@gmail.com, liujiaqi13, hangpeng, sunjian@tongji.edu.cn) Corresponding author: Peng Hang Concurrently, LLMs, exemplified by architectures such as GPT-4, have demonstrated exceptional proficiency in under- standing and generating human-like text. Leveraging vast repositories of knowledge and advanced contextual reasoning capabilities, LLMs have been applied to decision-making processes in autonomous driving [7]\u2013[9]. However, their prac- tical deployment as standalone decision-making agents is constrained by inherent challenges [10]. Specifically, LLMs struggle to ensure real-time responsiveness and exhibit a de- gree of randomness in their decision outputs, which are critical limitations in time-sensitive and safety-critical applications inherent to autonomous driving systems. To address these challenges, we propose TeLL-Drive, a framework that synergistically combines the strengths of DRL and LLMs to enhance decision-making in autonomous vehi- cles. TeLL-Drive mitigates the inherent limitations of DRL and LLMs by leveraging the contextual understanding and reason- ing capabilities of LLMs to improve the sampling efficiency and quality of the DRL process. Specifically, we develop a risk-aware LLM agent equipped with memory, reflection, and reasoning abilities, enabling efficient and safe decision- making in complex traffic environments. Meanwhile, a DRL agent based on the Actor-Critic architecture is designed, which employs hybrid strategies to enhance sampling efficiency and quality while preserving robust exploration capabilities. As shown in Fig. 1, LLMs function as \u201dteachers\u201d, providing high-quality guidance and contextual insights that inform and streamline the learning process of DRL agents. Subsequently, DRL serves as the \u201dstudent\u201d, acting as the final decision maker to ensure real-time responsiveness and mitigate the randomness associated with LLM-driven decisions. The main contributions of this article are listed as follows: 1) TeLL-Drive is proposed, a novel framework for au- tonomous driving decision-making that combines a \u201cteacher\u201d LLM with a \u201cstudent\u201d DRL agent, which integrates the LLM\u2019s high-level reasoning and knowledge with DRL\u2019s adaptability and computational efficiency. 2) A risk-aware LLM agent is developed, which is endowed with memory, reflection, and reasoning capabilities, to provide context-sensitive guidance in dynamic traffic en- vironments and enhance driving safety and efficiency. 3) Through experiments in multiple scenarios, TeLL-Drive outperforms standard DRL algorithms in exploration ef- ficiency and achieves favorable overall results compared to alternative methods. arXiv:2502.01387v2 [cs.AI] 8 Feb 2025"
}