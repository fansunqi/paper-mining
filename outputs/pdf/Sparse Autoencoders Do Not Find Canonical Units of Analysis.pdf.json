{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Mechanistic Interpretability"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Sparse Autoencoders",
    "SAE stitching",
    "Meta-SAEs"
  ],
  "results": [
    "SAEs do not find canonical units of analysis",
    "SAEs learn both finer-grained versions of latents from smaller SAEs and entirely new latents",
    "Latents in larger SAEs are often not atomic but compositions of interpretable meta-latents"
  ],
  "title": "Sparse Autoencoders Do Not Find Canonical Units of Analysis.pdf",
  "abstract": "A common goal of mechanistic interpretability is to decompose the activations of neural networks into features: interpretable properties of the input computed by the model. Sparse autoencoders (SAEs) are a popular method for finding these features in LLMs, and it has been postulated that they can be used to find a canon- ical set of units: a unique and complete list of atomic features. We cast doubt on this belief using two novel techniques: SAE stitching to show they are in- complete, and meta-SAEs to show they are not atomic. SAE stitching involves inserting or swapping latents from a larger SAE into a smaller one. Latents from the larger SAE can be divided into two categories: novel latents, which improve performance when added to the smaller SAE, indicating they capture novel in- formation, and reconstruction latents, which can replace corresponding latents in the smaller SAE that have similar behavior. The existence of novel features in- dicates incompleteness of smaller SAEs. Using meta-SAEs - SAEs trained on the decoder matrix of another SAE - we find that latents in SAEs often decom- pose into combinations of latents from a smaller SAE, showing that larger SAE latents are not atomic. The resulting decompositions are often interpretable; e.g. a latent representing \u201cEinstein\u201d decomposes into \u201cscientist\u201d, \u201cGermany\u201d, and \u201cfa- mous person\u201d. Even if SAEs do not find canonical units of analysis, they may still be useful tools. We suggest that future research should either pursue differ- ent approaches for identifying such units, or pragmatically choose the SAE size suited to their task. We provide an interactive dashboard to explore meta-SAEs: https://metasaes.streamlit.app/ 1 INTRODUCTION Mechanistic interpretability aims to reverse-engineer neural networks into human-interpretable al- gorithms (Olah et al., 2020; Meng et al., 2022; Geva et al., 2023; Nanda et al., 2023; Elhage et al., 2021). A key challenge of mechanistic interpretability is identifying the correct units of analysis \u2014 fundamental components that can be individually understood and collectively explain the network\u2019s function. Ideally, these units would be unique, with no variations (Bricken et al., 2023); complete, encompassing all necessary features (Elhage et al., 2022); and atomic or irreducible, indivisible into smaller components (Engels et al., 2024). We refer to a set of units with all of these properties as canonical. Initially, researchers hoped that individual MLP neurons (Meng et al., 2022; Olah et al., 2020) and attention heads (Wang et al., 2022; Olsson et al., 2022) could serve as these units. However, these \u2217These authors contributed equally to this work. 1 arXiv:2502.04878v1 [cs.LG] 7 Feb 2025"
}