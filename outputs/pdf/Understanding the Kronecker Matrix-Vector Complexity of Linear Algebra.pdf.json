{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Understanding the Kronecker Matrix-Vector Complexity of Linear Algebra"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Kronecker matrix-vector product",
    "orthogonality",
    "exponential lower bounds",
    "zero testing"
  ],
  "results": [
    "Exponential lower bounds on the number of queries needed to estimate properties like trace and top eigenvalue",
    "Exponential gap between sketching with Gaussian and Rademacher vectors for zero testing"
  ],
  "title": "Understanding the Kronecker Matrix-Vector Complexity of Linear Algebra.pdf",
  "abstract": "We study the computational model where we can access a matrix A only by computing matrix-vector products Ax for vectors of the form x = x1 \u2297\u00b7 \u00b7 \u00b7\u2297xq. We prove exponential lower bounds on the number of queries needed to estimate various properties, including the trace and the top eigenvalue of A. Our proofs hold for all adaptive algorithms, modulo a mild conditioning assumption on the algorithm\u2019s queries. We further prove that algorithms whose queries come from a small alphabet (e.g., xi \u2208{\u00b11}n) cannot test if A is identically zero with polynomial complexity, despite the fact that a single query using Gaussian vectors solves the problem with probability 1. In steep contrast to the non-Kronecker case, this shows that sketching A with di\ufb00erent distributions of the same subguassian norm can yield exponentially di\ufb00erent query complexities. Our proofs follow from the observation that random vectors with Kronecker structure have exponentially smaller inner products than their non-Kronecker counterparts. 1 Introduction Tensors have emerged as a canonical way to represent multi-modal or very high-dimensional datasets in areas ranging from quantum information science [Biamonte, 2019] to medical imaging [Selvan and Dam, 2020, Sedighin, 2024]. Such applications often result in compact representations of tensors. For instance, applications in quantum information theory use the so-called PEPS network or other compact tensor networks, while applications in partial di\ufb00erential equations often use tucker or tensor train decom- positions. These applications overcome the curse of dimensionality by representing an underlying high dimensional linear operator as a network of a series of low dimensional tensors. Abstractly, in these applications we are given an order 2q tensor A \u2208(Rn)\u22972q that represents a linear operator from (Rn)\u2297q to (Rn)\u2297q, and we often want to approximately compute some properties of this linear operator, such as its trace or spectral norm. By appropriately reordering the entries of A, we can explicitly write down a matrix A \u2208Rnq\u00d7nq that describes this linear operator. Our goal then becomes to estimate the trace, spectral sum, operator norm, or some other property of A. However, since we may not know the structure of the underlying compact representation, we would like to estimate properties of A without explicitly forming A, as doing so would break our compact representation of A. Instead we take advantage of our compact representation to e\ufb03ciently and implicitly access A through linear measurements, such as the Kronecker matrix vector product: De\ufb01nition 1. Let A \u2208Rnq\u00d7nq. Then Kronecker Matrix-Vector Product Oracle is an oracle that, given x1, . . . , xq \u2208Rn, returns Ax \u2208Rnq where x = \u2297q i=1xi. Here, \u2297denotes the Kronecker product. 1"
}