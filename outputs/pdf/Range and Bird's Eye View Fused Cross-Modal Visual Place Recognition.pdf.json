{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Image-to-point cloud cross-modal Visual Place Recognition"
  ],
  "datasets": [
    "KITTI"
  ],
  "methods": [
    "Initial retrieval + re-rank",
    "Global descriptor similarity search",
    "Points average distance",
    "Generalized triplet loss"
  ],
  "results": [
    "Outperforms state-of-the-art approaches",
    "Recall@1: 99.03%"
  ],
  "title": "Range and Bird's Eye View Fused Cross-Modal Visual Place Recognition.pdf",
  "abstract": "\u2014 Image-to-point cloud cross-modal Visual Place Recognition (VPR) is a challenging task where the query is an RGB image, and the database samples are LiDAR point clouds. Compared to single-modal VPR, this approach benefits from the widespread availability of RGB cameras and the robustness of point clouds in providing accurate spatial geometry and distance information. However, current methods rely on intermediate modalities that capture either the vertical or horizontal field of view, limiting their ability to fully exploit the complementary information from both sensors. In this work, we propose an innovative initial retrieval + re-rank method that effectively combines information from range (or RGB) images and Bird\u2019s Eye View (BEV) images. Our approach relies solely on a computationally efficient global descriptor similarity search process to achieve re-ranking. Additionally, we introduce a novel similarity label supervision technique to maximize the utility of limited training data. Specifically, we employ points average distance to approximate appearance similarity and incorporate an adaptive margin, based on similarity differences, into the vanilla triplet loss. Experimental results on the KITTI dataset demonstrate that our method significantly outperforms state-of-the-art approaches. I. INTRODUCTION Image-to-point cloud cross-modal Visual Place Recogni- tion (cross-modal VPR) involves querying a LiDAR point cloud database using an RGB image captured by a camera. Cameras are cost-effective and widely deployed in vehicles, making them ideal for online queries, while LiDAR provides precise spatial geometry and distance information. Cross- modal VPR addresses challenges such as environmental variations (e.g., lighting, weather, and seasonal changes) and eliminates the need for simultaneous mapping in visual SLAM systems. However, its retrieval performance lags behind single-modal VPR, primarily due to the significant modality gap between RGB images and LiDAR point clouds. RGB images capture dense color and texture details, whereas LiDAR point clouds provide accurate spatial data. Existing methods bridge this gap using intermediate modalities or similarity labels for supervision. * corresponding author Jianyi Peng, Fan Lu, Bin Li, Sanqing Qu and Guang Chen are with Tongji University, Shanghai 201804, China. E-mail: 2233589@tongji.edu.cn, lufan@tongji.edu.cn, libin2021@tongji.edu.cn, sanqingqu@tongji.edu.cn, guangchen@tongji.edu.cn. Yuan Huang is with Automation, Beijing Institute of Control Engineering. E-mail: hy0103053428@gmail.com. This work was supported by the National Key Research and Development Program of China (No. 2024YFE0211000), in part by the National Natural Science Foundation of China (No. 62372329), in part by Shanghai Scientific Innovation Foundation (No. 23DZ1203400), in part by Tongji-Qomolo Autonomous Driving Commercial Vehicle Joint Lab Project, and in part by Xiaomi Young Talents Program. Fig. 1. Illustration of our image-to-point cloud cross-modal visual place recognition. It\u2019s mainly composed of two separate similarity search process by only using global descriptors, in this way, we can effectively combine the information from range (or RGB) images and Bird\u2019s Eye View (BEV) images, significantly reducing the modality gap. Previous works have explored intermediate modalities to reduce the modality gap. For example, i3dLoc [1] predicts pseudo range images from RGB images, while (LC)2 [2] generates depth maps from RGB images, which I2P-Rec [3] converts into pseudo point clouds for BEV images. ModaLink [4] densifies depth projections from LiDAR point clouds to replace range images. Recent studies, such as LIP-Loc [5] and CMVM [6], align global descriptors of RGB and range images to reduce retrieval difficulty. Despite these efforts, the modality gap persists. To address this, we propose a strategy that combines the strengths of range (or RGB) images and BEV images, minimizing the gap\u2019s adverse effects. Supervision with precise labels is another mainstream approach. (LC)2 [2] uses a sector area-based overlap ratio as a similarity label, combined with generalized contrastive loss. CMVM [6] adopts a pixel-based overlap ratio from OverlapNet [7] and predefined thresholds for binary labels. However, these methods suffer from inaccurate similarity approximations and lack continuous similarity label loss functions suitable for small datasets. To overcome these limitations, we propose a novel similarity label supervision method that provides more precise labels and ensures effec- tive training with limited data. In this work, we propose a fused pipeline leveraging range images and BEV images to enhance cross-modal VPR performance. As shown in Fig. 1, our method operates in two phases. First, we retrieve top-k candidates by computing global descriptor similarity between the query RGB image and range images of database submaps. Then, for the top- k candidates, we compute similarity between the camera BEV image and LiDAR BEV images. Finally, we per- arXiv:2502.11742v1 [cs.CV] 17 Feb 2025"
}