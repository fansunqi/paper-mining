{
  "code_links": [
    "https://chart-reading.github.io"
  ],
  "tasks": [
    "Task-driven Eye Movement Control for Chart Reading"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "LLMs",
    "Reinforcement Learning",
    "Hierarchical Control Architecture"
  ],
  "results": [
    "DTW: 26,018",
    "LEV: 154.7",
    "Sequence Score: 0.413"
  ],
  "title": "Chartist Task-driven Eye Movement Control for Chart Reading.pdf",
  "abstract": "To design data visualizations that are easy to comprehend, we need to understand how people with different interests read them. Com- putational models of predicting scanpaths on charts could comple- ment empirical studies by offering estimates of user performance inexpensively; however, previous models have been limited to gaze patterns and overlooked the effects of tasks. Here, we contribute Chartist, a computational model that simulates how users move their eyes to extract information from the chart in order to per- form analysis tasks, including value retrieval, filtering, and finding extremes. The novel contribution lies in a two-level hierarchical control architecture. At the high level, the model uses LLMs to comprehend the information gained so far and applies this repre- sentation to select a goal for the lower-level controllers, which, in turn, move the eyes in accordance with a sampling policy learned via reinforcement learning. The model is capable of predicting human-like task-driven scanpaths across various tasks. It can be applied in fields such as explainable AI, visualization design eval- uation, and optimization. While it displays limitations in terms of generalizability and accuracy, it takes modeling in a promising direction, toward understanding human behaviors in interacting with charts. CCS CONCEPTS \u2022 Human-centered computing \u2192HCI theory, concepts and models; Information visualization. KEYWORDS User model; Simulation; Scanpath; Reinforcement learning; LLMs ACM Reference Format: Danqing Shi, Yao Wang, Yunpeng Bai, Andreas Bulling, and Antti Oulasvirta. 2025. Chartist: Task-driven Eye Movement Control for Chart Reading. In CHI Conference on Human Factors in Computing Systems (CHI \u201925), April 26-May 1, 2025, Yokohama, Japan. ACM, New York, NY, USA, 14 pages. https://doi.org/10.1145/3706598.3713128 1 INTRODUCTION Visual attention plays a pivotal role in the field of information visualization [11, 31]. By understanding the visual attention on This work is licensed under a Creative Commons Attribution 4.0 International License. CHI \u201925, April 26-May 1, 2025, Yokohama, Japan \u00a9 2025 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-1394-1/25/04. https://doi.org/10.1145/3706598.3713128 charts, designers can iteratively refine visualizations using visual saliency as feedback [67]; engineers can enhance their AI models by incorporating human-like attention [69]; and researchers can better understand the link between comprehension and gaze behavior when people read charts [77]. Eye tracking has long been used to understand human visual attention on charts [27]. Beyond visual saliency [66], analyzing human scanpaths provides details of the sequence of fixations, helping researchers understand strategies for reasoning [76]. Previous literature has demonstrated that users observe completely different visual elements when performing dif- ferent analytical tasks [56]. However, collecting human scanpaths by using eye trackers is expensive both time-wise and monetarily. Simulations are effective in developing theories by rigorously test- ing user interactions with visual elements in controlled settings. By simulating eye movements, researchers can uncover the mecha- nisms behind behaviors of users interpreting data visualizations. This enhances understanding of chart reading [51]. Human visual attention is guided by two processes: bottom-up and top-down processes [36]. These processes apply to chart read- ing as well [83]. Bottom-up attention is driven by salient visual stimuli (e.g., high-contrast colors), whereas top-down attention is task-driven, with specific goals or intentions shaping where and how users focus their attention. However, most visual attention models applied for information visualizations capture only bottom- up (free-viewing) attention [49, 66, 76], thus leaving a gap in under- standing how tasks influence human visual attention [4]. Compared to exploratory free viewing, scanpaths for the same analysis task are more coherent; also, they vary greatly between tasks [56]. While recent research has been able to predict task-driven saliency on charts [80], it has remained one step away from addressing how people read charts. Temporal information and individual-specific behaviors are still missing from task-driven-saliency maps. In other words, what would the scanpath look like when a person carries out a particular task on a given chart? In this paper, we present Chartist, the first computational model for predicting task-driven scanpaths on charts 1. When given both an image of a chart and a sentence as the analytical task, Chartist can simulate a sequence of human-like fixation positions related to the task (see Figure 1). The model has two key distinctions from preexisting models for scanpath prediction: 1) Our model focuses on predicting fixations made during analytical tasks, including both fixation positions and their order, in contrast against the current state-of-the-art mod- els, which concentrate on free-viewing conditions. Task factors\u2019 influence makes it challenging to predict task-driven scanpaths via prior models. 2) Unlike goal-driven scanpath predictions, which are 1https://chart-reading.github.io arXiv:2502.03575v1 [cs.HC] 5 Feb 2025"
}