{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Temporal Sentence Grounding"
  ],
  "datasets": [
    "Charades-STA",
    "ActivityNet Captions",
    "Charades-CD"
  ],
  "methods": [
    "Contrast-Unity Framework",
    "Quadruple Contrastive Learning",
    "Multi-Instance Learning"
  ],
  "results": [
    "State-of-the-art performance on all IoU regimes",
    "7.74% mIoU gain over the previous SOTA on Charades-STA"
  ],
  "title": "Contrast-Unity for Partially-Supervised Temporal Sentence Grounding.pdf",
  "abstract": "\u2014Temporal sentence grounding aims to detect event timestamps described by the natural language query from given untrimmed videos. The existing fully-supervised setting achieves great results but requires expensive annotation costs; while the weakly-supervised setting adopts cheap labels but performs poorly. To pursue high performance with less annotation costs, this paper introduces an intermediate partially-supervised set- ting, i.e., only short-clip is available during training. To make full use of partial labels, we specially design one contrast- unity framework, with the two-stage goal of implicit-explicit progressive grounding. In the implicit stage, we align event-query representations at fine granularity using comprehensive quadru- ple contrastive learning: event-query gather, event-background separation, intra-cluster compactness and inter-cluster separabil- ity. Then, high-quality representations bring acceptable ground- ing pseudo-labels. In the explicit stage, to explicitly optimize grounding objectives, we train one fully-supervised model using obtained pseudo-labels for grounding refinement and denoising. Extensive experiments and thoroughly ablations on Charades- STA and ActivityNet Captions demonstrate the significance of partial supervision, as well as our superior performance. Index Terms\u2014Video Grounding, Partial Supervision. I. INTRODUCTION Temporal sentence grounding (TSG) plays an important role for video-language understanding, with the goal to detect the start and end timestamps of the event described by a given natural language query from untrimmed videos. TSG covers extensive application scenarios [1]\u2013[3], as it could learn high- quality cross-modal representations from large-scale data. TSG has developed two popular settings for data annotation: fully-supervised setting (FTSG) [4], [5], where each (video, query) pair is annotated with precise temporal boundaries, and weakly-supervised setting (WTSG) [6], [7], where only the corresponding (video, query) is provided without temporal annotations. While the fully-supervised approach is accurate, it is time-consuming and prone to subjective interpretation, especially for events with complex semantics. The weakly- supervised approach reduces annotation effort but results in lower performance, limiting its practical applications. Hence, one question naturally arises: Is there an intermedi- ate setting between full and weak supervisions in TSG, which can obtain relatively high performance but requires less anno- tation cost? This paper answers the question by introducing the partially-supervised setting (PTSG). Specifically, for each *: Equal contribution. Q: Corresponding author text query, a partial temporal region corresponding to a short video-clip is annotated within the whole event interval. And in the strictest case, partial labels could degenerate to single- frame labels, i.e., labeling one timestamp for each event. At a slight more cost than WTSG in annotation time, such partial supervision greatly improves grounding performance, which is very effective comparing to full or weak supervisions. Hereafter, our goal is to ground complete event intervals through limited yet precise partial labels. With the same data formulation (video, query, timestamps), partial supervision can approach full supervision continuously, by annotating a proper event duration. Thus, an intuitive thought is that, PTSG and FTSG can share the same training architecture. Following this idea, one trivial solution is to simply train FTSG model using partial annotations. As tested preliminarily, FTSG model performs well using high-quality partial annotation (80% event coverage), proving its robustness for small turbulence. How- ever, the limited short-clip partial label is too noisy for FTSG model to learn semantic patterns, resulting in an unsatisfying result. Therefore, We design a contrast-unity framework for implicit-explicit (two-stage) progressive grounding. Given the training set with incomplete partial labels, the implicit stage aims to refine the partial annotation at fine gran- ularity. To get better labels, we propose one novel quadruple contrast pipeline, leveraging inter and intra-sample contrast for uni and cross-modal alignment. The first two contrasts are built on intra-samples to promote event-query gather for cross- modal correspondence and raise event-background separation for visual uni-modality. Then, to build more semantic contrasts from the whole dataset, another two contrasts are proposed for inter-samples to further enable intra-cluster compactness and inter-cluster separability. To obtain refined event intervals, we introduce an event detector which takes partial labels as seed anchors and extends them for an event mask. Then, features for event and background can be calculated via the event mask. Thanks to the essence of multi-instance learning [8], with well-alignment representations, the event detector can output refined grounding pseudo-labels. Next, we bridge another explicit stage after the implicit stage, by treating grounding pseudo-labels as ground-truth to train one fully-supervised model, then inference through this fully-supervised model. This framework could do more at one stroke. Structurally, it bridges the setting gap between PTSG and FTSG, enabling PTSG to enjoy advanced bonuses from FTSG, e.g., superior arXiv:2502.12917v1 [cs.CV] 18 Feb 2025"
}