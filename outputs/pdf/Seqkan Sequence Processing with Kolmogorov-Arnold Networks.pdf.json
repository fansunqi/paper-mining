{
  "code_links": [
    "https://github.com/data-eng/seqKAN",
    "https://github.com/remigenet/TKAN"
  ],
  "tasks": [
    "Sequence Processing"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "seqKAN",
    "TKAN",
    "RNN",
    "LSTM",
    "Symbolic Regression"
  ],
  "results": [
    "seqKAN achieves superior performance on both interpolation and extrapolation tasks",
    "seqKAN outperforms alternative architectures, including RNN, LSTM, and symbolic regression"
  ],
  "title": "Seqkan Sequence Processing with Kolmogorov-Arnold Networks.pdf",
  "abstract": ". Kolmogorov-Arnold Networks (KANs) have been recently pro- posed as a machine learning framework that is more interpretable and controllable than the multi-layer perceptron. Various network architectures have been proposed within the KAN framework tar- geting different tasks and application domains, including sequence processing. This paper proposes seqKAN, a new KAN architecture for sequence processing. Although multiple sequence processing KAN architectures have already been proposed, we argue that seqKAN is more faithful to the core concept of the KAN framework. Further- more, we empirically demonstrate that it achieves better results. The empirical evaluation is performed on generated data from a complex physics problem on an interpolation and an extrapolation task. Using this dataset we compared seqKAN against a prior KAN network for timeseries prediction, recurrent deep networks, and symbolic regres- sion. seqKAN substantially outperforms all architectures, particularly on the extrapolation dataset, while also being the most transparent. Keywords: seqKAN, Machine Learning, Kolmogorov-Arnold Net- works, KAN, Timeseries Processing 1 Introduction Kolmogorov-Arnold Networks (KANs) were recently proposed by Liu et al. [5] as an alternative machine learning framework to the ubiquitous multi-layer perceptron (MLP). KANs introduce the idea that if edge weights are lifted to learnable functions, this suffices to capture non-linearities in the data so that nodes can simply sum incoming edges. This idea is inspired by the Kolmogorov-Arnold Representation Theorem (KAT) that proves that any multi-variate function can be re-formulated using two layers of uni-variate functions and simple (unweighted) summation. KAT has been the object of an extensive discussion regarding its rel- evance to machine learning, summarized by Schmidt-Hieber [6]. One notable point in this discussion is that KAT guarantees the existence of uni-variate functions that can be combined into an exact represen- tation any multi-variate function, but offers neither a construction nor any guarantee on the properties of these uni-variate functions other than being continuous. In fact, with the exception of trivial cases, these functions are suspected to be highly non-smooth and practically impossible to construct either analytically or empirically. The KAN architecture circumvents objections to KAT\u2019s relevance to machine learning by framing KANs as approximators (as opposed to KAT\u2019s exact representations) that stack two or more layers. This re-contextualization into the modern deep learning environment has mustered impressive interest from the machine learning community with more than 10 extensions and applications published within the few months since the original KAN article [1, Section 1]. In this paper we present seqKAN, a new architecture within the KAN framework for processing sequences. Our main contribution is that this architecture is more faithful to the core concept of the KAN framework as it avoids re-introducing weighted summation and fixed activation function in the form of conventional MLP-styled recurrency cells. A secondary contribution is the definition of a new evaluation task that has several characteristics (lacking from existing tasks) geared towards combining quantitative evaluation with a quali- titative analysis of the results. In the remainder of this article we will first provide the relevant background on KANs and their recurrent extensions (Section 2) and then present seqKAN (Section 3). We will then present and discuss the evaluation task and our experimental results (Section 4) and finally close with conclusions and directions for future work (Section 5). 2 Background 2.1 Kolmogorov-Arnold networks As mentioned above, the basic idea underlying KANs is that the acti- vation functions that transform values as they move forward through layers are learned from the data and not pre-determined; and that each node in a layer performs simple addition (without weights) over the values it receives from each node of the preceding layer. Figure 1 gives a characteristic example of how values from a 3-node layer propagate to a 2-node layer. Each activation function \u03d5i(\u00b7) is the weighted sum of a learned spline and silu(x) = x/(1 + exp(\u2212x)). That is: \u03d5i(x) = w1 \u00b7 silu(x) + w2 \u00b7 splinei(x) where w1, w2 and splinei need to be trained for each edge of the network. The original publication explains w1, w2 as implementa- tion details that make the network well-optimizable [5, p. 6]. Besides completely learned splines, activation functions can also be param- eterized prior functions. In this case, the user provides univariate function implementations which the network parameterizes via affine transformations (translation and scaling) to fit the data. By comparison to a conventional neural network, KANs have sig- nificantly more parameters to train. Assuming the same number of nodes and edges, a NN learns a single weight for each incoming edge of each node. A KAN, by comparison, learns w1, w2 and the spline arXiv:2502.14681v1 [cs.LG] 20 Feb 2025"
}