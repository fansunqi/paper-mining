{
  "code_links": "None",
  "tasks": [
    "Opinion formation and resistance",
    "Misinformation spread and debunking",
    "Opinion dynamics in social networks"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Bounded Confidence Model (BCM)",
    "Non-cooperative game framework",
    "LLM agents",
    "Resource optimization"
  ],
  "results": [
    "Higher confirmation bias strengthens opinion alignment but exacerbates overall polarization",
    "High-resource debunking strategy risks rapid resource depletion and diminished long-term influence",
    "Adversarial LLMs can effectively model misinformation spread and counter"
  ],
  "title": "Competing LLM Agents in a Non-Cooperative Game of Opinion Polarisation.pdf",
  "abstract": "We introduce a novel non-cooperative game to analyse opinion formation and resistance, incorporating principles from social psychol- ogy such as confirmation bias, resource con- straints, and influence penalties. Our simula- tion features Large Language Model (LLM) agents competing to influence a population, with penalties imposed for generating mes- sages that propagate or counter misinformation. This framework integrates resource optimisa- tion into the agents\u2019 decision-making process. Our findings demonstrate that while higher con- firmation bias strengthens opinion alignment within groups, it also exacerbates overall po- larisation. Conversely, lower confirmation bias leads to fragmented opinions and limited shifts in individual beliefs. Investing heavily in a high-resource debunking strategy can initially align the population with the debunking agent, but risks rapid resource depletion and dimin- ished long-term influence. 1 Introduction and Background The study of opinion dynamics, originating from efforts to understand how individuals modify their views under social influence (Kelman, 1958, 1961), has broad applications in areas such as public health campaigns, conflict resolution, and com- bating misinformation. Within social networks, opinions spread and evolve, influenced by various factors including peer interactions (Kandel, 1986), media exposure (Zucker, 1978), and group dynam- ics (Friedkin and Johnsen, 2011). Developing accu- rate models of these processes is essential not only for predicting trends like opinion polarisation (Tan et al., 2024) or consensus formation but also for crafting targeted interventions to mitigate harmful effects, such as the spread of misinformation or societal fragmentation (Hegselmann and Krause, 2015). Agent-based models (ABMs), simulating interactions among individual agents as proxies for humans, serve as valuable tools for examining the emergent properties of opinion dynamics. These models offer robust frameworks for analysing com- plex scenarios (Deffuant et al., 2002; Mathias et al., 2016), evaluating strategies to reduce negative con- sequences, and potentially fostering constructive social influence by integrating explicit cognitive mechanisms into opinion-updating processes. This work investigates how Large Language Models (LLMs) can model human-like opinion dy- namics and influence propagation within social net- works. Traditional ABMs often employ simplified rules that fail to capture the complexity of human communicative strategies. To address this limita- tion, we introduce a novel non-cooperative game framework where adversarial LLMs, one spreading misinformation and the other countering it, inter- act. This work introduces a non-cooperative game where LLM agents engage in adversarial interac- tions to model misinformation spread and coun- tering. Unlike prior studies (Wang et al., 2025; Chuang et al., 2024) on passive opinion evolution and nudging, it focuses on resource-constrained influence operations and debunking effectiveness in competitive environments. We pose the following research questions: RQ1 What are the emergent behaviors in networks of agents influenced by competing LLMs? RQ2 How does the competition between LLM agents shape the evolution of opinion clusters over time, also known as echo-chambers? 2 Methodology We use LLMs to simulate the propagation and de- bunking of misinformation on social media within a non-cooperative game framework. Scenario: Our scenario is strategically designed to reflect the asymmetric nature of contested in- formation environments, specifically highlighting the challenges faced by the \"Blue team\" (i.e., those 1 arXiv:2502.11649v1 [cs.AI] 17 Feb 2025"
}