{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Mathematical Reasoning",
    "Long Chain-of-Thoughts (CoT) Generation"
  ],
  "datasets": [
    "GSM8K",
    "Weak12K",
    "Math-500",
    "AquA",
    "CMath",
    "GaoKao-Math-QA",
    "CN-Middle-School",
    "GaoKao2023"
  ],
  "methods": [
    "Constrained Monte Carlo Tree Search (C-MCTS)",
    "Action Space Constraints",
    "Search Strategy Guidance",
    "Process Reward Models (PRM)"
  ],
  "results": [
    "7B model achieves reasoning capabilities surpassing those of 72B model",
    "Significant performance improvements on challenging datasets like Gaokao2023",
    "Average accuracy improvement of 3.9% compared to voting baseline",
    "Consistent performance improvement across various datasets",
    "Superior efficiency of PRM model compared to rule-based processes"
  ],
  "title": "Leveraging Constrained Monte Carlo Tree Search to Generate Reliable Long Chain-of-Thought for Mathem.pdf",
  "abstract": "Recently, Long Chain-of-Thoughts (CoTs) have gained widespread attention for improving the reasoning capabilities of Large Language Models (LLMs). This necessitates that existing LLMs, which lack the ability to generate Long CoTs, to acquire such capability through post- training methods. Without additional training, LLMs typically enhance their mathematical rea- soning abilities through inference scaling meth- ods such as MCTS. However, they are hindered by the large action space and inefficient search strategies, making it challenging to generate Long CoTs effectively. To tackle this issue, we propose constraining the action space and guiding the emergence of Long CoTs through a refined search strategy. In our proposed Con- strained Monte Carlo Tree Search (C-MCTS) framework, we limit the actions selected from a constrained action space, which is divided into five disjoint subsets: understanding, planning, reflection, coding, and summary. Each subset is further constrained to a small number of pre- defined prompts, rather than allowing LLMs to generate actions arbitrarily. Additionally, we refine the search strategy by incorporating prior knowledge about the action sets, such as a human-like partial order of the action sub- sets and the pretrained process reward models. These strategies work together to significantly reduce the vast search space of Long CoTs. Ex- tensive evaluations on mathematical reasoning benchmarks show that, under zero-shot settings, our method enables the 7B model to achieve reasoning capabilities that surpass those of the 72B model. 1 Introduction Improving the reasoning ability, especially the mathematical reasoning ability, occupies a central position in current large language models (LLMs) research. The Chain of Thought (CoT) technique (Wei et al., 2022) has emerged as a mainstream *Corresponding author, cairuichu@gmail.com solution to enhance LLMs\u2019 reasoning ability in a step-by-step manner. Recently, the generation of Long Chains of Thought (Long CoTs) has led to significant performance improvements. Compared to the original CoT, Long CoTs not only focus on problem decomposition but also introduce addi- tional reflection and detailed calculations into the reasoning process. However, existing works typ- ically rely on supervised fine-tuning or reinforce- ment learning-based post-training, making them highly dependent on extensive training resources. Thus, lots of existing foundational open-source lan- guage models, such as Qwen (Yang et al., 2024a,b), cannot generate Long CoTs effectively. In contrast, foundational open-source models often achieve competitive results in structured rea- soning through inference scaling. For instance, Monte Carlo Tree Search (MCTS) and its numer- ous variants (Hao et al., 2023; Feng et al., 2023; Qi et al., 2024; Sun et al., 2024) constrain the model to reason in the form of CoT through process su- pervision. React (Yao et al., 2022) and Rethink (Schwarzschild et al., 2024) constrain the model to rethink and summarize the CoT in reasoning to ar- rive at new answers. NLRL (Feng et al., 2024) con- strains the model to summarize and learn new ex- periences through continuous interaction with the environment. These existing research demonstrate that constraining MCTS through process supervi- sion has led to notable improvements. However, due to the fundamental limitation of its large action space generated by the LLMs, MCTS struggles to generate Long CoTs effectively. First, within such a vast action space, LLMs often become trapped in an endless loop of ineffective decompositions, repeatedly generating redundant or suboptimal rea- soning steps without making actual progress. Sec- ond, within such a vast action space, the process re- ward model fails to effectively constrain the model to generate human-like action sequences. While reward-based methods aim to guide reasoning, they arXiv:2502.11169v1 [cs.CL] 16 Feb 2025"
}