{
  "code_links": [
    "https://github.com/qhengncsu/AA-CNC"
  ],
  "tasks": [
    "Convex-nonconvex Regularized Problems"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Operator splitting",
    "Anderson acceleration",
    "Fixed-point iteration",
    "Convex optimization",
    "Nonconvex regularization"
  ],
  "results": [
    "FBFS is overall slower since it requires an additional forward step per iteration",
    "FBFS can out perform FBS since the singular value thresholding step in the backward iteration is the main computational bottleneck",
    "Applying AA can speed up the computation by a factor of 2 to 4 on the considered problems"
  ],
  "title": "Anderson Accelerated Operator Splitting Methods for Convex-nonconvex Regularized Problems.pdf",
  "abstract": "\u2014Convex-nonconvex (CNC) regularization is a novel paradigm that employs a nonconvex penalty function while maintaining the convexity of the entire objective function. It has been successfully applied to problems in signal processing, statistics, and machine learning. Despite its wide application, the computation of CNC regularized problems remains challenging and under-investigated. To \ufb01ll the gap, we study several operator splitting methods and their Anderson accelerated counterparts for solving least squares problems with CNC regularization. We establish the global convergence of the proposed algorithm to an optimal point and demonstrate its practical speed-ups in various applications. Index Terms\u2014Operator splitting, Anderson acceleration, \ufb01xed- point iteration, convex optimization, nonconvex regularization I. INTRODUCTION A core task in signal processing, statistics, and machine learning is to solve a regularized least squares problem: min x\u2208Rp 1 2\u2225y \u2212Ax\u22252 2 + \u03bb\u03c8(x), (1) where A is a matrix and \u03c8 is a regularizer that penalizes deviations away from a desired solution structure. The nonneg- ative tuning parameter \u03bb trades off the emphasis on model \ufb01t, quanti\ufb01ed by the quadratic data \ufb01delity term, and the model structure imposed by the regularizer \u03c8. Convex regularizers such as the \u21131-norm [1], total variation seminorm [2], and the nuclear norm [3] are the typical choice of a diverse array of classic signal processing and machine learning problems. Moreover, when \u03c8 is convex, problem (1) is convex and often admits scalable iterative algorithms with convergence guarantees to global minima. Using a convex regularizer \u03c8, however, model parameters with large magnitudes are underestimated. This underestimation is referred to as \u201cshrinkage\u201d bias in statistics. By contrast, nonconvex regularizers such as the smoothly clipped absolute deviation (SCAD) [4] and minimax concave penalty (MCP) [5] introduce less shrinkage bias while simultaneously producing sparser solutions. Employing a nonconvex regularizer, how- ever, typically renders problem (1) nonconvex. Unfortunately, iterative algorithms are typically no longer guaranteed to converge to global minima on nonconvex problems. Convex-nonconvex (CNC) regularization, as a middle ground, is a way to leverage the strengths of convex and nonconvex regularizers while simultaneously mitigating their Manuscript created December 2024. Qiang Heng is with the School of Mathematics, Southeast University. Xiaoqian Liu is with the Department of Statistics, University of California, Riverside. Eric C. Chi is with the Department of Statistics, Rice University. The \ufb01rst two authors contributed equally to this article. weaknesses [6]\u2013[12]. The idea is to design a nonconvex reg- ularizer in a way that preserves the convexity of the objective function. In this paper, we focus on a popular formulation of CNC regularization proposed in [12]. min x\u2208Rp h(x) = 1 2\u2225y \u2212Ax\u22252 2 + \u03bb\u03c8B(x), (2) where \u03c8B(x) = \u03c1(x) \u2212min v\u2208Rp \u001a \u03c1(v) + 1 2\u2225B(x \u2212v)\u22252 2 \u001b . (3) Here, \u03c1(x) is a convex function, and B \u2208Rn\u00d7p is a tuning parameter matrix speci\ufb01ed by the user. The regularizer \u03c8B(x) is typically nonconvex, but the objective function h(x) remains convex when B satis\ufb01es A\u22a4A \u2ab0\u03bbB\u22a4B [12]. For instance, B = p \u03b3/\u03bbA, (4) for some \u03b3 \u2208[0, 1]. Alternative strategies for setting B in the context of image restoration have been explored in [13]. For simplicity and consistency, throughout this paper, we assume that B is speci\ufb01ed as in (4). Various regularizers \u03c8B have been proposed in the litera- ture. Choosing \u03c1 to be the \u21131-norm leads to an extension to the MCP, the generalized minimax concave (GMC) penalty [12]. Choosing \u03c1 to be the total-variation penalty [2], a standard regularizer in signal and image denoising tasks, leads to estimators that better capture sharp transitions in signals more reliably [13], [14]. Choosing \u03c1 to be the nuclear norm, a convex relaxation of the matrix rank, leads to improved methods for matrix completion [13] and robust principal component analysis [15]. Choosing \u03c1 to be the \u21132,1-norm [16], [17], leads to less biased grouped variable selection compared to the group Lasso penalty [18]. In all cases, \u03c8B achieves the same desired regularization effect as \u03c1 without its induced bias while also maintaining convexity of the objective function h. Regarding its computation, problem (2) is typically solved via operator splitting methods. Setting B as in (4), problem (2) is equivalent to the following saddle point problem [12]. min x\u2208Rp max v\u2208Rp H(x, v) (5) where H(x, v) = 1 2\u2225y\u2212Ax\u22252 2+\u03bb\u03c1(x)\u2212\u03bb\u03c1(v)\u2212\u03b3 2 \u2225A(x\u2212v)\u22252 2. (6) We will see shortly that problem (5) in turn is equivalent to an inclusion problem of the sum of two monotone operators. Therefore, operator splitting methods are natural strategies for solving (5). Interestingly, despite the availability of multiple"
}