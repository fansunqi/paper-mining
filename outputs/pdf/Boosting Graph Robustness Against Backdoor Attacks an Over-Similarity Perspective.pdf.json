{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Graph backdoor attacks",
    "Graph neural networks (GNNs) defense"
  ],
  "datasets": [
    "Cora",
    "Citeseer",
    "PubMed",
    "Physics",
    "Flickr",
    "OGB-arxiv"
  ],
  "methods": [
    "DBSCAN",
    "Contrastive learning",
    "Autoencoder",
    "Global anomaly score"
  ],
  "results": [
    "Defense Recovery Rate (DRR) near 100%",
    "High precision and recall in trigger detection",
    "Maintains accuracy on clean data comparable to vanilla GCN"
  ],
  "title": "Boosting Graph Robustness Against Backdoor Attacks an Over-Similarity Perspective.pdf",
  "abstract": "Graph Neural Networks (GNNs) have achieved notable success in tasks such as social and trans- portation networks. However, recent studies have highlighted the vulnerability of GNNs to back- door attacks, raising significant concerns about their reliability in real-world applications. De- spite initial efforts to defend against specific graph backdoor attacks, existing defense methods face two main challenges: either the inability to es- tablish a clear distinction between triggers and clean nodes, resulting in the removal of many clean nodes, or the failure to eliminate the impact of triggers, making it challenging to restore the target nodes to their pre-attack state. Through em- pirical analysis of various existing graph backdoor attacks, we observe that the triggers generated by these methods exhibit over-similarity in both fea- tures and structure. Based on this observation, we propose a novel graph backdoor defense method SimGuard. We first utilizes a similarity-based metric to detect triggers and then employs con- trastive learning to train a backdoor detector that generates embeddings capable of separating trig- gers from clean nodes, thereby improving detec- tion efficiency. Extensive experiments conducted on real-world datasets demonstrate that our pro- posed method effectively defends against various graph backdoor attacks while preserving perfor- mance on clean nodes. The code will be released upon acceptance. 1. Introduction Graph-structured data, which serves as a fundamental and ubiquitous representation in the real world, plays a pivotal role in modeling complex interactions among entities, such as those observed in social networks, transportation systems, and protein-protein interaction networks (Fan et al., 2019; 1Beijing University of Posts and Telecommunications. Corre- spondence to: Hai Huang <hhuang@bupt.edu.cn>. Rahmani et al., 2023). Graph Neural Networks (GNNs) (Kipf & Welling, 2016; Velickovic et al., 2017; Hamilton et al., 2017), widely recognized as representative method- ologies in graph-based machine learning, are capable of deriving high-quality representations from graph data. How- ever, despite the remarkable performance of GNNs across various tasks, recent studies (Xi et al., 2021; Zhang et al., 2021; Dai et al., 2023; Zhang et al., 2024a) have revealed that they are vulnerable to backdoor attacks. Backdoor at- tacks on GNNs typically involve generating and attaching backdoor triggers to a selected set of target nodes, which are subsequently assigned to a specific target class. These trig- gers, often represented as nodes or subgraphs, can be either predefined or dynamically created using a trigger generator. During training on a dataset contaminated with these trig- gers, due to the graph message-passing paradigm, the GNN model learns to associate the presence of the trigger with the specific target class. Consequently, during inference, the backdoored model misclassifies test nodes containing the trigger into the target class while maintaining high predic- tive accuracy for clean nodes without triggers. Backdoor attacks on graphs pose a significant challenge to the reliable deployment of GNNs in real-world applications. The study of graph backdoor attacks and defenses has gar- nered increasing attention, with several foundational efforts emerging in this area (Zhang et al., 2021; Dai et al., 2023). As the pioneering work on graph backdoor attacks, SBA (Zhang et al., 2021) employs randomly generated graphs as triggers to launch attacks. GTA (Xi et al., 2021) first employs a backdoor trigger generator that creates more ef- fective triggers. Building on this foundation, UGBA (Dai et al., 2023) and DPGBA (Zhang et al., 2024a) constrain the trigger generator from the perspectives of homogeneity and feature distribution, making the generated triggers more dif- ficult to detect. As graph backdoor attacks have advanced, efforts have also been made to develop graph backdoor defenses. Existing graph backdoor defense methods can generally be categorized into two strategies: deletion-based strategies and robust training-based strategies. Deletion- based defense methods detect and remove suspicious back- door connections using trigger detection techniques. When accurate, they effectively eliminate trigger influence on tar- get nodes. However, these methods often rely on specific 1 arXiv:2502.01272v1 [cs.LG] 3 Feb 2025"
}