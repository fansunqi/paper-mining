{
  "code_links": "None",
  "tasks": [
    "Mathematical Reasoning"
  ],
  "datasets": [
    "GSM8K"
  ],
  "methods": [
    "Uncertainty-Aware Search",
    "Uncertainty-Aware Value Models",
    "Group Thompson Sampling"
  ],
  "results": [
    "UVM-guided search mitigates scaling flaws",
    "UVM-guided search achieves higher coverage than conventional VM-guided search"
  ],
  "title": "Uncertainty-Aware Search and Value Models Mitigating Search Scaling Flaws in LLMs.pdf",
  "abstract": "Value model-guided search is effective in steer- ing the generation but suffers from scaling flaws: Its superiority diminishes with larger sample sizes, underperforming non-search baselines. This limitation arises from relia- bility degradation in value models in unseen reasoning paths. To address this, we propose an uncertainty-aware search framework that in- cludes two key components: (1) uncertainty- aware value models that incorporate uncer- tainty into predictions, and (2) an uncertainty- aware selection process using the proposed ef- ficient Group Thompson Sampling algorithm. Experiments on GSM8K show that our method mitigates search scaling flaws, achieving 90.5% coverage at 16 samples compared to 85.8% for conventional value-guided search. This work establishes the first systematic integration of uncertainty quantification in LLM search paradigms. 1 Introduction Test-time scaling (Brown et al., 2024; Snell et al., 2024; Wu et al., 2024) boosts performance sig- nificantly on multi-step mathematical reasoning tasks (Cobbe et al., 2021; Hendrycks et al., 2021). Value Model (VM)-guided search (Yu et al., 2024; Wan et al., 2024) efficiently solves more problems by steering the generation toward more effective reasoning paths. However, a recent study (Yu et al., 2025) iden- tifies scaling flaws in conventional VM-guided search: it outperforms repeated sampling (i.e. non- search baseline) at small sample sizes but improves more slowly, leading to inferior performance at larger sample sizes. As shown in Table 1, VM- guided search surpasses repeated sampling at a sample size of 1 (75.4% v.s. 52.9%), but loses the superiority as the sample sizes increase to 8 and 16 (85.8% v.s. 90.8%). \u2020Corresponding to Yingru Li and Benyou Wang. Table 1: Comparison of coverage on GSM8K: Con- ventional VM-guided search faces search scaling flaws, underperforming the non-search baseline (repeated sam- pling). Our proposed uncertainty-aware search effec- tively enhances the effectiveness of the search scaling. Sample Size 1 8 16 Repeated Sampling 52.9% \u00b1 0.6% 84.7% \u00b1 0.8% 90.8% \u00b1 0.1% VM-Guided Search 75.4% \u00b1 0.6% 84.0% \u00b1 0.4% 85.8% \u00b1 0.3% Uncertainty-Aware Search (Ours) 67.8% \u00b1 0.4% 87.5% \u00b1 0.6% 90.5% \u00b1 0.1% According to Yu et al. (2025), this issue arises from VM failures during the evaluation and selec- tion of candidates in the search process. When VMs underestimate the values of promising can- didates and misidentify them, the selection fails, ultimately leading to search failures. Uncertainty-Aware Modeling To mitigate scal- ing flaws, it is crucial to address unreliable VM predictions at the selection stage. Since VMs rely heavily on training data, they are more likely to pro- vide unreliable predictions when encountering un- seen data during inference. This can be addressed by incorporating uncertainty to reflect the reliabil- ity of VM evaluations, particularly for candidates underrepresented in the training data. Intuitively, candidates less frequently seen during training have higher uncertainty, indicating lower VM reliability and greater risk of selection failures. To capture uncertainty and develop Uncertainty- Aware Value Models (UVMs), we employ Ensem- ble++ architecture (Li et al., 2024a,b) to model value distributions, which encapsulate the inherent uncertainty, with a more dispersed distribution indi- cating higher uncertainty. The trained UVM allows us to evaluate candidates while accounting for un- certainty by sampling from the value distribution. Uncertainty-Aware Selection During Search Leveraging the accessibility to uncertainty-aware value distributions, we develop an efficient algo- 1 arXiv:2502.11155v1 [cs.AI] 16 Feb 2025"
}