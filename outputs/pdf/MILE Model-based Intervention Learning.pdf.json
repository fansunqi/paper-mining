{
  "code_links": [
    "https://liralab.usc.edu/mile"
  ],
  "tasks": [
    "Interactive Imitation Learning",
    "Model-based Intervention Learning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Intervention Model",
    "Neural Network",
    "Policy Learning",
    "Probit Model"
  ],
  "results": [
    "Higher sample-efficiency",
    "Near-optimal success levels",
    "80% success rate"
  ],
  "title": "MILE Model-based Intervention Learning.pdf",
  "abstract": "\u2014 Imitation learning techniques have been shown to be highly effective in real-world control scenarios, such as robotics. However, these approaches not only suffer from compounding error issues but also require human experts to provide complete trajectories. Although there exist interactive methods where an expert oversees the robot and intervenes if needed, these extensions usually only utilize the data collected during intervention periods and ignore the feedback signal hidden in non-intervention timesteps. In this work, we create a model to formulate how the interventions occur in such cases, and show that it is possible to learn a policy with just a handful of expert interventions. Our key insight is that it is possible to get crucial information about the quality of the current state and the optimality of the chosen action from expert feedback, regardless of the presence or the absence of intervention. We evaluate our method on various discrete and continuous simulation environments, a real-world robotic manipulation task, as well as a human subject study. Videos and the code can be found at https://liralab.usc.edu/mile. I. INTRODUCTION Imagine training a household robot to help users place the dishes in the dishwasher. One way to do this is to use reinforcement learning (RL) that has been proven successful in several areas ranging from gaming to dialogue systems and autonomous driving [1]\u2013[4]. However, its need for lots of online interactions with the environment as well as a well- defined reward function make it unsuitable in a real-world situation like this. An alternative is to use imitation learning (IL) where an expert provides demonstrations of how to place the dishes. IL requires fewer interactions in the world than RL and does not require a reward function. A common drawback of this approach is the compounding distributional shift, which results from the accumulation of errors when deploying a learned policy [5]: the small inaccuracies in the robot\u2019s learned policy will move it to an out-of-distribution state where the policy may fail more significantly, which may result in the robot breaking the dishes. Interactive learning methods try to overcome this com- pounding errors problem by iteratively querying the expert with system states, and fine-tuning the policy based on the expert actions [6]\u2013[11]. Most of these methods do not let the human intervene at will but transfer the control to the human according to some criterion [8], [9]. In others, the human can take over the control at any timestep [7], [12]\u2013[14]. Going back to the running example, assume the robot had a mediocre policy in the beginning, thanks to some initial training done in the factory setup. In this interactive scheme, we would control the robot only when we think it is doing or about to do something wrong. This is clearly Both authors are with Thomas Lord Department of Computer Science, University of Southern California. Emails: {ykorkmaz, biyik}@usc.edu more convenient than operating the robot for long periods to generate full demonstrations. But what about the intervals where we did not input any actions? Did we not provide any information to the robot? The answer is no: the fact that we chose to not intervene means the robot\u2019s actions were already good enough in those intervals. This is what the existing interactive learning techniques are missing: even though they try to improve the robot\u2019s policy based on the states where the expert intervenes, they do not utilize any structure of how or when those interventions occur, ignoring an important feedback signal that is leaking through the states where the expert does not intervene. To efficiently use the information in both the states with and without human interventions, we argue one must un- derstand and utilize the structure behind how interventions occur. To this end, we make the following contributions: 1) We propose a novel model that is fully differentiable to formulate how and when an expert intervenes. 2) We utilize this intervention model to fine-tune a weak policy and evaluate our method in various simulated and real-world environments to prove its effectiveness. 3) We compare our method against the state-of-the-art baselines which utilize interventions to show its higher sample-efficiency and performance. II. RELATED WORK Interactive Imitation Learning. In imitation learning, ex- pert data is used to train a policy in a supervised way [15]\u2013 [20]. Interactive imitation learning methods try to overcome its compounding errors problem [5], [6] by querying the expert on the learned policy\u2019s rollouts [6]\u2013[9], [15], [21]. While the expert relabels either the whole trajectory of the policy with actions, or some parts of it that are automatically selected by estimating various task performance quantities [6], [9], there are also works where the expert has the freedom to intervene at will [7], [12], [13], [22]. Some of these methods consider the implicit feedback coming from the states where the expert chooses to not intervene. They attempt to incorporate the information leaking from non- intervention intervals either by enforcing those state-action pairs to be constrained in the action-value cost functions [14] or utilize weighted behavioral cloning (BC) to incorporate the signal from non-interventions, with different heuristics used for assigning weights to on-policy robot samples, and expert human interventions [12], [13], [22]. However, none of these algorithms uses a model to understand and learn from why the expert chooses to not intervene. In our method, we propose a model that attempts to capture how the interventions occur, and how satisfied the user is with arXiv:2502.13519v1 [cs.RO] 19 Feb 2025"
}