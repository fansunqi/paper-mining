{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Robust Multi-Agent Reinforcement Learning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Wolfpack Adversarial Attack",
    "WALL framework"
  ],
  "results": [
    "WALL significantly improves robustness compared to existing methods"
  ],
  "title": "Wolfpack Adversarial Attack for Robust Multi-Agent Reinforcement Learning.pdf",
  "abstract": "Traditional robust methods in multi-agent re- inforcement learning (MARL) often struggle against coordinated adversarial attacks in coop- erative scenarios. To address this limitation, we propose the Wolfpack Adversarial Attack frame- work, inspired by wolf hunting strategies, which targets an initial agent and its assisting agents to disrupt cooperation. Additionally, we introduce the Wolfpack-Adversarial Learning for MARL (WALL) framework, which trains robust MARL policies to defend against the proposed Wolfpack attack by fostering system-wide collaboration. Ex- perimental results underscore the devastating im- pact of the Wolfpack attack and the significant robustness improvements achieved by WALL. 1. Introduction Multi-agent Reinforcement Learning (MARL) has gained attention for solving complex problems requiring agent co- operation (Oroojlooy & Hajinezhad, 2023) and competition, such as drone control (Yun et al., 2022), autonomous nav- igation (Chen et al., 2023), robotics (Orr & Dutta, 2023), and energy management (Jendoubi & Bouffard, 2023). To handle partially observable environments, the Centralized Training and Decentralized Execution (CTDE) framework (Oliehoek et al., 2008) trains a global value function cen- trally while agents execute policies based on local obser- vations. Notable credit-assignment methods in CTDE in- clude Value Decomposition Networks (VDN) (Sunehag et al., 2017), QMIX (Rashid et al., 2020), which satisfies the Individual-Global-Max (IGM) condition ensuring that optimal joint actions align with positive gradients in global and individual value functions, and QPLEX (Wang et al., 2020b), which encodes IGM into its architecture. However, CTDE methods face challenges from exploration inefficien- cies (Mahajan et al., 2019; Jo et al., 2024) and mismatches 1Graduate School of Artificial Intelligence, UNIST, Ul- san, South Korea. \u2217Correspondence to: Seungyul Han <sy- han@unist.ac.kr>. Proceedings of the 42 st International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s). between training and deployment environments, leading to unexpected agent behaviors and degraded performance (Moos et al., 2022; Guo et al., 2022). Thus, enhancing the robustness of CTDE remains a critical research focus. To improve learning robustness, single-agent RL methods have explored strategies based on game theory (Yu et al., 2021), such as max-min approaches and adversarial learning (Goodfellow et al., 2014; Huang et al., 2017; Pattanaik et al., 2017; Pinto et al., 2017). In multi-agent systems, simulta- neous agent interactions introduce additional uncertainties (Zhang et al., 2021b). To address this, methods like per- turbing local observations (Lin et al., 2020), training with adversarial policies for Nash equilibrium (Li et al., 2023a), adversarial value decomposition (Phan et al., 2021), and attacking inter-agent communication (Xue et al., 2021) have been proposed. However, these approaches often target a single agent per attack, overlooking interdependencies in cooperative MARL, making them vulnerable to scenarios where multiple agents are attacked simultaneously. To overcome the vulnerabilities posed by coordinated ad- versarial attacks in MARL, we propose the Wolfpack adver- sarial attack framework, inspired by wolf hunting strategies. This approach disrupts inter-agent cooperation by target- ing a single agent and subsequently attacking the group of agents assisting the initially targeted agent, resulting in more devastating impacts. Experimental results reveal that traditional robust MARL methods are highly susceptible to such coordinated attacks, underscoring the need for new defense mechanisms. In response, we also introduce the Wolfpack-Adversarial Learning for MARL (WALL) frame- work, a robust policy training approach specifically designed to counter the Wolfpack Adversarial Attack. By fostering system-wide collaboration and avoiding reliance on spe- cific agent subsets, WALL enables agents to defend effec- tively against coordinated attacks. Experimental evaluations demonstrate that WALL significantly improves robustness compared to existing methods while maintaining high per- formance under a wide range of adversarial attack scenarios. The key contributions of this paper in constructing the Wolf- pack Adversarial Attack are summarized as follows: \u2022 A novel MARL attack strategy, Wolfpack Adversarial Attack, is introduced, targeting multiple agents simul- taneously to foster stronger and more resilient agent 1 arXiv:2502.02844v1 [cs.LG] 5 Feb 2025"
}