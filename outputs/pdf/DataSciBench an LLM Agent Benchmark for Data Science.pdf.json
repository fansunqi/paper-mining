{
  "code_links": [
    "https://datascibench.github.io/"
  ],
  "tasks": [
    "Data Science"
  ],
  "datasets": [
    "CodeGeeX",
    "BigCodeBench",
    "BCB"
  ],
  "methods": [
    "Task-Function-Code (TFC) framework",
    "LLM-based self-consistency and human verification",
    "VLM-as-a-judge"
  ],
  "results": [
    "API-based models outperform open-sourced models",
    "GPT-4o achieves the highest score among open-sourced models"
  ],
  "title": "DataSciBench an LLM Agent Benchmark for Data Science.pdf",
  "abstract": "This paper presents DataSciBench, a com- prehensive benchmark for evaluating Large Language Model (LLM) capabilities in data science. Recent related benchmarks have pri- marily focused on single tasks, easily obtain- able ground truth, and straightforward eval- uation metrics, which limits the scope of tasks that can be evaluated. In contrast, DataSciBench is constructed based on a more comprehensive and curated collection of natural and challenging prompts for uncertain ground truth and evaluation metrics. We de- velop a semi-automated pipeline for generat- ing ground truth (GT) and validating evalua- tion metrics. This pipeline utilizes and im- plements an LLM-based self-consistency and human verification strategy to produce accu- rate GT by leveraging collected prompts, pre- defined task types, and aggregate functions (metrics). Furthermore, we propose an inno- vative Task - Function - Code (TFC) frame- work to assess each code execution outcome based on precisely defined metrics and pro- grammatic rules. Our experimental frame- work involves testing 6 API-based models, 8 open-source general models, and 9 open- source code generation models using the di- verse set of prompts we have gathered. This approach aims to provide a more comprehen- sive and rigorous evaluation of LLMs in data science, revealing their strengths and weak- nesses. Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder- 33B-Instruct achieves the highest score among open-sourced models. We release all code and data at https://github.com/THUDM/ DataSciBench/. 1 Introduction Large language models (LLMs) (Achiam et al., 2023; Team et al., 2023; GLM et al., 2024) are \u2020 work done while these authors interned at Zhipu AI. Implement the desired _marketing_expenditure function, which returns the required amount of money that needs to be invested in a new marketing campaign to sell the desired number of units. Subtasks: Task (1) Expenditure Calculation:\\n -Input: Use the model parameters from `model_parameters.csv` and the desired number of units to sell.\\n - Output: Calculate the required marketing expenditure and save the result as `required_expenditure.txt` containing a single float value. Task (2) \u00b7\u00b7\u00b7 Task (3) \u00b7\u00b7\u00b7 Predictive modeling R-squared Value Task Function Code def model_performance_r_squared(ground_truth): import pandas as pd from sklearn.metrics import r2_score output = pd.read_csv(\"model_parameters.csv\") output_gt = pd.read_csv(ground_truth) # Read cleaned campaign data for actual units sold cleaned_data = pd.read_csv(\"cleaned_campaign_data.csv\") X = cleaned_data[['Marketing expenditure']] y = cleaned_data['Units sold'] # Retrieving model coefficients coefficients = output.iloc[0].tolist() model_predictions = X * coefficients[0] coefficients_gt = output_gt.iloc[0].tolist() model_predictions_gt = X * coefficients_gt[0] return bool(r2_score(y, model_predictions) >= r2_score(y, model_predictions_gt)) DataSciBench Vanilla Response Prompt for Data Science import csv def load_model_parameters(filename): with open(filename, mode='r') as file: csv_reader = csv.DictReader(file) parameters = {} for row in csv_reader: parameters[row['Parameter']] = float(row['Value' return parameters def calculate_marketing_expenditure(parameters, desired_unit a = parameters['a'] b = parameters['b'] expenditure = a + b * desired_units return expenditure def save_expenditure(expenditure, filename='required_expendit with open(filename, mode='w') as file: file.write(f\"{expenditure:.2f}\") def main(): param_file = 'model_parameters.csv' desired_units = 1000 # You can modify this value as need # Load model parameters parameters = load_model_parameters(param_file) # Calculate required expenditure expenditure = calculate_marketing_expenditure(parameters # Save the result 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 \u2304 \u2304 \u2304 \u2304 \u2304 \u2304 \u2304 Figure 1: This example compares the vanilla response and the DataSciBench response for a given prompt. The vanilla response provides only code, lacking eval- uation metrics. In contrast, DataSciBench identi- fies evaluation tasks, provides evaluation functions, and generates programmatic code to form a TFC list. increasingly used in data science and scientific do- mains, e.g., data analysis (Hong et al., 2024), pro- tein generation (Jumper et al., 2021; Chen et al., 2024a), and scientific discovery (Lu et al., 2024) and reasoning (Zhang et al., 2024a,b). For data science tasks, given a publicly known problem, LLMs offer the potential to (semi-)autonomously conduct data analysis (Huang et al., 2023) and data visualization (Hong et al., 2024) by invok- ing code interpreters with corresponding Python libraries. These works are benchmarked on rel- atively straightforward tasks where ground truth (GT) labels can be precisely obtained. However, much of real-world data analysis requires reason- ing over more complex scenarios (Chen et al., 2024b) as shown in Figure 1, such as calculat- ing expenditure, and evaluating the quality of the images generated by the data visualization task. Properly evaluating these more complex data sci- ence tasks remains an open research direction. While some existing benchmarks are used to arXiv:2502.13897v1 [cs.CL] 19 Feb 2025"
}