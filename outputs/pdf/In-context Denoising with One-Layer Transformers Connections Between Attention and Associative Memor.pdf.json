{
  "code_links": [
    "None"
  ],
  "tasks": [
    "In-context denoising"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "One-layer transformer",
    "Dense associative memory (DAM) networks",
    "Bayesian framework"
  ],
  "results": [
    "One-layer transformers can solve certain denoising problems optimally",
    "Trained attention layers process denoising prompts by performing a single gradient descent update on a context-aware DAM energy landscape"
  ],
  "title": "In-context Denoising with One-Layer Transformers Connections Between Attention and Associative Memor.pdf",
  "abstract": "We introduce in-context denoising, a task that refines the connection between attention-based ar- chitectures and dense associative memory (DAM) networks, also known as modern Hopfield net- works. Using a Bayesian framework, we show the- oretically and empirically that certain restricted denoising problems can be solved optimally even by a single-layer transformer. We demonstrate that a trained attention layer processes each de- noising prompt by performing a single gradient descent update on a context-aware DAM energy landscape, where context tokens serve as asso- ciative memories and the query token acts as an initial state. This one-step update yields better solutions than exact retrieval of either a context token or a spurious local minimum, providing a concrete example of DAM networks extending beyond the standard retrieval paradigm. Overall, this work solidifies the link between associative memory and attention mechanisms first identified by Ramsauer et al., and demonstrates the rele- vance of associative memory models in the study of in-context learning. 1. Introduction The transformer architecture (Vaswani et al., 2017) has achieved remarkable success across diverse domains, from natural language processing (Devlin et al., 2019; Brown et al., 2020; Touvron et al., 2023) to computer vision (Doso- vitskiy, 2020). Despite their practical success, understand- ing the mechanisms behind transformer-based networks remains an open challenge. This challenge is exacerbated 1 Center for Computational Biology, Flatiron Institute, New York, NY, USA 2 Center for Computational Mathematics, Flat- iron Institute, New York, NY, USA 3 Center for Computa- tional Quantum Physics, Flatiron Institute, New York, NY, USA 4 Department of Physics and Astronomy, Rutgers Univer- sity, Piscataway, NJ, USA. Correspondence to: Matthew Smart <msmart@flatironinstitute.org>, Anirvan M. Sengupta <asen- gupta@flatironinstitute.org>. by the growing scale and complexity of modern large net- works. Toward addressing this, researchers studying simpli- fied architectures have identified connections between the attention operation that is central to transformers and asso- ciative memory models (Ramsauer et al., 2021), providing not only an avenue for understanding how such architectures encode and retrieve information but also potentially ways to improve them further. The most celebrated model for associative memories in sys- tems neuroscience is the so-called Hopfield model (Amari, 1972; Nakano, 1972; Little, 1974; Hopfield, 1982). This model has a capacity to store \u201cmemories\u201d (stable fixed points of a recurrent update rule) proportional to the number of nodes (Hopfield, 1982; Amit et al., 1985). In the last decade, new energy functions (Krotov & Hopfield, 2016; Demircigil et al., 2017) were proposed for dense associative associative memories with much higher capacities. These energy functions are often referred to as modern Hopfield models. Ramsauer et al. (2021) pointed out the similar- ity between the one-step update rule of a certain modern Hopfield network (Demircigil et al., 2017) and a particu- lar one-layer transformer map, generating interest in the statistical physics and the systems neuroscience commu- nity (Krotov & Hopfield, 2021; Krotov, 2023; Lucibello & M\u00b4ezard, 2024). However, the construction in (Ramsauer et al., 2021) appears to emphasize the specific task of ex- act retrieval (converging to a fixed point), while in practice transformers may tackle many other tasks. To explore this connection beyond retrieval, we intro- duce in-context denoising, a task that bridges the behav- ior of trained transformers and associative memory net- works through the lens of in-context learning (ICL). In standard ICL, a sequence model is trained to infer an un- known function g from contextual examples, predicting g(XL+1) given a sequence of input-output pairs E = ((X1, g(X1)), ..., (XL, g(XL)), (XL+1, \u2212)). Crucially, g is implied solely through the context and differs across prompts \u2013 performant models are therefore said to \u201clearn g(x) in context\u201d. While ICL has been extensively stud- ied in supervised settings (Garg et al., 2022; Zhang et al., 2024; Aky\u00a8urek et al., 2023; Reddy, 2024), recent work suggests that transformers may internally emulate gradient 1 arXiv:2502.05164v1 [cs.LG] 7 Feb 2025"
}