{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Best arm identification with post-action context"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Track-and-Stop algorithm",
    "G-tracking rule"
  ],
  "results": [
    "Asymptotically achieve optimal sample complexity"
  ],
  "title": "Optimal Best Arm Identification with Post-Action Context.pdf",
  "abstract": "We introduce the problem of best arm identifica- tion (BAI) with post-action context, a new BAI problem in a stochastic multi-armed bandit envi- ronment and the fixed-confidence setting. The problem addresses the scenarios in which the learner receives a post-action context in addition to the reward after playing each action. This post-action context provides additional informa- tion that can significantly facilitate the decision process. We analyze two different types of the post-action context: (i) non-separator, where the reward depends on both the action and the con- text, and (ii) separator, where the reward depends solely on the context. For both cases, we derive instance-dependent lower bounds on the sample complexity and propose algorithms that asymp- totically achieve the optimal sample complexity. For the non-separator setting, we do so by demon- strating that the Track-and-Stop algorithm can be extended to this setting. For the separator set- ting, we propose a novel sampling rule called G- tracking, which uses the geometry of the context space to directly track the contexts rather than the actions. Finally, our empirical results showcase the advantage of our approaches compared to the state of the art. 1. Introduction Multi-armed bandit (MAB) refers to a class of sequen- tial decision-making problems, where a learner selects ac- tions (arms) in order to maximize a reward. MAB has widespread applications in various domains, such as clin- ical trials (Thompson, 1933), dynamic pricing (Kleinberg & Leighton, 2003; Besbes & Zeevi, 2009), recommender systems (Li et al., 2010), and resource allocation (Gai et al., 2012). Depending on the learner\u2019s goal and constraints, 1College of Management of Technology, EPFL. 2School of Computer and Communication Sciences, EPFL. 3Department of Computer Engineering, Sharif University of Technology. Correspondence to: Mohammad Shahverdikondori <moham- mad.shahverdikondori@epfl.ch>. X Z Y (a) Non-separator context X Z Y (b) Separator context Figure 1: Two possible structures for the post-action context. different objectives may be pursued. For example, if the learner\u2019s goal is to minimize cumulative regret, they must balance the exploration-exploitation trade-off (Auer, 2002; Garivier & Capp\u00b4e, 2011). Alternatively, in the best Arm Identification (BAI) setting, the learner seeks the arm with the highest expected reward and must minimize the sample complexity\u2014i.e., the number of rounds needed to identify this arm (Mannor & Tsitsiklis, 2004; Audibert & Bubeck, 2010; Garivier & Kaufmann, 2016). Best arm identification is typically studied in two scenarios: fixed-budget, where the time horizon T is fixed, and the ob- jective is to minimize the error probability (Gabillon et al., 2012; Karnin et al., 2013; Carpentier & Locatelli, 2016); and fixed-confidence, where the error probability \u03b4 for iden- tifying the best arm is fixed, and the objective is to minimize the sample complexity (Garivier & Kaufmann, 2016; Audib- ert & Bubeck, 2010; Kaufmann, 2020; Jamieson & Nowak, 2014; Mannor & Tsitsiklis, 2004). We focus on the latter. While the classic MAB model is suitable for a broad range of applications, additional side information about the en- vironment can lead to more efficient algorithms. In the bandit literature, various types of side information have been considered. For example, causal bandits (Lattimore et al., 2016; Lu et al., 2020) or linear bandits (Auer, 2002; Abbasi-Yadkori et al., 2011) impose specific structures on the actions, and contextual bandits (Tewari & Murphy, 2017; Langford & Zhang, 2007) allow the learner to observe a con- text before choosing an action. In this work, we consider a different form of side information, which we call post- action context. Specifically, after choosing an action in each round, the learner receives intermediate feedback from the environment along with the reward. This post-action context can substantially accelerate the process of identi- 1 arXiv:2502.03061v1 [cs.LG] 5 Feb 2025"
}