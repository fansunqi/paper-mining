{
  "code_links": [
    "https://github.com/hustmrlab/PoGDiff"
  ],
  "tasks": [
    "Imbalanced Text-to-Image Generation"
  ],
  "datasets": [
    "AgeDB-IT2I",
    "DigiFace-IT2I"
  ],
  "methods": [
    "Product-of-Gaussians Diffusion Models (PoGDiff)"
  ],
  "results": [
    "FID \u2193",
    "gRecall Score \u2191",
    "Human & GPT-4o Evaluation Score \u2191",
    "DINO Score \u2191"
  ],
  "title": "PoGDiff Product-of-Gaussians Diffusion Models for Imbalanced Text-to-Image Generation.pdf",
  "abstract": "Diffusion models have made significant advance- ments in recent years. However, their perfor- mance often deteriorates when trained or fine- tuned on imbalanced datasets. This degradation is largely due to the disproportionate representation of majority and minority data in image-text pairs. In this paper, we propose a general fine-tuning approach, dubbed PoGDiff, to address this chal- lenge. Rather than directly minimizing the KL di- vergence between the predicted and ground-truth distributions, PoGDiff replaces the ground-truth distribution with a Product of Gaussians (PoG), which is constructed by combining the original ground-truth targets with the predicted distribu- tion conditioned on a neighboring text embedding. Experiments on real-world datasets demonstrate that our method effectively addresses the imbal- ance problem in diffusion models, improving both generation accuracy and quality. 1. Introduction The development of diffusion models (Ho et al., 2020; Song et al., 2020b) and their subsequent extensions (Song et al., 2020a; Nichol & Dhariwal, 2021; Huang et al., 2023) has significantly advanced the learning of complex probability distributions across various data types, including images (Ho et al., 2022; Rombach et al., 2022; Saharia et al., 2022; Ho & Salimans, 2022), audio (Kong et al., 2020), and 3D biomed- ical imaging data (Luo & Hu, 2021; Poole et al., 2022; Shi et al., 2023; Pinaya et al., 2022). For these generative models, the amount of training data plays a critical role in determining both the accuracy of probability estimation and the model\u2019s ability to generalize, which enables effective extrapolation within the probability space. Data diversity and abundance are key to improving the gen- erative capabilities of large-scale models, enabling them to capture intricate details within a vast probability space. However, many data-driven modeling tasks often rely on small, imbalanced real-world datasets, leading to poor gener- ation quality, particularly for minority groups. For example, Ground Truth Stable Diffusion CBDM PoGDiff (Ours) Jeanette MacDonald Edward G. Robinson J.P. Morgan Albert Einstein J. Willard Marriott Ground Truth Stable Diffusion CBDM PoGDiff (Ours) Ground Truth Stable Diffusion CBDM PoGDiff (Ours) Ground Truth Stable Diffusion CBDM PoGDiff (Ours) Ground Truth Stable Diffusion CBDM PoGDiff (Ours) High Density Low Density Figure 1. PoGDiff for imbalanced text-to-image generation. Ex- isting methods, e.g., Stable Diffusion (Rombach et al., 2022) and CBDM (Qin et al., 2023), fall short for minority data (Low Den- sity). In contrast, Our PoGDiff successfully generates high-quality images even for minority data, outperforming all baselines. when training and fine-tuning a diffusion model with an imbalanced dataset of individuals, existing models often struggle to generate accurate images for those who appear less frequently (i.e., minorities) in the training data (Fig. 1). This challenge is further compounded when accuracy is prioritized over simply high resolution. For example, gener- ated images of individuals need to match the identity of at least one individual in the training set (Fig. 1). Addressing this gap is crucial for deploying diffusion models in real- world applications where correctness is paramount, such as personalized content generation or medical imaging. This limitation is true even for finetuning large diffusion models pretrained on large-scale datasets like LAION- 5B (Schuhmann et al., 2022), e.g., Stable Diffusion (Rom- bach et al., 2022). Imagine an imbalanced dataset consisting of employees in a small company, senior employees might have more photos available, while new employees only have a very limited number of them. Since none of the employees appear in the LAION-5B dataset, generating photos of them require finetuning the Stable Diffusion model. Unfortu- nately, finetuning the model on such an imbalanced dataset might enable the model to generate accurate images for the majority group (i.e., senior employees), but it will perform poorly for the minority group (i.e., new employees). To address this challenge, we propose a general fine-tuning approach, dubbed PoGDiff. Rather than directly minimizing the KL divergence between the predicted and ground-truth distributions, PoGDiff replaces the ground-truth distribution with a Product of Gaussians (PoG), which is constructed by combining the original ground-truth targets with the 1 arXiv:2502.08106v1 [cs.LG] 12 Feb 2025"
}