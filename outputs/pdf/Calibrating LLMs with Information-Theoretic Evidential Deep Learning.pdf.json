{
  "code_links": [
    "https://github.com/sandylaker/ib-edl"
  ],
  "tasks": [
    "Uncertainty quantification",
    "Calibration of fine-tuned LLMs"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Information Bottleneck (IB)",
    "Evidential Deep Learning (EDL)",
    "Variational Bayes (VB)"
  ],
  "results": [
    "Improved calibration",
    "Reduced overconfidence",
    "Better uncertainty estimation"
  ],
  "title": "Calibrating LLMs with Information-Theoretic Evidential Deep Learning.pdf",
  "abstract": "Fine-tuned large language models (LLMs) often exhibit overcon\ufb01dence, particu- larly when trained on small datasets, resulting in poor calibration and inaccurate uncertainty estimates. Evidential Deep Learning (EDL), an uncertainty-aware approach, enables uncertainty estimation in a single forward pass, making it a promising method for calibrating \ufb01ne-tuned LLMs. However, despite its compu- tational ef\ufb01ciency, EDL is prone to over\ufb01tting, as its training objective can re- sult in overly concentrated probability distributions. To mitigate this, we propose regularizing EDL by incorporating an information bottleneck (IB). Our approach IB-EDL suppresses spurious information in the evidence generated by the model and encourages truly predictive information to in\ufb02uence both the predictions and uncertainty estimates. Extensive experiments across various \ufb01ne-tuned LLMs and tasks demonstrate that IB-EDL outperforms both existing EDL and non-EDL ap- proaches. By improving the trustworthiness of LLMs, IB-EDL facilitates their broader adoption in domains requiring high levels of con\ufb01dence calibration. Code is available at https://github.com/sandylaker/ib-edl. 1 INTRODUCTION Large language models (LLMs) have revolutionized natural language processing, with \ufb01ne-tuning emerging as a prevalent method to adapt these models for speci\ufb01c tasks or domains (Houlsby et al., 2019; Hu et al., 2022). However, \ufb01ne-tuned LLMs often display overcon\ufb01dence in their predic- tions (Jiang et al., 2021; Yang et al., 2024), which compromises their reliability and limits their ap- plicability in critical domains where trustworthiness is essential. Overcon\ufb01dence in LLMs often manifests as poor calibration, where the predicted probabilities do not accurately re\ufb02ect the model\u2019s uncertainty about its predictions. Uncertainty-aware methods improve calibration by explicitly quantifying the uncertainty in the model\u2019s predictions, allowing the model to produce con\ufb01dence scores that better correspond to the actual likelihood of correct- ness. Traditional uncertainty-aware methods, such as MC-Dropout (Gal & Ghahramani, 2016) and Deep Ensemble (Lakshminarayanan et al., 2017; Fort et al., 2019) are commonly used to mitigate overcon\ufb01dence in neural networks. However, these approaches typically require multiple forward passes, signi\ufb01cantly increasing the inference time for LLMs. Evidential Deep Learning (EDL) (Sensoy et al., 2018; Malinin & Gales, 2018) offers a more ef\ufb01- cient alternative by providing uncertainty estimates with a single forward pass. Despite its success in various tasks, recent studies (Deng et al., 2023; Chen et al., 2024) indicate that EDL can still yield overcon\ufb01dent predictions which leads to inaccurate uncertainty estimates, degrading the model\u2019s calibration performance. This issue arises from the propensity of vanilla EDL to encourage models to generate excessive evidence (i.e., support for a class) with extremely large magnitudes, leading to overly con\ufb01dent predicted class probabilities. Motivated by these challenges, we propose a novel regularization approach for EDL using an infor- mation bottleneck, which we term IB-EDL. IB-EDL adaptively distorts the evidence generated by the LLM while maximally preserving the model\u2019s performance. In doing so, IB-EDL encourages the model to suppress spurious or uninformative evidence that could lead to overcon\ufb01dent predictions. Our theoretical analysis shows that the information bottleneck effectively penalizes the generation 1"
}