{
  "code_links": [
    "https://github.com/Key5n/IncFed-MDRS/"
  ],
  "tasks": [
    "Time Series Anomaly Detection"
  ],
  "datasets": [
    "SMD",
    "SMAP",
    "PSM"
  ],
  "methods": [
    "Incremental Federated Learning with MD-RS (IncFed MD-RS)",
    "Mahalanobis Distance of Reservoir States (MD-RS)",
    "Reservoir Computing",
    "Echo State Networks (ESN)"
  ],
  "results": [
    "IncFed MD-RS significantly outperforms other methods across all evaluation metrics",
    "IncFed MD-RS performs well with relatively short training data compared to the two deep learning-based methods",
    "IncFed MD-RS is robust against subsampling N\u0303x to some extent",
    "IncFed MD-RS is well-suited for federated learning"
  ],
  "title": "Federated Learning with Reservoir State Analysis for Time Series Anomaly Detection.pdf",
  "abstract": "\u2014With a growing data privacy concern, federated learning has emerged as a promising framework to train machine learning models without sharing locally distributed data. In federated learning, local model training by multiple clients and model integration by a server are repeated only through model parameter sharing. Most existing federated learning methods assume training deep learning models, which are often com- putationally demanding. To deal with this issue, we propose federated learning methods with reservoir state analysis to seek computational efficiency and data privacy protection simultane- ously. Specifically, our method relies on Mahalanobis Distance of Reservoir States (MD-RS) method targeting time series anomaly detection, which learns a distribution of reservoir states for normal inputs and detects anomalies based on a deviation from the learned distribution. Iterative updating of statistical parameters in the MD-RS enables incremental federated learning (IncFed MD-RS). We evaluate the performance of IncFed MD-RS using benchmark datasets for time series anomaly detection. The results show that IncFed MD-RS outperforms other federated learning methods with deep learning and reservoir computing models particularly when clients\u2019 data are relatively short and heterogeneous. We demonstrate that IncFed MD-RS is robust against reduced sample data compared to other methods. We also show that the computational cost of IncFed MD-RS can be reduced by subsampling from the reservoir states without performance degradation. The proposed method is beneficial especially in anomaly detection applications where computational efficiency, algorithm simplicity, and low communication cost are required. Index Terms\u2014Reservoir Computing, Federated Learning, Time Series Anomaly Detection I. INTRODUCTION Driven by privacy concerns, a privacy-preserving decentral- ized approach, called federated learning [1], is expected to be a new design for machine learning implementation. In federated learning, each client trains a deep learning model with its local data, and then a federation of the local models is performed on a central server to aggregate all knowledge among clients. However, traditional federated learning methods are generally based on deep learning models, which require a large amount of computational resources on the client side. To address the issue, federated learning with reservoir computing models, such as echo state networks [2], is a promising option that can achieve computational efficiency and protect data privacy at the same time. However, such research is still limited. For example, Incremental Federated Learning with ESN (IncFed ESN) [3] is the first method that applies federated learning to ESN. Despite federated learning, constructed models through IncFed ESN are mathematically equivalent to models obtained with the corresponding central- ized methods. Other than such a federated learning method that uses a server with reservoir computing, there are decentralized federated learning methods that work without a server [4], [5]. Note that all of the methods are designed for time series prediction or classification tasks. To our best knowledge, no study has applied federated learning with reservoir computing to anomaly detection tasks. We propose a novel federated learning scheme for time se- ries anomaly detection with reservoir state analyses, called In- cFed MD-RS, which bases Mahalanobis Distance of Reservoir States (MD-RS) [6]. MD-RS utilizes distributional analysis of reservoir states to detect anomalous data. Standard anomaly detection methods with reservoir computing are based on a reconstruction approach in which models are trained with normal data to reconstruct input data [7], [8]. The anomaly score for unknown input data is given by the reconstruction error, which is expected to be small for normal data due to successful reconstruction and large for anomaly data due to reconstruction failure. However, the reconstruction often fails for normal data when they are too short or noisy. Unlike such a reconstruction error-based method, MD-RS uses the distance between the current reservoir state and the distribution of the normal reservoir states for anomaly detection. It combines the statistical method for anomaly detection using Mahalanobis distance and a reservoir-based nonlinear feature extraction from input data. During training, MD-RS fits a multivariate Guassian distribution to reservoir state vectors for only normal data in the training phase. After training, it calculates anomaly scores given by the Mahalanobis distance between a reservoir state vector for test input data and the fitted distribution. We leverage MD-RS in federated learning settings to per- form efficient time series anomaly detection while protecting data privacy. In particular, Incremental Federated Learning with MD-RS (IncFed MD-RS) enables to construct the same anomaly detection model as that obtained with the centralized method as well as IncFed ESN. IncFed MD-RS outperforms other federated learning meth- ods for time series anomaly detection, including reservoir- based and deep learning-based methods for every dataset. Moreover, IncFed MD-RS achieves high performance even arXiv:2502.05679v1 [cs.LG] 8 Feb 2025"
}