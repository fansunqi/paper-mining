{
  "code_links": [
    "https://github.com/amir-abolfazli/CLTV"
  ],
  "tasks": [
    "Offline Reinforcement Learning",
    "Trajectory Valuation"
  ],
  "datasets": [
    "D4RL",
    "MuJoCo"
  ],
  "methods": [
    "Transition Scoring (TS)",
    "Curriculum Learning-Based Trajectory Valuation (CLTV)"
  ],
  "results": [
    "Improved performance and transferability of offline RL policies",
    "Significant improvements across various environments"
  ],
  "title": "Enhancing Offline Reinforcement Learning with Curriculum Learning-Based Trajectory Valuation.pdf",
  "abstract": "The success of deep reinforcement learning (DRL) relies on the availability and quality of training data, often requiring extensive interactions with specific environments. In many real-world scenar- ios, where data collection is costly and risky, offline reinforcement learning (RL) offers a solution by utilizing data collected by do- main experts and searching for a batch-constrained optimal policy. This approach is further augmented by incorporating external data sources, expanding the range and diversity of data collection possi- bilities. However, existing offline RL methods often struggle with challenges posed by non-matching data from these external sources. In this work, we specifically address the problem of source-target domain mismatch in scenarios involving mixed datasets, character- ized by a predominance of source data generated from random or suboptimal policies and a limited amount of target data generated from higher-quality policies. To tackle this problem, we introduce Transition Scoring (TS), a novel method that assigns scores to tran- sitions based on their similarity to the target domain, and propose Curriculum Learning-Based Trajectory Valuation (CLTV), which ef- fectively leverages these transition scores to identify and prioritize high-quality trajectories through a curriculum learning approach. Our extensive experiments across various offline RL methods and MuJoCo environments, complemented by rigorous theoretical anal- ysis, demonstrate that CLTV enhances the overall performance and transferability of policies learned by offline RL algorithms. KEYWORDS Offline Reinforcement Learning, Trajectory Valuation 1 INTRODUCTION Offline Reinforcement Learning (RL) is a class of RL methods that requires the agent to learn from a dataset of pre-collected experi- ences without further environment interaction [26]. This learning paradigm decouples exploration from exploitation, rendering it particularly advantageous in scenarios where the process of data collection is costly, time-consuming, or risky [11, 19]. By utilizing pre-collected datasets, offline RL can bypass the technical challenges that are associated with online data collection, and has potential benefits for a number of real environments, such as human-robot collaboration and autonomous systems [2, 44]. However, this task is challenging, as offline RL methods suffer from the extrapolation error [11, 24]. This issue arises when offline deep RL methods are trained under one distribution but evaluated on a different one. More specifically, value functions implemented by a function approximator have a tendency to predict unrealistic values for unseen state-action pairs for standard off-policy deep RL algorithms such as BCQ [11], TD3+BC [9], CQL [25] and IQL [23]. This highlights the necessity for approaches that restrict the action space, forcing the agent to learn a behavior that is closely aligned with on-policy with respect to a subset of the source data [11]. In recent years, there have been a number of efforts within the paradigm of supervised learning for overcoming the source-target domain mismatch problem valuating data, including data Shap- ley [13] and data valuation using reinforcement learning (DVRL) [51]. Such methods have shown promising results on several application scenarios such as robust learning and domain adaptation [51]. Despite the success of such methods in the supervised learning setting, adapting them to the offline reinforcement learning (RL) setting presents several challenges. One major issue is the non-i.i.d. nature of the data. In supervised learning, data samples are typically assumed to be independent and identically distributed (i.i.d.), but this assumption is violated in offline RL since the data is generated by an agent interacting with an environment [27]. The presence of correlated and non-i.i.d. data samples complicates the valuation of these transitions and hinders generalization to the target domain. Additionally, the data distribution in offline RL often changes due to the agent\u2019s evolving policy or the environment\u2019s dynamics, leading to distributional shifts that aggravate the valuation of transitions, as their value may fluctuate with changing dynamics [24, 25]. Furthermore, unlike supervised learning, where the objective is to optimize a loss function given input-output pairs, the objective in RL is to maximize cumulative rewards by learning a policy that maps states to actions [43]. Therefore, designing a reward function that accounts for distributional similarities between transition items from different domains is essential for effective policy learning. These challenges highlight the complexities involved in adapting supervised learning methods to the offline RL setting and under- score the need for novel approaches to handle these challenges. However, a recent work [3] introduced CUORL, a curriculum learning-based approach aimed at enhancing the performance of offline RL methods by strategically selecting valuable transition items. The limitation of CUORL is that only the current policy is arXiv:2502.00601v1 [cs.LG] 2 Feb 2025"
}