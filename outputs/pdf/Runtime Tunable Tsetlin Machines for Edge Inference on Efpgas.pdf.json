{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Edge Inference",
    "Machine Learning",
    "Tsetlin Machine"
  ],
  "datasets": [
    "MNIST",
    "CIFAR 2",
    "Google Speech Commands",
    "EMG",
    "Gesture Phase",
    "Human Activity",
    "Sensorless Drives",
    "Gas Sensor Array Drift"
  ],
  "methods": [
    "Tsetlin Machine",
    "Bitwise Computation",
    "Model Compression",
    "Runtime Tunability"
  ],
  "results": [
    "2.5x fewer LUTs and 3.38x fewer registers than the current most resource-frugal design",
    "129x energy reduction compared with low-power microcontrollers"
  ],
  "title": "Runtime Tunable Tsetlin Machines for Edge Inference on Efpgas.pdf",
  "abstract": "Embedded Field-Programmable Gate Arrays (eFPGAs) allow for the design of hardware accelerators of edge Machine Learning (ML) applications at a lower power budget compared with traditional FPGA platforms. However, the limited eFPGA logic and memory significantly constrain compute capabilities and model size. As such, ML application deployment on eFPGAs is in direct contrast with the most recent FPGA approaches developing architecture-specific implementations and maximizing throughput over resource frugal- ity. This paper focuses on the opposite side of this trade-off: the proposed eFPGA accelerator focuses on minimizing resource usage and allowing flexibility for on-field recalibration over throughput. This allows for runtime changes in model size, architecture, and input data dimensionality without offline resynthesis. This is made possible through the use of a bitwise compressed inference archi- tecture of the Tsetlin Machine (TM) algorithm. TM compute does not require any multiplication operations, being limited to only bitwise AND, OR, NOT, summations and additions. Additionally, TM model compression allows the entire model to fit within the on- chip block RAM of the eFPGA. The paper uses this accelerator to propose a strategy for runtime model tuning in the field. The pro- posed approach uses 2.5x fewer Look-up-Tables (LUTs) and 3.38x fewer registers than the current most resource-fugal design and achieves up to 129x energy reduction compared with low-power microcontrollers running the same ML application. KEYWORDS Tsetlin Machine, System-on-Chip, Embedded FPGA, Inference Ac- celerator, Machine Learning, Edge inference ACM Reference Format: Tousif Rahman, Gang Mao, Bob Pattison, Sidharth Maheshwari, Marcos Sartori, Adrian Wheeldon, Rishad Shafik, Alex Yakovlev. 2025. Runtime Tunable Tsetlin Machines for Edge Inference on eFPGAs. In Proceedings of tinyML Research Symposium. ACM, New York, NY, USA, 7 pages. 1 INTRODUCTION Deploying Machine Learning (ML) applications on the edge in- volves finding an adequate trade-off between the performance of the ML model, in terms of accuracy and the compute and memory constraints of the edge device [20, 25]. The trade-off is made more challenging when considering the compute cost of floating-point multiply-accumulate (MAC) arithmetic and 32-bit model sizes of Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). tinyML Research Symposium, April 2025, Austin, TX \u00a9 2025 Copyright held by the owner/author(s). Figure 1: Comparing the proposed design (3480 LUTs configuration) to state-of-the-art accelerator automation flows targeting FPGAs. All accelerators were designed for MNIST. Each vertical line indicates the max LUTs of an off-the-shelf eFPGA platform. This work uses 2.5x fewer LUTs than the next closest work (MATADOR). Figure 2: Core components of the Tsetlin Machine: Input conversion to Boolean literals, the Tsetlin Automata (TA) and Clause compute. traditional Deep Neural Networks (DNNs). Fortunately, these chal- lenges can be somewhat alleviated by quantizing DNN weights and data, often to a single bit producing Binary Neural Networks (BNNs) [2, 5, 6, 17, 28]. This means that the MAC is simplified to XNOR and popcount operations and weights in memory now use 1-bit each. This approach is particularly effective when trans- lated to accelerators targeting Field Programmable Gate Arrays (FPGAs). The most recent works follow one of two strategies: ef- ficiently mapping quantized computation to the FPGA\u2019s Look-up- Tables (LUTs) by converting the memory elements into custom compute [2, 18, 23, 26]. Or, they develop architecture-specific ap- proaches for the ML model that are centered around a parameterized compute engine [5, 17, 22, 28]. Often both types of approach are also wrapped in design automation flows. Fig. 1 shows recent state-of-the-art automation flows that can generate accelerators for FPGAs, including this work. The figure shows that the chosen trade-off in current FPGA accelerator work is maximizing inference throughput through custom designs at the expense of LUTs. In doing so, most works are unable to deploy their accelerators onto smaller, cheaper, and more power efficient embed- ded FPGA (eFPGA) platforms, even for a simple MNIST [7] applica- tion (70K LUTs for PolyLUT [2] and 260K LUTs using hls4ml [17]). This paper focuses on the alternative side of this trade-off: mini- mizing resource utilization as much as possible to target smaller arXiv:2502.07823v1 [cs.AR] 10 Feb 2025"
}