{
  "code_links": "None",
  "tasks": [
    "Large Language Models (LLMs)"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Benchmarking"
  ],
  "results": [
    "LLMs demonstrate new and impressive performance on a wide range of language, knowledge, and reasoning benchmarks",
    "LLMs do not have robust competence in many language and reasoning tasks",
    "LLMs often fail to learn representations which facilitate generalisable inferences"
  ],
  "title": "Line Goes Up Inherent Limitations of Benchmarks for Evaluating Large Language Models.pdf",
  "abstract": "Large language models (LLMs) regularly demonstrate new and impressive performance on a wide range of language, knowledge, and reasoning benchmarks. Such rapid progress has led many com- mentators to argue that LLM general cognitive capabilities have likewise rapidly improved, with the implication that such models are becoming progressively more capable on various real-world tasks. Here I summarise theoretical and empirical considerations to challenge this narrative. I argue that inherent limitations with the benchmarking paradigm, along with speci\ufb01c limitations of existing benchmarks, render benchmark performance highly unsuitable as a metric for generalisable compe- tence over cognitive tasks. I also contend that alternative methods for assessing LLM capabilities, including adversarial stimuli and interpretability techniques, have shown that LLMs do not have ro- bust competence in many language and reasoning tasks, and often fail to learn representations which facilitate generalisable inferences. I conclude that benchmark performance should not be used as a reliable indicator of general LLM cognitive capabilities. Keywords Benchmarking \u00b7 Large language models \u00b7 Arti\ufb01cial intelligence \u00b7 Interpretability 1 Introduction The recent development of large language models (LLMs)1 based on the transformer architecture has led to extensive discussion as to how to best measure their capabilities. The most common way to assess LLMs is by their performance on standardised tests called benchmarks. It is often argued that recent substantial improvements in LLM benchmark scores, largely driven by state-of-the-art systems like GPT-4 and OpenAI\u2019s o3, are indicative of large increases in the capability of LLMs. On this basis, it is often inferred that such models are becoming signi\ufb01cantly more capable, potentially outstripping human-level performance, on a wide range of real-world tasks. In this article I contend that this argument is \ufb02awed in two key respects. First, I argue that inherent limitations with the benchmarking paradigm, along with speci\ufb01c limitations of existing benchmarks, combine to render benchmark performance highly unsuitable as a metric for generalisable competence across a range of tasks. Second, I argue that an alternative set of methods more suited for investigating the robustness and generalisability of LLM capabilities, namely adversarial stimuli and interpretability techniques, has yielded strong evidence that LLMs do not have robust competence in many language and reasoning tasks, and often fail to learn representations which facilitate generalisable inferences. 1A note on terminology. In this article I use the term \u2018large language models\u2019 (LLMs) to refer to models based on the transformer architecture trained on large corpus of text data to interact with the user via natural language. Examples of such models include the Llama series, the GPT series, and the Gemini series. Although these models are capable of tasks beyond language processing, such as mathematical and coding tasks, this is still performed by next token prediction after training on linear strings of text. I avoid the term \u2018arti\ufb01cial intelligence\u2019, as it has no accepted clear meaning and does not refer to any speci\ufb01c model architecture."
}