{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Visual Navigation",
    "Feature Tracking",
    "Safety-Critical Control"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Quadratic Programming",
    "Safety Filters",
    "Visibility Maintenance"
  ],
  "results": [
    "None"
  ],
  "title": "Enhancing Feature Tracking Reliability for Visual Navigation Using Real-Time Safety Filter.pdf",
  "abstract": "\u2014 Vision sensors are extensively used for localizing a robot\u2019s pose, particularly in environments where global localization tools such as GPS or motion capture systems are unavailable. In many visual navigation systems, localization is achieved by detecting and tracking visual features or land- marks, which provide information about the sensor\u2019s relative pose. For reliable feature tracking and accurate pose estimation, it is crucial to maintain visibility of a sufficient number of features. This requirement can sometimes conflict with the robot\u2019s overall task objective. In this paper, we approach it as a constrained control problem. By leveraging the invariance properties of visibility constraints within the robot\u2019s kinematic model, we propose a real-time safety filter based on quadratic programming. This filter takes a reference velocity command as input and produces a modified velocity that minimally deviates from the reference while ensuring the information score from the currently visible features remains above a user- specified threshold. Numerical simulations demonstrate that the proposed safety filter preserves the invariance condition and ensures the visibility of more features than the required minimum. We also validated its real-world performance by in- tegrating it into a visual simultaneous localization and mapping (SLAM) algorithm, where it maintained high estimation quality in challenging environments, outperforming a simple tracking controller. I. INTRODUCTION Vision sensors are widely used for self-localization in mobile robots. Visual Odometry (VO) and Visual Simulta- neous Localization and Mapping (V-SLAM) are extensively researched in both the computer vision and robotics fields. State-of-the-art visual pose estimation algorithms, such as ORB-SLAM [1] and VINS-Mono [2], have proven to be highly effective. While most research has traditionally focused on improv- ing the accuracy and robustness of visual estimation using available image data, recent studies have begun examining the impact of image data quality on the performance of vision-based localization algorithms. The quality of image data is often influenced by the camera\u2019s trajectory. For exam- ple, when a camera follows a trajectory that captures texture- less surfaces (e.g., plain walls), the accuracy of VO may decrease due to the lack of sufficient visual features. This challenge has sparked a growing interest in perception-aware \u2217The first two authors contributed equally to this work. This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (RS-2024-00436984). The work by Inkyu Jang was partially supported by Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (RS-2024-00407121). The authors are with the Department of Aerospace Engineering, Seoul National University, Seoul 08826, South Korea (e-mail: dabin404@snu.ac.kr, janginkyu.larr@gmail.com, swsw0411@snu.ac.kr, cat7945@snu.ac.kr, hjinkim@snu.ac.kr, corresponding author: H. Jin Kim). (a) (b) Fig. 1. The result of experiments with (a) the proposed safety filter and (b) the baseline controller. For each experiment, the robots at three timestamps (A,B,C) are visualized along with their corresponding on-board images, with features visualized in ORB-SLAM2. The detected features are represented with green dots. The proposed safety filter adaptively adjusts the control input to maintain sufficient tracking features, as demonstrated by the camera heading (arrow) and onboard image at timestamp B. In contrast, the baseline controller struggles with texture-poor surfaces, where fewer features are trackable. A detailed explanation and analysis are provided in Section V. planning and control, where the camera trajectory is adjusted to ensure high-quality visual data for reliable localization. This idea has led to several studies that integrate visual estimation considerations into advanced motion planning and control strategies. Belief-space planning and optimal control methods [3] [4], which rely on explicit state estimation and uncertainty mod- eling, are often unsuitable for real-time control in modern V- SLAM systems due to the high computational load of tasks such as bundle adjustment involving hundreds of landmarks. To address this limitation, recent research has explored incor- porating information directly extracted from image inputs, such as feature points, into the planning and control loop. These algorithms typically frame the problem as a multi- objective optimization, where perception requirements be- come an additional objective for the controller. However, this arXiv:2502.01092v1 [cs.RO] 3 Feb 2025"
}