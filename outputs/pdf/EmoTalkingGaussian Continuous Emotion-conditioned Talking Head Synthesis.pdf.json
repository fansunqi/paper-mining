{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Talking Head Synthesis",
    "Emotion-Conditioned Talking Head Synthesis"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "3D Gaussian Splatting",
    "Lip-aligned Emotional Face Generator",
    "Text-to-Speech",
    "SyncNet",
    "Normal Map Loss",
    "Sync Loss"
  ],
  "results": [
    "PSNR (\u2191)",
    "SSIM (\u2191)",
    "LPIPS (\u2193)",
    "LMD (\u2193)",
    "Sync-E(\u2193)/C(\u2191)",
    "AUE-U(\u2193)/L(\u2193)",
    "V-RMSE(\u2193)",
    "A-RMSE(\u2193)",
    "V-SA(\u2191)",
    "A-SA(\u2191)",
    "E-Acc(\u2191)"
  ],
  "title": "EmoTalkingGaussian Continuous Emotion-conditioned Talking Head Synthesis.pdf",
  "abstract": "3D Gaussian splatting-based talking head synthesis has recently gained attention for its ability to render high-fidelity images with real-time inference speed. However, since it is typically trained on only a short video that lacks the diversity in facial emotions, the resultant talking heads struggle to represent a wide range of emotions. To address this issue, we propose a lip-aligned emotional face generator and leverage it to train our EmoTalkingGaussian model. It is able to manipulate fa- cial emotions conditioned on continuous emotion values (i.e., valence and arousal); while retaining synchronization of lip \u2020 This research was conducted when Junuk Cha was an intern at Inria. movements with input audio. Additionally, to achieve the accu- rate lip synchronization for in-the-wild audio, we introduce a self-supervised learning method that leverages a text-to-speech network and a visual-audio synchronization network. We experi- ment our EmoTalkingGaussian on publicly available videos and have obtained better results than state-of-the-arts in terms of image quality (measured in PSNR, SSIM, LPIPS), emotion ex- pression (measured in V-RMSE, A-RMSE, V-SA, A-SA, Emotion Accuracy), and lip synchronization (measured in LMD, Sync-E, Sync-C), respectively. 1 arXiv:2502.00654v1 [cs.CV] 2 Feb 2025"
}