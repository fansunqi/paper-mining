{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Natural Language Processing (NLP)",
    "Differential Privacy (DP)"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "DP-NLP",
    "gradient perturbation",
    "embedding vector perturbation",
    "ensemble model"
  ],
  "results": [
    "None"
  ],
  "paper_id": "65372d51939a5f40823b88e1",
  "title": "Differentially Private Natural Language Models: Recent Advances and\n  Future Directions",
  "abstract": "  Recent developments in deep learning have led to great success in various natural language processing (NLP) tasks. However, these applications may involve data that contain sensitive information. Therefore, how to achieve good performance while also protecting the privacy of sensitive data is a crucial challenge in NLP. To preserve privacy, Differential Privacy (DP), which can prevent reconstruction attacks and protect against potential side knowledge, is becoming a de facto technique for private data analysis. In recent years, NLP in DP models (DP-NLP) has been studied from different perspectives, which deserves a comprehensive review. In this paper, we provide the first systematic review of recent advances in DP deep learning models in NLP. In particular, we first discuss some differences and additional challenges of DP-NLP compared with the standard DP deep learning. Then, we investigate some existing work on DP-NLP and present its recent developments from three aspects: gradient perturbation based methods, embedding vector perturbation based methods, and ensemble model based methods. We also discuss some challenges and future directions. "
}