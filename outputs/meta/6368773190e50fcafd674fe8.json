{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Concentration inequalities for leave-one-out cross validation"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Concentration bounds in a general framework",
    "Random variables with distribution satisfying the logarithmic Sobolev inequality"
  ],
  "results": [
    "Estimator stability shows leave-one-out cross validation is a sound procedure",
    "Results illustrated by examples including linear regression, kernel density estimation, and stabilized/truncated estimators"
  ],
  "paper_id": "6368773190e50fcafd674fe8",
  "title": "Concentration inequalities for leave-one-out cross validation",
  "abstract": "  In this article we prove that estimator stability is enough to show that leave-one-out cross validation is a sound procedure, by providing concentration bounds in a general framework. In particular, we provide concentration bounds beyond Lipschitz continuity assumptions on the loss or on the estimator. In order to obtain our results, we rely on random variables with distribution satisfying the logarithmic Sobolev inequality, providing us a relatively rich class of distributions. We illustrate our method by considering several interesting examples, including linear regression, kernel density estimation, and stabilized / truncated estimators such as stabilized kernel regression. "
}