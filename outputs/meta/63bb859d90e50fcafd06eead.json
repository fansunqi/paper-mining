{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Reinforcement Learning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "No-regret reduction",
    "Two-player game",
    "Linear Markov decision processes"
  ],
  "results": [
    "Sublinear regret",
    "Reset-free learning"
  ],
  "paper_id": "63bb859d90e50fcafd06eead",
  "title": "Provable Reset-free Reinforcement Learning by No-Regret Reduction",
  "abstract": "  Real-world reinforcement learning (RL) is often severely limited since typical RL algorithms heavily rely on the reset mechanism to sample proper initial states. In practice, the reset mechanism is expensive to implement due to the need for human intervention or heavily engineered environments. To make learning more practical, we propose a generic no-regret reduction to systematically design reset-free RL algorithms. Our reduction turns reset-free RL into a two-player game. We show that achieving sublinear regret in this two-player game would imply learning a policy that has both sublinear performance regret and sublinear total number of resets in the original RL problem. This means that the agent eventually learns to perform optimally and avoid resets. By this reduction, we design an instantiation for linear Markov decision processes, which is the first provably correct reset-free RL algorithm to our knowledge. "
}