{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Cross-modal representation learning",
    "Online handwriting recognition"
  ],
  "datasets": [
    "Synthetic data",
    "Handwriting recognition data from sensor-enhanced pens"
  ],
  "methods": [
    "Triplet loss for cross-modal representation learning between image and time-series modalities (CMR-IS)"
  ],
  "results": [
    "Improved classification accuracy",
    "Faster convergence",
    "Better generalizability"
  ],
  "paper_id": "620dbcfa5aee126c0f5db36f",
  "title": "Auxiliary Cross-Modal Representation Learning with Triplet Loss\n  Functions for Online Handwriting Recognition",
  "abstract": "  Cross-modal representation learning learns a shared embedding between two or more modalities to improve performance in a given task compared to using only one of the modalities. Cross-modal representation learning from different data types -- such as images and time-series data (e.g., audio or text data) -- requires a deep metric learning loss that minimizes the distance between the modality embeddings. In this paper, we propose to use the triplet loss, which uses positive and negative identities to create sample pairs with different labels, for cross-modal representation learning between image and time-series modalities (CMR-IS). By adapting the triplet loss for cross-modal representation learning, higher accuracy in the main (time-series classification) task can be achieved by exploiting additional information of the auxiliary (image classification) task. Our experiments on synthetic data and handwriting recognition data from sensor-enhanced pens show improved classification accuracy, faster convergence, and better generalizability. "
}