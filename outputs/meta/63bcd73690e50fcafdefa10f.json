{
  "code_links": [
    "None"
  ],
  "tasks": [
    "On-demand Federated Learning over Heterogeneous Edge Devices"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "AnycostFL",
    "Model Shrinking",
    "Gradient Compression",
    "Enhanced Parameter Aggregation",
    "Optimization Design for Minimizing Global Training Loss"
  ],
  "results": [
    "Reduced training latency and energy consumption by up to 1.9 times compared to state-of-the-art efficient FL algorithms",
    "Significant improvement in converged global accuracy"
  ],
  "paper_id": "63bcd73690e50fcafdefa10f",
  "title": "AnycostFL: Efficient On-Demand Federated Learning over Heterogeneous\n  Edge Devices",
  "abstract": "  In this work, we investigate the challenging problem of on-demand federated learning (FL) over heterogeneous edge devices with diverse resource constraints. We propose a cost-adjustable FL framework, named AnycostFL, that enables diverse edge devices to efficiently perform local updates under a wide range of efficiency constraints. To this end, we design the model shrinking to support local model training with elastic computation cost, and the gradient compression to allow parameter transmission with dynamic communication overhead. An enhanced parameter aggregation is conducted in an element-wise manner to improve the model performance. Focusing on AnycostFL, we further propose an optimization design to minimize the global training loss with personalized latency and energy constraints. By revealing the theoretical insights of the convergence analysis, personalized training strategies are deduced for different devices to match their locally available resources. Experiment results indicate that, when compared to the state-of-the-art efficient FL algorithms, our learning framework can reduce up to 1.9 times of the training latency and energy consumption for realizing a reasonable global testing accuracy. Moreover, the results also demonstrate that, our approach significantly improves the converged global accuracy. "
}