{
  "code_links": [
    "https://seanchenxy.github.io/HandAvatarWeb"
  ],
  "tasks": [
    "Hand animation and rendering from monocular video"
  ],
  "datasets": [
    "Monocular video data"
  ],
  "methods": [
    "MANO-HD model",
    "self-occlusion-aware shading field (SelF)",
    "directed soft occupancy"
  ],
  "results": [
    "Free-pose hand animation and rendering",
    "Superior appearance fidelity",
    "Hand appearance editing"
  ],
  "paper_id": "637ee0ee90e50fcafd0f6e7a",
  "title": "Hand Avatar: Free-Pose Hand Animation and Rendering from Monocular Video",
  "abstract": "  We present HandAvatar, a novel representation for hand animation and rendering, which can generate smoothly compositional geometry and self-occlusion-aware texture. Specifically, we first develop a MANO-HD model as a high-resolution mesh topology to fit personalized hand shapes. Sequentially, we decompose hand geometry into per-bone rigid parts, and then re-compose paired geometry encodings to derive an across-part consistent occupancy field. As for texture modeling, we propose a self-occlusion-aware shading field (SelF). In SelF, drivable anchors are paved on the MANO-HD surface to record albedo information under a wide variety of hand poses. Moreover, directed soft occupancy is designed to describe the ray-to-surface relation, which is leveraged to generate an illumination field for the disentanglement of pose-independent albedo and pose-dependent illumination. Trained from monocular video data, our HandAvatar can perform free-pose hand animation and rendering while at the same time achieving superior appearance fidelity. We also demonstrate that HandAvatar provides a route for hand appearance editing. Project website: https://seanchenxy.github.io/HandAvatarWeb. "
}