{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Few-shot learning",
    "Recurrent neural nets"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Reduced-entropy algorithm",
    "Sparse coding solver approximation"
  ],
  "results": [
    "Significant potential for improving learning models' sample efficiency",
    "Generalization",
    "Time complexity improvement"
  ],
  "paper_id": "63350ce890e50fcafd351213",
  "title": "Less is More: Rethinking Few-Shot Learning and Recurrent Neural Nets",
  "abstract": "  The statistical supervised learning framework assumes an input-output set with a joint probability distribution that is reliably represented by the training dataset. The learner is then required to output a prediction rule learned from the training dataset's input-output pairs. In this work, we provide meaningful insights into the asymptotic equipartition property (AEP) \\citep{Shannon:1948} in the context of machine learning, and illuminate some of its potential ramifications for few-shot learning. We provide theoretical guarantees for reliable learning under the information-theoretic AEP, and for the generalization error with respect to the sample size. We then focus on a highly efficient recurrent neural net (RNN) framework and propose a reduced-entropy algorithm for few-shot learning. We also propose a mathematical intuition for the RNN as an approximation of a sparse coding solver. We verify the applicability, robustness, and computational efficiency of the proposed approach with image deblurring and optical coherence tomography (OCT) speckle suppression. Our experimental results demonstrate significant potential for improving learning models' sample efficiency, generalization, and time complexity, that can therefore be leveraged for practical real-time applications. "
}