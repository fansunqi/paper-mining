{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Continual Learning of Multi-modal Dynamics"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Neural episodic memory with Dirichlet Process prior on attention weights",
    "Knowledge transfer across tasks using mode descriptors"
  ],
  "results": [
    "Performance compares favorably to mainstream parameter transfer approach"
  ],
  "paper_id": "622032395aee126c0fe2f73c",
  "title": "Continual Learning of Multi-modal Dynamics with External Memory",
  "abstract": "  We study the problem of fitting a model to a dynamical environment when new modes of behavior emerge sequentially. The learning model is aware when a new mode appears, but it does not have access to the true modes of individual training sequences. The state-of-the-art continual learning approaches cannot handle this setup, because parameter transfer suffers from catastrophic interference and episodic memory design requires the knowledge of the ground-truth modes of sequences. We devise a novel continual learning method that overcomes both limitations by maintaining a descriptor of the mode of an encountered sequence in a neural episodic memory. We employ a Dirichlet Process prior on the attention weights of the memory to foster efficient storage of the mode descriptors. Our method performs continual learning by transferring knowledge across tasks by retrieving the descriptors of similar modes of past tasks to the mode of a current sequence and feeding this descriptor into its transition kernel as control input. We observe the continual learning performance of our method to compare favorably to the mainstream parameter transfer approach. "
}