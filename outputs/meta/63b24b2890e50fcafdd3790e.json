{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Bayesian inference"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Deep linear networks",
    "Gaussian weight priors",
    "MSE negative log-likelihood loss",
    "Meijer-G functions",
    "Asymptotic expansions"
  ],
  "results": [
    "Predictive posterior and Bayesian model evidence in closed form",
    "Deep linear networks make provably optimal predictions",
    "Bayesian model evidence maximized at infinite depth in wide networks",
    "Effective depth determines Bayesian posterior in wide linear networks"
  ],
  "paper_id": "63b24b2890e50fcafdd3790e",
  "title": "Bayesian Interpolation with Deep Linear Networks",
  "abstract": "  This article concerns Bayesian inference using deep linear networks with output dimension one. In the interpolating (zero noise) regime we show that with Gaussian weight priors and MSE negative log-likelihood loss both the predictive posterior and the Bayesian model evidence can be written in closed form in terms of a class of meromorphic special functions called Meijer-G functions. These results are non-asymptotic and hold for any training dataset, network depth, and hidden layer widths, giving exact solutions to Bayesian interpolation using a deep Gaussian process with a Euclidean covariance at each layer. Through novel asymptotic expansions of Meijer-G functions, a rich new picture of the role of depth emerges. Specifically, we find that the posteriors in deep linear networks with data-independent priors are the same as in shallow networks with evidence maximizing data-dependent priors. In this sense, deep linear networks make provably optimal predictions. We also prove that, starting from data-agnostic priors, Bayesian model evidence in wide networks is only maximized at infinite depth. This gives a principled reason to prefer deeper networks (at least in the linear case). Finally, our results show that with data-agnostic priors a novel notion of effective depth given by \\[\\#\\text{hidden layers}\\times\\frac{\\#\\text{training data}}{\\text{network width}}\\] determines the Bayesian posterior in wide linear networks, giving rigorous new scaling laws for generalization error. "
}