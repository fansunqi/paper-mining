{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Unsupervised Depth Estimation from Unstabilized Photography"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "plane plus depth model",
    "neural RGB-D representation",
    "test-time optimization approach",
    "coarse-to-fine refinement"
  ],
  "results": [
    "geometrically accurate depth reconstructions",
    "no additional hardware or separate data pre-processing and pose-estimation steps"
  ],
  "paper_id": "63a910a290e50fcafd2a8941",
  "title": "Shakes on a Plane: Unsupervised Depth Estimation from Unstabilized\n  Photography",
  "abstract": "  Modern mobile burst photography pipelines capture and merge a short sequence of frames to recover an enhanced image, but often disregard the 3D nature of the scene they capture, treating pixel motion between images as a 2D aggregation problem. We show that in a ''long-burst'', forty-two 12-megapixel RAW frames captured in a two-second sequence, there is enough parallax information from natural hand tremor alone to recover high-quality scene depth. To this end, we devise a test-time optimization approach that fits a neural RGB-D representation to long-burst data and simultaneously estimates scene depth and camera motion. Our plane plus depth model is trained end-to-end, and performs coarse-to-fine refinement by controlling which multi-resolution volume features the network has access to at what time during training. We validate the method experimentally, and demonstrate geometrically accurate depth reconstructions with no additional hardware or separate data pre-processing and pose-estimation steps. "
}