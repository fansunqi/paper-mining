{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Meta-learning",
    "Neural network training",
    "Synaptic plasticity"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Meta-learning approach",
    "Random feedback alignment"
  ],
  "results": [
    "Improved online training of deep models in the low data regime",
    "Effective, interpretable learning rules satisfying biological constraints"
  ],
  "paper_id": "63608e4f90e50fcafdee0fd7",
  "title": "Meta-Learning Biologically Plausible Plasticity Rules with Random\n  Feedback Pathways",
  "abstract": "  Backpropagation is widely used to train artificial neural networks, but its relationship to synaptic plasticity in the brain is unknown. Some biological models of backpropagation rely on feedback projections that are symmetric with feedforward connections, but experiments do not corroborate the existence of such symmetric backward connectivity. Random feedback alignment offers an alternative model in which errors are propagated backward through fixed, random backward connections. This approach successfully trains shallow models, but learns slowly and does not perform well with deeper models or online learning. In this study, we develop a meta-learning approach to discover interpretable, biologically plausible plasticity rules that improve online learning performance with fixed random feedback connections. The resulting plasticity rules show improved online training of deep models in the low data regime. Our results highlight the potential of meta-learning to discover effective, interpretable learning rules satisfying biological constraints. "
}