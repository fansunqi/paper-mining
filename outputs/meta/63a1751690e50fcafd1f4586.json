{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Risk-Sensitive Reinforcement Learning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Policy Gradient algorithms",
    "Monte Carlo Policy Gradient algorithm",
    "online (temporal-difference) Actor-Critic algorithm"
  ],
  "results": [
    "None"
  ],
  "paper_id": "63a1751690e50fcafd1f4586",
  "title": "Risk-Sensitive Reinforcement Learning with Exponential Criteria",
  "abstract": "  While risk-neutral reinforcement learning has shown experimental success in a number of applications, it is well-known to be non-robust with respect to noise and perturbations in the parameters of the system. For this reason, risk-sensitive reinforcement learning algorithms have been studied to introduce robustness and sample efficiency, and lead to better real-life performance. In this work, we introduce new model-free risk-sensitive reinforcement learning algorithms as variations of widely-used Policy Gradient algorithms with similar implementation properties. In particular, we study the effect of exponential criteria on the risk-sensitivity of the policy of a reinforcement learning agent, and develop variants of the Monte Carlo Policy Gradient algorithm and the online (temporal-difference) Actor-Critic algorithm. Analytical results showcase that the use of exponential criteria generalize commonly used ad-hoc regularization approaches. The implementation, performance, and robustness properties of the proposed methods are evaluated in simulated experiments. "
}