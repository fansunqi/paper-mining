{
  "code_links": [
    "https://github.com/emadeldeen24/CA-TCC"
  ],
  "tasks": [
    "Semi-supervised Time-Series Classification"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "TS-TCC",
    "Class-Aware TS-TCC (CA-TCC)",
    "Contrastive Learning",
    "Time-series Augmentation"
  ],
  "results": [
    "Comparably with fully supervised training",
    "High efficiency in few labeled data and transfer learning scenarios"
  ],
  "paper_id": "62fb0ae490e50fcafd5f16de",
  "title": "Self-supervised Contrastive Representation Learning for Semi-supervised\n  Time-Series Classification",
  "abstract": "  Learning time-series representations when only unlabeled data or few labeled samples are available can be a challenging task. Recently, contrastive self-supervised learning has shown great improvement in extracting useful representations from unlabeled data via contrasting different augmented views of data. In this work, we propose a novel Time-Series representation learning framework via Temporal and Contextual Contrasting (TS-TCC) that learns representations from unlabeled data with contrastive learning. Specifically, we propose time-series specific weak and strong augmentations and use their views to learn robust temporal relations in the proposed temporal contrasting module, besides learning discriminative representations by our proposed contextual contrasting module. Additionally, we conduct a systematic study of time-series data augmentation selection, which is a key part of contrastive learning. We also extend TS-TCC to the semi-supervised learning settings and propose a Class-Aware TS-TCC (CA-TCC) that benefits from the available few labeled data to further improve representations learned by TS-TCC. Specifically, we leverage robust pseudo labels produced by TS-TCC to realize class-aware contrastive loss. Extensive experiments show that the linear evaluation of the features learned by our proposed framework performs comparably with the fully supervised training. Additionally, our framework shows high efficiency in few labeled data and transfer learning scenarios. The code is publicly available at \\url{https://github.com/emadeldeen24/CA-TCC}. "
}