{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Convergence and generalization of one-hidden-layer neural networks"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "One-hidden-layer neural network training with Gaussian mixture model input",
    "Minimizing non-convex risk function",
    "Sample complexity analysis"
  ],
  "results": [
    "Linear convergence to a critical point",
    "Guaranteed generalization error",
    "Impact of input distributions on sample complexity and learning rate characterized"
  ],
  "paper_id": "62cb94ae5aee126c0ff8149b",
  "title": "Learning and generalization of one-hidden-layer neural networks, going\n  beyond standard Gaussian data",
  "abstract": "  This paper analyzes the convergence and generalization of training a one-hidden-layer neural network when the input features follow the Gaussian mixture model consisting of a finite number of Gaussian distributions. Assuming the labels are generated from a teacher model with an unknown ground truth weight, the learning problem is to estimate the underlying teacher model by minimizing a non-convex risk function over a student neural network. With a finite number of training samples, referred to the sample complexity, the iterations are proved to converge linearly to a critical point with guaranteed generalization error. In addition, for the first time, this paper characterizes the impact of the input distributions on the sample complexity and the learning rate. "
}