{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Comparing language models and humans",
    "Processing recursively nested grammatical structures"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Comparative psychology",
    "Large language models"
  ],
  "results": [
    "Large LMs outperform humans",
    "Humans may not perform above chance at difficult structures"
  ],
  "paper_id": "635b486f90e50fcafd330c56",
  "title": "Can language models handle recursively nested grammatical structures? A\n  case study on comparing models and humans",
  "abstract": "  How should we compare the capabilities of language models (LMs) and humans? I draw inspiration from comparative psychology to highlight some challenges. In particular, I consider a case study: processing of recursively nested grammatical structures. Prior work suggests that LMs cannot handle these structures as reliably as humans can. However, the humans were provided with instructions and training, while the LMs were evaluated zero-shot. I therefore match the evaluation more closely. Providing large LMs with a simple prompt -- substantially less content than the human training -- allows the LMs to consistently outperform the human results, and even to extrapolate to more deeply nested conditions than were tested with humans. Further, reanalyzing the prior human data suggests that the humans may not perform above chance at the difficult structures initially. Thus, large LMs may indeed process recursively nested grammatical structures as reliably as humans. This case study highlights how discrepancies in the evaluation can confound comparisons of language models and humans. I therefore reflect on the broader challenge of comparing human and model capabilities, and highlight an important difference between evaluating cognitive models and foundation models. "
}