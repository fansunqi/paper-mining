{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Bayesian neural networks"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Partially stochastic networks"
  ],
  "results": [
    "No systematic benefit of full stochasticity",
    "Partially stochastic networks can match and sometimes outperform fully stochastic networks"
  ],
  "paper_id": "6371b1a790e50fcafdb2e83c",
  "title": "Do Bayesian Neural Networks Need To Be Fully Stochastic?",
  "abstract": "  We investigate the benefit of treating all the parameters in a Bayesian neural network stochastically and find compelling theoretical and empirical evidence that this standard construction may be unnecessary. To this end, we prove that expressive predictive distributions require only small amounts of stochasticity. In particular, partially stochastic networks with only $n$ stochastic biases are universal probabilistic predictors for $n$-dimensional predictive problems. In empirical investigations, we find no systematic benefit of full stochasticity across four different inference modalities and eight datasets; partially stochastic networks can match and sometimes even outperform fully stochastic networks, despite their reduced memory costs. "
}