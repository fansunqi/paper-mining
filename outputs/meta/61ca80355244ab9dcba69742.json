{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Federated Learning on Time-Evolving Heterogeneous Data"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Continual Federated Learning (CFL)"
  ],
  "results": [
    "CFL has a faster convergence rate than FedAvg in time-evolving scenarios",
    "CFL methods significantly outperform other state-of-the-art FL baselines"
  ],
  "paper_id": "61ca80355244ab9dcba69742",
  "title": "Towards Federated Learning on Time-Evolving Heterogeneous Data",
  "abstract": "  Federated Learning (FL) is a learning paradigm that protects privacy by keeping client data on edge devices. However, optimizing FL in practice can be difficult due to the diversity and heterogeneity of the learning system. Despite recent research efforts to improve the optimization of heterogeneous data, the impact of time-evolving heterogeneous data in real-world scenarios, such as changing client data or intermittent clients joining or leaving during training, has not been studied well. In this work, we propose Continual Federated Learning (CFL), a flexible framework for capturing the time-evolving heterogeneity of FL. CFL can handle complex and realistic scenarios, which are difficult to evaluate in previous FL formulations, by extracting information from past local data sets and approximating local objective functions. We theoretically demonstrate that CFL methods have a faster convergence rate than FedAvg in time-evolving scenarios, with the benefit depending on approximation quality. Through experiments, we show that our numerical findings match the convergence analysis and that CFL methods significantly outperform other state-of-the-art FL baselines. "
}