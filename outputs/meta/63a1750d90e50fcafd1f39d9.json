{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Asymptotic Analysis of Deep Residual Networks"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Deep Residual Networks (ResNets)",
    "ODEs (Ordinary Differential Equations)",
    "SDEs (Stochastic Differential Equations)"
  ],
  "results": [
    "Scaling regimes for trained weights",
    "Convergence of hidden state dynamics",
    "Diffusive regime with SDEs",
    "Scaling limits for backpropagation dynamics"
  ],
  "paper_id": "63a1750d90e50fcafd1f39d9",
  "title": "Asymptotic Analysis of Deep Residual Networks",
  "abstract": "  We investigate the asymptotic properties of deep Residual networks (ResNets) as the number of layers increases. We first show the existence of scaling regimes for trained weights markedly different from those implicitly assumed in the neural ODE literature. We study the convergence of the hidden state dynamics in these scaling regimes, showing that one may obtain an ODE, a stochastic differential equation (SDE) or neither of these. In particular, our findings point to the existence of a diffusive regime in which the deep network limit is described by a class of stochastic differential equations (SDEs). Finally, we derive the corresponding scaling limits for the backpropagation dynamics. "
}