{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Deep Learning Misconduct"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Pure-Guess Nearest Neighbor (PGNN)"
  ],
  "results": [
    "Establishes a theorem on the misconduct",
    "Misconduct violates transparency and cross-validation protocols",
    "Misconduct is fatal to generalizability of Deep Learning"
  ],
  "paper_id": "6386c9e790e50fcafdfa1154",
  "title": "On \"Deep Learning\" Misconduct",
  "abstract": "  This is a theoretical paper, as a companion paper of the plenary talk for the same conference ISAIC 2022. In contrast to the author's plenary talk in the same conference, conscious learning (Weng, 2022b; Weng, 2022c) which develops a single network for a life (many tasks), \"Deep Learning\" trains multiple networks for each task. Although \"Deep Learning\" may use different learning modes, including supervised, reinforcement and adversarial modes, almost all \"Deep Learning\" projects apparently suffer from the same misconduct, called \"data deletion\" and \"test on training data\". This paper establishes a theorem that a simple method called Pure-Guess Nearest Neighbor (PGNN) reaches any required errors on validation data set and test data set, including zero-error requirements, through the same misconduct, as long as the test data set is in the possession of the authors and both the amount of storage space and the time of training are finite but unbounded. The misconduct violates well-known protocols called transparency and cross-validation. The nature of the misconduct is fatal, because in the absence of any disjoint test, \"Deep Learning\" is clearly not generalizable. "
}