{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Lossy compression",
    "Bounding generalization error"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Super-modular $f$-divergences",
    "Mutual $f$-information",
    "Generalized rate-distortion function"
  ],
  "results": [
    "Improved Sanov's bound",
    "Better bounds on achievable rates",
    "Improved bounds on generalization error"
  ],
  "paper_id": "62b3da1f5aee126c0fb1b7f7",
  "title": "f-divergences and their applications in lossy compression and bounding\n  generalization error",
  "abstract": "  In this paper, we provide three applications for $f$-divergences: (i) we introduce Sanov's upper bound on the tail probability of the sum of independent random variables based on super-modular $f$-divergence and show that our generalized Sanov's bound strictly improves over ordinary one, (ii) we consider the lossy compression problem which studies the set of achievable rates for a given distortion and code length. We extend the rate-distortion function using mutual $f$-information and provide new and strictly better bounds on achievable rates in the finite blocklength regime using super-modular $f$-divergences, and (iii) we provide a connection between the generalization error of algorithms with bounded input/output mutual $f$-information and a generalized rate-distortion problem. This connection allows us to bound the generalization error of learning algorithms using lower bounds on the $f$-rate-distortion function. Our bound is based on a new lower bound on the rate-distortion function that (for some examples) strictly improves over previously best-known bounds. "
}