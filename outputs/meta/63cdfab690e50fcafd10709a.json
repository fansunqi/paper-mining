{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Contextual word expectation",
    "Situation models investigation"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Computational language models",
    "Word vectors"
  ],
  "results": [
    "Effect of Nieuwland and van Berkum (2006) can be modeled by two language models and two sets of word vectors",
    "Reduced effect in other models"
  ],
  "paper_id": "63cdfab690e50fcafd10709a",
  "title": "Can Peanuts Fall in Love with Distributional Semantics?",
  "abstract": "  The context in which a sentence appears can drastically alter our expectations about upcoming words - for example, following a short story involving an anthropomorphic peanut, experimental participants are more likely to expect the sentence 'the peanut was in love' than 'the peanut was salted', as indexed by N400 amplitude (Nieuwland & van Berkum, 2006). This rapid and dynamic updating of comprehenders' expectations about the kind of events that a peanut may take part in based on context has been explained using the construct of Situation Models - updated mental representations of key elements of an event under discussion, in this case, the peanut protagonist. However, recent work showing that N400 amplitude can be predicted based on distributional information alone raises the question whether situation models are in fact necessary for the kinds of contextual effects observed in previous work. To investigate this question, we attempt to model the results of Nieuwland and van Berkum (2006) using six computational language models and three sets of word vectors, none of which have explicit situation models or semantic grounding. We find that the effect found by Nieuwland and van Berkum (2006) can be fully modeled by two language models and two sets of word vectors, with others showing a reduced effect. Thus, at least some processing effects normally explained through situation models may not in fact require explicit situation models. "
}