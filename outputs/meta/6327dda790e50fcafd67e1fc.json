{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Image classification"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Top-tuning"
  ],
  "results": [
    "Comparable accuracy to fine-tuning",
    "Training time reduced by one to two orders of magnitude"
  ],
  "paper_id": "6327dda790e50fcafd67e1fc",
  "title": "Fine-tuning or top-tuning? Transfer learning with pretrained features\n  and fast kernel methods",
  "abstract": "  The impressive performances of deep learning architectures is associated to massive increase of models complexity. Millions of parameters need be tuned, with training and inference time scaling accordingly. But is massive fine-tuning necessary? In this paper, focusing on image classification, we consider a simple transfer learning approach exploiting pretrained convolutional features as input for a fast kernel method. We refer to this approach as top-tuning, since only the kernel classifier is trained. By performing more than 2500 training processes we show that this top-tuning approach provides comparable accuracy w.r.t. fine-tuning, with a training time that is between one and two orders of magnitude smaller. These results suggest that top-tuning provides a useful alternative to fine-tuning in small/medium datasets, especially when training efficiency is crucial. "
}