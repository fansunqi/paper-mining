{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Existence of fixed points in nonnegative neural networks"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Nonlinear Perron-Frobenius theory",
    "Monotonic and scalable functions",
    "Convex analysis",
    "Loop-unrolling technique"
  ],
  "results": [
    "Weaker conditions for fixed point existence than previous convex analysis methods",
    "Fixed point set shape is an interval, degenerates to a point under mild conditions",
    "Theoretical results verified in numerical simulations"
  ],
  "paper_id": "60de8b9591e0110ac15e4659",
  "title": "Fixed points of nonnegative neural networks",
  "abstract": "  We consider the existence of fixed points of nonnegative neural networks, i.e., neural networks that take as an input and produce as an output nonnegative vectors. We first show that nonnegative neural networks with nonnegative weights and biases can be recognized as monotonic and (weakly) scalable functions within the framework of nonlinear Perron-Frobenius theory. This fact enables us to provide conditions for the existence of fixed points of nonnegative neural networks, and these conditions are weaker than those obtained recently using arguments in convex analysis. Furthermore, we prove that the shape of the fixed point set of nonnegative neural networks with nonnegative weights and biases is an interval, which under mild conditions degenerates to a point. These results are then used to obtain the existence of fixed points of more general types of nonnegative neural networks. The results of this paper contribute to the understanding of the behavior of autoencoders, and they provide insight into neural networks designed using the loop-unrolling technique, which can be seen as a fixed point searching algorithm. The chief theoretical results of this paper are verified in numerical simulations. "
}