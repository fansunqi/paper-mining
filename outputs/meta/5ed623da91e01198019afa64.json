{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Deep Anomaly Detection"
  ],
  "datasets": [
    "ImageNet"
  ],
  "methods": [
    "Classifiers trained to discern between normal samples and a few random natural images"
  ],
  "results": [
    "Classifiers outperform current state of the art in deep AD",
    "Multiscale structure of image data makes example anomalies exceptionally informative"
  ],
  "paper_id": "5ed623da91e01198019afa64",
  "title": "Rethinking Assumptions in Deep Anomaly Detection",
  "abstract": "  Though anomaly detection (AD) can be viewed as a classification problem (nominal vs. anomalous) it is usually treated in an unsupervised manner since one typically does not have access to, or it is infeasible to utilize, a dataset that sufficiently characterizes what it means to be \"anomalous.\" In this paper we present results demonstrating that this intuition surprisingly seems not to extend to deep AD on images. For a recent AD benchmark on ImageNet, classifiers trained to discern between normal samples and just a few (64) random natural images are able to outperform the current state of the art in deep AD. Experimentally we discover that the multiscale structure of image data makes example anomalies exceptionally informative. "
}