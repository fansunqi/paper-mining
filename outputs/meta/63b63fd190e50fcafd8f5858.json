{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Adaptive Sampling Algorithm Performance"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Gittins Index"
  ],
  "results": [
    "Learning: comparable to traditional non-adaptive designs",
    "Earning: substantially better than traditional non-adaptive designs"
  ],
  "paper_id": "63b63fd190e50fcafd8f5858",
  "title": "Computing the Performance of A New Adaptive Sampling Algorithm Based on\n  The Gittins Index in Experiments with Exponential Rewards",
  "abstract": "  Designing experiments often requires balancing between learning about the true treatment effects and earning from allocating more samples to the superior treatment. While optimal algorithms for the Multi-Armed Bandit Problem (MABP) provide allocation policies that optimally balance learning and earning, they tend to be computationally expensive. The Gittins Index (GI) is a solution to the MABP that can simultaneously attain optimality and computationally efficiency goals, and it has been recently used in experiments with Bernoulli and Gaussian rewards. For the first time, we present a modification of the GI rule that can be used in experiments with exponentially-distributed rewards. We report its performance in simulated 2- armed and 3-armed experiments. Compared to traditional non-adaptive designs, our novel GI modified design shows operating characteristics comparable in learning (e.g. statistical power) but substantially better in earning (e.g. direct benefits). This illustrates the potential that designs using a GI approach to allocate participants have to improve participant benefits, increase efficiencies, and reduce experimental costs in adaptive multi-armed experiments with exponential rewards. "
}