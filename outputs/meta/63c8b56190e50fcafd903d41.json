{
  "code_links": [
    "https://github.com/zetaalphavector/inPars/tree/master/tpu"
  ],
  "tasks": [
    "Information Retrieval"
  ],
  "datasets": [
    "BEIR"
  ],
  "methods": [
    "InPars-v2: uses open-source LLMs and rerankers"
  ],
  "results": [
    "New state-of-the-art results on the BEIR benchmark"
  ],
  "paper_id": "63c8b56190e50fcafd903d41",
  "title": "InPars-v2: Large Language Models as Efficient Dataset Generators for\n  Information Retrieval",
  "abstract": "  Recently, InPars introduced a method to efficiently use large language models (LLMs) in information retrieval tasks: via few-shot examples, an LLM is induced to generate relevant queries for documents. These synthetic query-document pairs can then be used to train a retriever. However, InPars and, more recently, Promptagator, rely on proprietary LLMs such as GPT-3 and FLAN to generate such datasets. In this work we introduce InPars-v2, a dataset generator that uses open-source LLMs and existing powerful rerankers to select synthetic query-document pairs for training. A simple BM25 retrieval pipeline followed by a monoT5 reranker finetuned on InPars-v2 data achieves new state-of-the-art results on the BEIR benchmark. To allow researchers to further improve our method, we open source the code, synthetic data, and finetuned models: https://github.com/zetaalphavector/inPars/tree/master/tpu "
}