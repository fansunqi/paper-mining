{
  "code_links": [
    "https://www.github.com/thartvigsen/grace"
  ],
  "tasks": [
    "Lifelong Model Editing"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "General Retrieval Adaptors for Continual Editing (GRACE)"
  ],
  "results": [
    "Improves over recent alternatives",
    "Generalizes to unseen inputs"
  ],
  "paper_id": "637c3dd090e50fcafd77c69b",
  "title": "Aging with GRACE: Lifelong Model Editing with Discrete Key-Value\n  Adaptors",
  "abstract": "  Large pre-trained models decay over long-term deployment as input distributions shift, user requirements change, or crucial knowledge gaps are discovered. Recently, model editors have been proposed to modify a model's behavior by adjusting its weights during deployment. However, when editing the same model multiple times, these approaches quickly decay a model's performance on upstream data and forget how to fix previous errors. We propose and study a novel Lifelong Model Editing setting, where streaming errors are identified for a deployed model and we update the model to correct its predictions without influencing unrelated inputs without access to training edits, exogenous datasets, or any upstream data for the edited model. To approach this problem, we introduce General Retrieval Adaptors for Continual Editing, or GRACE, which learns to cache a chosen layer's activations in an adaptive codebook as edits stream in, leaving original model weights frozen. GRACE can thus edit models thousands of times in a row using only streaming errors, without influencing unrelated inputs. Experimentally, we show that GRACE improves over recent alternatives and generalizes to unseen inputs. Our code is available at https://www.github.com/thartvigsen/grace. "
}