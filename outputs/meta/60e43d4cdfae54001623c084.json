{
  "code_links": [
    "None"
  ],
  "tasks": [
    "glioma classification based on MRI data"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "explanation-guided training",
    "Layer-wise relevance propagation (LRP)"
  ],
  "results": [
    "promising results in using interpretation techniques in model training"
  ],
  "paper_id": "60e43d4cdfae54001623c084",
  "title": "Improving a neural network model by explanation-guided training for\n  glioma classification based on MRI data",
  "abstract": "  In recent years, artificial intelligence (AI) systems have come to the forefront. These systems, mostly based on Deep learning (DL), achieve excellent results in areas such as image processing, natural language processing, or speech recognition. Despite the statistically high accuracy of deep learning models, their output is often a decision of \"black box\". Thus, Interpretability methods have become a popular way to gain insight into the decision-making process of deep learning models. Explanation of a deep learning model is desirable in the medical domain since the experts have to justify their judgments to the patient. In this work, we proposed a method for explanation-guided training that uses a Layer-wise relevance propagation (LRP) technique to force the model to focus only on the relevant part of the image. We experimentally verified our method on a convolutional neural network (CNN) model for low-grade and high-grade glioma classification problems. Our experiments show promising results in a way to use interpretation techniques in the model training process. "
}