{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Deep Reinforcement Learning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Augmenting a single demonstration",
    "Deep Deterministic Policy Gradients (DDPG)",
    "Hindsight Experience Replay (HER)"
  ],
  "results": [
    "Significant training advantage using a single human example",
    "Less than a minute of human input",
    "Agent learns policies significantly different from human demonstration"
  ],
  "paper_id": "6331182b90e50fcafdcbbaa6",
  "title": "Minimizing Human Assistance: Augmenting a Single Demonstration for Deep\n  Reinforcement Learning",
  "abstract": "  The use of human demonstrations in reinforcement learning has proven to significantly improve agent performance. However, any requirement for a human to manually 'teach' the model is somewhat antithetical to the goals of reinforcement learning. This paper attempts to minimize human involvement in the learning process while retaining the performance advantages by using a single human example collected through a simple-to-use virtual reality simulation to assist with RL training. Our method augments a single demonstration to generate numerous human-like demonstrations that, when combined with Deep Deterministic Policy Gradients and Hindsight Experience Replay (DDPG + HER) significantly improve training time on simple tasks and allows the agent to solve a complex task (block stacking) that DDPG + HER alone cannot solve. The model achieves this significant training advantage using a single human example, requiring less than a minute of human input. Moreover, despite learning from a human example, the agent is not constrained to human-level performance, often learning a policy that is significantly different from the human demonstration. "
}