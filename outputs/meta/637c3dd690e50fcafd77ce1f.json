{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Federated Unlearning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Sequential Informed Federated Unlearning (IFU)",
    "randomized perturbation mechanism"
  ],
  "results": [
    "More efficient unlearning procedures compared to basic re-training and state-of-the-art FU approaches"
  ],
  "paper_id": "637c3dd690e50fcafd77ce1f",
  "title": "Sequential Informed Federated Unlearning: Efficient and Provable Client\n  Unlearning in Federated Optimization",
  "abstract": "  The aim of Machine Unlearning (MU) is to provide theoretical guarantees on the removal of the contribution of a given data point from a training procedure. Federated Unlearning (FU) consists in extending MU to unlearn a given client's contribution from a federated training routine. Current FU approaches are generally not scalable, and do not come with sound theoretical quantification of the effectiveness of unlearning. In this work we present Informed Federated Unlearning (IFU), a novel efficient and quantifiable FU approach. Upon unlearning request from a given client, IFU identifies the optimal FL iteration from which FL has to be reinitialized, with unlearning guarantees obtained through a randomized perturbation mechanism. The theory of IFU is also extended to account for sequential unlearning requests. Experimental results on different tasks and dataset show that IFU leads to more efficient unlearning procedures as compared to basic re-training and state-of-the-art FU approaches. "
}