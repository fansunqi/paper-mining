{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Learning functional form of ODEs on complex networks"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Graph neural networks",
    "Inductive biases",
    "Neural network architecture",
    "Learning task"
  ],
  "results": [
    "Generalisation capacity depends on the first moment of initial value data distribution",
    "Learns the non-dissipative nature of dynamics implicitly",
    "Accuracy resolution limit is of order O(1/sqrt(n)) for a system of size n"
  ],
  "paper_id": "63c0cc6490e50fcafd2a8bf7",
  "title": "Universality of neural dynamics on complex networks",
  "abstract": "  This paper discusses the capacity of graph neural networks to learn the functional form of ordinary differential equations that govern dynamics on complex networks. We propose necessary elements for such a problem, namely, inductive biases, a neural network architecture and a learning task. Statistical learning theory suggests that generalisation power of neural networks relies on independence and identical distribution (i.i.d.)\\ of training and testing data. Although this assumption together with an appropriate neural architecture and a learning mechanism is sufficient for accurate out-of-sample predictions of dynamics such as, e.g.\\ mass-action kinetics, by studying the out-of-distribution generalisation in the case of diffusion dynamics, we find that the neural network model: (i) has a generalisation capacity that depends on the first moment of the initial value data distribution; (ii) learns the non-dissipative nature of dynamics implicitly; and (iii) the model's accuracy resolution limit is of order $\\mathcal{O}(1/\\sqrt{n})$ for a system of size $n$. "
}