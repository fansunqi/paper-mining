{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Mini-batch SGD analysis"
  ],
  "datasets": [
    "MNIST",
    "CIFAR10",
    "synthetic problems"
  ],
  "methods": [
    "Generating functions",
    "Spectrally Expressible approximations"
  ],
  "results": [
    "SGD dynamics exhibits several convergent and divergent regimes",
    "Explicit stability conditions and loss asymptotics for power-law spectral distributions",
    "Optimal convergence rate at negative momenta"
  ],
  "paper_id": "62b3da1f5aee126c0fb1b8f6",
  "title": "A view of mini-batch SGD via generating functions: conditions of\n  convergence, phase transitions, benefit from negative momenta",
  "abstract": "  Mini-batch SGD with momentum is a fundamental algorithm for learning large predictive models. In this paper we develop a new analytic framework to analyze noise-averaged properties of mini-batch SGD for linear models at constant learning rates, momenta and sizes of batches. Our key idea is to consider the dynamics of the second moments of model parameters for a special family of \"Spectrally Expressible\" approximations. This allows to obtain an explicit expression for the generating function of the sequence of loss values. By analyzing this generating function, we find, in particular, that 1) the SGD dynamics exhibits several convergent and divergent regimes depending on the spectral distributions of the problem; 2) the convergent regimes admit explicit stability conditions, and explicit loss asymptotics in the case of power-law spectral distributions; 3) the optimal convergence rate can be achieved at negative momenta. We verify our theoretical predictions by extensive experiments with MNIST, CIFAR10 and synthetic problems, and find a good quantitative agreement. "
}