{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Finding feasible policies for Constrained Markov Decision Processes under probability one constraints"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Endowing controller with a scalar budget",
    "Bellman-like operator for minimal budget",
    "Sample-complexity bounds for learning minimal budget"
  ],
  "results": [
    "Minimal budget as smallest fixed point of Bellman-like operator",
    "Illustration of different nature of probability one constraints"
  ],
  "paper_id": "61b6b9a05244ab9dcbf117de",
  "title": "Reinforcement Learning with Almost Sure Constraints",
  "abstract": "  In this work we address the problem of finding feasible policies for Constrained Markov Decision Processes under probability one constraints. We argue that stationary policies are not sufficient for solving this problem, and that a rich class of policies can be found by endowing the controller with a scalar quantity, so called budget, that tracks how close the agent is to violating the constraint. We show that the minimal budget required to act safely can be obtained as the smallest fixed point of a Bellman-like operator, for which we analyze its convergence properties. We also show how to learn this quantity when the true kernel of the Markov decision process is not known, while providing sample-complexity bounds. The utility of knowing this minimal budget relies in that it can aid in the search of optimal or near-optimal policies by shrinking down the region of the state space the agent must navigate. Simulations illustrate the different nature of probability one constraints against the typically used constraints in expectation. "
}