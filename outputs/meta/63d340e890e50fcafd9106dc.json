{
  "code_links": [
    "https://github.com/verena-hallitschke/pet-ct-annotate"
  ],
  "tasks": [
    "Lung Lesion Segmentation"
  ],
  "datasets": [
    "in-domain validation dataset",
    "unseen PET/CT dataset"
  ],
  "methods": [
    "multimodal interactive segmentation framework",
    "geodesic distance transform",
    "ellipsoid-based user simulation scheme"
  ],
  "results": [
    "None"
  ],
  "paper_id": "63d340e890e50fcafd9106dc",
  "title": "Multimodal Interactive Lung Lesion Segmentation: A Framework for\n  Annotating PET/CT Images based on Physiological and Anatomical Cues",
  "abstract": "  Recently, deep learning enabled the accurate segmentation of various diseases in medical imaging. These performances, however, typically demand large amounts of manual voxel annotations. This tedious process for volumetric data becomes more complex when not all required information is available in a single imaging domain as is the case for PET/CT data. We propose a multimodal interactive segmentation framework that mitigates these issues by combining anatomical and physiological cues from PET/CT data. Our framework utilizes the geodesic distance transform to represent the user annotations and we implement a novel ellipsoid-based user simulation scheme during training. We further propose two annotation interfaces and conduct a user study to estimate their usability. We evaluated our model on the in-domain validation dataset and an unseen PET/CT dataset. We make our code publicly available: https://github.com/verena-hallitschke/pet-ct-annotate. "
}