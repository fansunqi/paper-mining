{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Graph Neural Networks (GNNs) Explainability"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Lower Bound Optimization Approach",
    "Nearest neighbor matching",
    "Continuous masks with a sampling strategy"
  ],
  "results": [
    "Excellent performance in generating the most necessary and sufficient explanations among a series of state-of-the-art methods"
  ],
  "paper_id": "639a906390e50fcafdefe4a3",
  "title": "On the Probability of Necessity and Sufficiency of Explaining Graph\n  Neural Networks: A Lower Bound Optimization Approach",
  "abstract": "  The explainability of Graph Neural Networks (GNNs) is critical to various GNN applications but remains an open challenge. A convincing explanation should be both necessary and sufficient simultaneously. However, existing GNN explaining approaches focus on only one of the two aspects, necessity or sufficiency, or a heuristic trade-off between the two. Theoretically, the Probability of Necessity and Sufficiency (PNS) can be applied to search for the most necessary and sufficient explanation since it can mathematically quantify the necessity and sufficiency of an explanation. Nevertheless, the difficulty of obtaining PNS due to non-monotonicity and the challenge of counterfactual estimation limit its wide use. To address the non-identifiability of PNS, we resort to a lower bound of PNS that can be optimized via counterfactual estimation, and propose Necessary and Sufficient Explanation for GNN (NSEG) via optimizing that lower bound. Specifically, we employ nearest neighbor matching to generate counterfactual samples and leverage continuous masks with a sampling strategy to optimize the lower bound. Empirical study shows that NSEG achieves excellent performance in generating the most necessary and sufficient explanations among a series of state-of-the-art methods. "
}