{
  "code_links": [
    "https://github.com/fnzhan/MISE"
  ],
  "tasks": [
    "Multimodal image synthesis",
    "Multimodal image editing"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Multimodal guidance",
    "Taxonomies according to data modalities and model types"
  ],
  "results": [
    "None"
  ],
  "paper_id": "61ca80355244ab9dcba69a79",
  "title": "Multimodal Image Synthesis and Editing: A Survey",
  "abstract": "  As information exists in various modalities in real world, effective interaction and fusion among multimodal information plays a key role for the creation and perception of multimodal data in computer vision and deep learning research. With superb power in modeling the interaction among multimodal information, multimodal image synthesis and editing has become a hot research topic in recent years. Instead of providing explicit guidance for network training, multimodal guidance offers intuitive and flexible means for image synthesis and editing. On the other hand, this field is also facing several challenges in alignment of multimodal features, synthesis of high-resolution images, faithful evaluation metrics, etc. In this survey, we comprehensively contextualize the advance of the recent multimodal image synthesis and editing and formulate taxonomies according to data modalities and model types. We start with an introduction to different guidance modalities in image synthesis and editing, and then describe multimodal image synthesis and editing approaches extensively according to their model types. After that, we describe benchmark datasets and evaluation metrics as well as corresponding experimental results. Finally, we provide insights about the current research challenges and possible directions for future research. A project associated with this survey is available at https://github.com/fnzhan/MISE. "
}