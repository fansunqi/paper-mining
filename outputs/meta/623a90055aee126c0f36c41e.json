{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Neural network training",
    "Classification",
    "Regression",
    "Clustering"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Random vector functional link (RVFL) network",
    "Shallow RVFLs",
    "Ensemble RVFLs",
    "Deep RVFLs",
    "Ensemble deep RVFL models",
    "Hyperparameter optimization techniques"
  ],
  "results": [
    "Fast training speed",
    "Direct links",
    "Simple architecture",
    "Universal approximation capability",
    "Improved generalization performance"
  ],
  "paper_id": "623a90055aee126c0f36c41e",
  "title": "Random vector functional link network: recent developments,\n  applications, and future directions",
  "abstract": "  Neural networks have been successfully employed in various domains such as classification, regression and clustering, etc. Generally, the back propagation (BP) based iterative approaches are used to train the neural networks, however, it results in the issues of local minima, sensitivity to learning rate and slow convergence. To overcome these issues, randomization based neural networks such as random vector functional link (RVFL) network have been proposed. RVFL model has several characteristics such as fast training speed, direct links, simple architecture, and universal approximation capability, that make it a viable randomized neural network. This article presents the first comprehensive review of the evolution of RVFL model, which can serve as the extensive summary for the beginners as well as practitioners. We discuss the shallow RVFLs, ensemble RVFLs, deep RVFLs and ensemble deep RVFL models. The variations, improvements and applications of RVFL models are discussed in detail. Moreover, we discuss the different hyperparameter optimization techniques followed in the literature to improve the generalization performance of the RVFL model. Finally, we give potential future research directions/opportunities that can inspire the researchers to improve the RVFL's architecture and learning algorithm further. "
}