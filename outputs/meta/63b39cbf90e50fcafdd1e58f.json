{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Learning to Re-Align With Human Values from Text Edits"
  ],
  "datasets": [
    "Value alignment benchmark datasets"
  ],
  "methods": [
    "LM fine-tuning",
    "Reinforcement learning"
  ],
  "results": [
    "Superior performance in three value alignment benchmark datasets",
    "Strong human-value transfer learning ability in few-shot scenarios",
    "Better interpretability and ease for interactive error correction"
  ],
  "paper_id": "63b39cbf90e50fcafdd1e58f",
  "title": "Second Thoughts are Best: Learning to Re-Align With Human Values from\n  Text Edits",
  "abstract": "  We present Second Thought, a new learning paradigm that enables language models (LMs) to re-align with human values. By modeling the chain-of-edits between value-unaligned and value-aligned text, with LM fine-tuning and additional refinement through reinforcement learning, Second Thought not only achieves superior performance in three value alignment benchmark datasets but also shows strong human-value transfer learning ability in few-shot scenarios. The generated editing steps also offer better interpretability and ease for interactive error correction. Extensive human evaluations further confirm its effectiveness. "
}