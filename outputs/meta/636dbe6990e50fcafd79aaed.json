{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Adversarially robust PAC learning",
    "Proper learning under relaxations of the worst-case robust loss"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Robust loss relaxations",
    "Generalization guarantees for the adversarially robust empirical risk minimizer"
  ],
  "results": [
    "VC classes are properly PAC learning with sample complexity close to what one would require in the standard PAC learning setup",
    "Finite VC dimension is not sufficient for proper learning under an existing relaxation of the worst-case robust loss"
  ],
  "paper_id": "636dbe6990e50fcafd79aaed",
  "title": "On Proper Learnability between Average- and Worst-case Robustness",
  "abstract": "  Recently, \\cite{montasser2019vc} showed that finite VC dimension is not sufficient for \\textit{proper} adversarially robust PAC learning. In light of this hardness result, there is a growing effort to study what type of relaxations to the adversarially robust PAC learning setup can enable proper learnability. In this work, we initiate the study of proper learning under relaxations of the worst-case robust loss. We give a family of robust loss relaxations under which VC classes are properly PAC learning with sample complexity close to what one would require in the standard PAC learning setup. On the other hand, we show that for an existing and natural relaxation of the worst-case robust loss, finite VC dimension is not sufficient for proper learning. Lastly, we give new generalization guarantees for the adversarially robust empirical risk minimizer. "
}