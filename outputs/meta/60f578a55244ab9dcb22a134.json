{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Incorporating masking mechanisms into Transformers"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "General mechanism for scalable masking in Transformers",
    "Topological (graph-based) modulation of unmasked attention",
    "Efficient d-dimensional RPE-masking",
    "Graph-kernel masking",
    "Mathematical techniques: spectral analysis, dynamic programming, random walks, solving Markov processes on graphs"
  ],
  "results": [
    "Comprehensive approach for scalable masked Transformers",
    "Special cases: linear causal attention, log-linear RPE-attention",
    "New results: efficient d-dimensional RPE-masking, graph-kernel masking",
    "Empirical evaluation provided"
  ],
  "paper_id": "60f578a55244ab9dcb22a134",
  "title": "From block-Toeplitz matrices to differential equations on graphs:\n  towards a general theory for scalable masked Transformers",
  "abstract": "  In this paper we provide, to the best of our knowledge, the first comprehensive approach for incorporating various masking mechanisms into Transformers architectures in a scalable way. We show that recent results on linear causal attention (Choromanski et al., 2021) and log-linear RPE-attention (Luo et al., 2021) are special cases of this general mechanism. However by casting the problem as a topological (graph-based) modulation of unmasked attention, we obtain several results unknown before, including efficient d-dimensional RPE-masking and graph-kernel masking. We leverage many mathematical techniques ranging from spectral analysis through dynamic programming and random walks to new algorithms for solving Markov processes on graphs. We provide a corresponding empirical evaluation. "
}