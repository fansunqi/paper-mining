{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Sparse representation of multi-dimensional visual data",
    "Hyperspectral image denoising"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Computationally efficient sparse coding optimization",
    "Learnable regularization techniques",
    "Deep unrolling",
    "Deep equilibrium based algorithms"
  ],
  "results": [
    "Proposed algorithms outperform other sparse coding approaches",
    "Superior performance against recent state-of-the-art deep-learning-based denoising models"
  ],
  "paper_id": "62451c2b5aee126c0f47abac",
  "title": "Connections between Deep Equilibrium and Sparse Representation Models\n  with Application to Hyperspectral Image Denoising",
  "abstract": "  In this study, the problem of computing a sparse representation of multi-dimensional visual data is considered. In general, such data e.g., hyperspectral images, color images or video data consists of signals that exhibit strong local dependencies. A new computationally efficient sparse coding optimization problem is derived by employing regularization terms that are adapted to the properties of the signals of interest. Exploiting the merits of the learnable regularization techniques, a neural network is employed to act as structure prior and reveal the underlying signal dependencies. To solve the optimization problem Deep unrolling and Deep equilibrium based algorithms are developed, forming highly interpretable and concise deep-learning-based architectures, that process the input dataset in a block-by-block fashion. Extensive simulation results, in the context of hyperspectral image denoising, are provided, which demonstrate that the proposed algorithms outperform significantly other sparse coding approaches and exhibit superior performance against recent state-of-the-art deep-learning-based denoising models. In a wider perspective, our work provides a unique bridge between a classic approach, that is the sparse representation theory, and modern representation tools that are based on deep learning modeling. "
}