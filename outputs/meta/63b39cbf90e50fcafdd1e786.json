{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Optimization of Directed Information over Discrete Alphabets"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Estimation-optimization framework",
    "Markov decision process",
    "Reinforcement learning",
    "Deep generative model",
    "DI neural estimator"
  ],
  "results": [
    "Theoretical bounds on feedback capacity",
    "Probabilistic shaping of constellations"
  ],
  "paper_id": "63b39cbf90e50fcafdd1e786",
  "title": "Data-Driven Optimization of Directed Information over Discrete Alphabets",
  "abstract": "  Directed information (DI) is a fundamental measure for the study and analysis of sequential stochastic models. In particular, when optimized over input distributions it characterizes the capacity of general communication channels. However, analytic computation of DI is typically intractable and existing optimization techniques over discrete input alphabets require knowledge of the channel model, which renders them inapplicable when only samples are available. To overcome these limitations, we propose a novel estimation-optimization framework for DI over discrete input spaces. We formulate DI optimization as a Markov decision process and leverage reinforcement learning techniques to optimize a deep generative model of the input process probability mass function (PMF). Combining this optimizer with the recently developed DI neural estimator, we obtain an end-to-end estimation-optimization algorithm which is applied to estimating the (feedforward and feedback) capacity of various discrete channels with memory. Furthermore, we demonstrate how to use the optimized PMF model to (i) obtain theoretical bounds on the feedback capacity of unifilar finite-state channels; and (ii) perform probabilistic shaping of constellations in the peak power-constrained additive white Gaussian noise channel. "
}