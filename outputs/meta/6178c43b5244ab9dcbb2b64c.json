{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Physics-informed neural networks optimization",
    "Parameterized PDEs solving"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Metalearning",
    "Transfer learning",
    "Model-aware metalearning applied to PINNs"
  ],
  "results": [
    "Greatly accelerated PINNs optimization process",
    "Tested on various canonical forward parameterized PDEs"
  ],
  "paper_id": "6178c43b5244ab9dcbb2b64c",
  "title": "A Metalearning Approach for Physics-Informed Neural Networks (PINNs):\n  Application to Parameterized PDEs",
  "abstract": "  Physics-informed neural networks (PINNs) as a means of discretizing partial differential equations (PDEs) are garnering much attention in the Computational Science and Engineering (CS&E) world. At least two challenges exist for PINNs at present: an understanding of accuracy and convergence characteristics with respect to tunable parameters and identification of optimization strategies that make PINNs as efficient as other computational science tools. The cost of PINNs training remains a major challenge of Physics-informed Machine Learning (PiML) - and, in fact, machine learning (ML) in general. This paper is meant to move towards addressing the latter through the study of PINNs on new tasks, for which parameterized PDEs provides a good testbed application as tasks can be easily defined in this context. Following the ML world, we introduce metalearning of PINNs with application to parameterized PDEs. By introducing metalearning and transfer learning concepts, we can greatly accelerate the PINNs optimization process. We present a survey of model-agnostic metalearning, and then discuss our model-aware metalearning applied to PINNs as well as implementation considerations and algorithmic complexity. We then test our approach on various canonical forward parameterized PDEs that have been presented in the emerging PINNs literature. "
}