{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Training convergence and implicit regularization of deep residual networks"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Gradient descent",
    "Non-asymptotic estimates for loss function and network weights"
  ],
  "results": [
    "Linear convergence to global optimum",
    "Finite $p-$variation with $p=2$ for scaling limit of trained weights"
  ],
  "paper_id": "625cd6b75aee126c0f3c6df6",
  "title": "Convergence and Implicit Regularization Properties of Gradient Descent\n  for Deep Residual Networks",
  "abstract": "  We prove linear convergence of gradient descent to a global optimum for the training of deep residual networks with constant layer width and smooth activation function. We show that if the trained weights, as a function of the layer index, admit a scaling limit as the depth increases, then the limit has finite $p-$variation with $p=2$. Proofs are based on non-asymptotic estimates for the loss function and for norms of the network weights along the gradient descent path. We illustrate the relevance of our theoretical results to practical settings using detailed numerical experiments on supervised learning problems. "
}