{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Story understanding",
    "Video-text retrieval",
    "Zero-shot alignment"
  ],
  "datasets": [
    "Synopses of Movie Narratives (SyMoN)"
  ],
  "methods": [
    "Collection and preprocessing of video-language story dataset"
  ],
  "results": [
    "Established benchmarks on video-text retrieval and zero-shot alignment",
    "Showcases importance of in-domain data and long-term memory in story understanding"
  ],
  "paper_id": "622eb2495aee126c0f62b0b4",
  "title": "Synopses of Movie Narratives: a Video-Language Dataset for Story\n  Understanding",
  "abstract": "  Despite recent advances of AI, story understanding remains an open and under-investigated problem. We collect, preprocess, and publicly release a video-language story dataset, Synopses of Movie Narratives (SyMoN), containing 5,193 video summaries of popular movies and TV series with a total length of 869 hours. SyMoN captures naturalistic storytelling videos made by human creators and intended for a human audience. As a prototypical and naturalistic story dataset, SyMoN features high coverage of multimodal story events and abundant mental-state descriptions. Its use of storytelling techniques cause cross-domain semantic gaps that provide appropriate challenges to existing models. We establish benchmarks on video-text retrieval and zero-shot alignment on movie summary videos, which showcase the importance of in-domain data and long-term memory in story understanding. With SyMoN, we hope to lay the groundwork for progress in multimodal story understanding. "
}