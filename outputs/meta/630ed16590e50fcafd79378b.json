{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Non-convex optimization",
    "Evolutionary dynamics",
    "Continuous Bayesian inference",
    "Parameter estimation of stochastic processes"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Fisher-Rao natural gradient descent (FR-NGD)",
    "Continuous-time replicator equation",
    "Bayes's rule"
  ],
  "results": [
    "Optimal approximation of evolutionary dynamics",
    "Optimal approximation of continuous Bayesian inference"
  ],
  "paper_id": "630ed16590e50fcafd79378b",
  "title": "Conjugate Natural Selection: Fisher-Rao Natural Gradient Descent\n  Optimally Approximates Evolutionary Dynamics and Continuous Bayesian\n  Inference",
  "abstract": "  Rather than refining individual candidate solutions for a general non-convex optimization problem, by analogy to evolution, we consider minimizing the average loss for a parametric distribution over hypotheses. In this setting, we prove that Fisher-Rao natural gradient descent (FR-NGD) optimally approximates the continuous-time replicator equation (an essential model of evolutionary dynamics) by minimizing the mean-squared error for the relative fitness of competing hypotheses. We term this finding \"conjugate natural selection\" and demonstrate its utility by numerically solving an example non-convex optimization problem over a continuous strategy space. Next, by developing known connections between discrete-time replicator dynamics and Bayes's rule, we show that when absolute fitness corresponds to the negative KL-divergence of a hypothesis's predictions from actual observations, FR-NGD provides the optimal approximation of continuous Bayesian inference. We use this result to demonstrate a novel method for estimating the parameters of stochastic processes. "
}