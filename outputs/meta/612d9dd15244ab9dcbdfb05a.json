{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Continual learning"
  ],
  "datasets": [
    "text classification"
  ],
  "methods": [
    "Prototype-Guided Memory Replay",
    "dynamic prototype-guided memory replay module",
    "online meta-learning model"
  ],
  "results": [
    "superiority in terms of forgetting mitigation and efficiency"
  ],
  "paper_id": "612d9dd15244ab9dcbdfb05a",
  "title": "Prototype-Guided Memory Replay for Continual Learning",
  "abstract": "  Continual learning (CL) refers to a machine learning paradigm that learns continuously without forgetting previously acquired knowledge. Thereby, major difficulty in CL is catastrophic forgetting of preceding tasks, caused by shifts in data distributions. Existing CL models often save a large number of old examples and stochastically revisit previously seen data to retain old knowledge. However, the occupied memory size keeps enlarging along with accumulating seen data. Hereby, we propose a memory-efficient CL method by storing a few samples to achieve good performance. We devise a dynamic prototype-guided memory replay module and incorporate it into an online meta-learning model. We conduct extensive experiments on text classification and investigate the effect of training set orders on CL model performance. The experimental results testify the superiority of our method in terms of forgetting mitigation and efficiency. "
}