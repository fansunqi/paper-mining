{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Reliable representation of inter-rater uncertainty in medical tasks"
  ],
  "datasets": [
    "Two public datasets (not specified by name)"
  ],
  "methods": [
    "STAPLE label fusion",
    "Average of the rater's segmentation",
    "Random sampling of each rater's segmentation",
    "Conventional training framework",
    "SoftSeg framework"
  ],
  "results": [
    "SoftSeg models had better calibration and preservation of inter-rater variability",
    "SoftSeg models had segmentation performance superior or equal to conventional models",
    "Conventional models were overconfident and underestimated inter-rater uncertainty",
    "Averaging with SoftSeg led to underconfident outputs and overestimation of rater disagreement",
    "Best label fusion method was dataset-dependent"
  ],
  "paper_id": "620c6b6a5aee126c0fe2961c",
  "title": "Label fusion and training methods for reliable representation of\n  inter-rater uncertainty",
  "abstract": "  Medical tasks are prone to inter-rater variability due to multiple factors such as image quality, professional experience and training, or guideline clarity. Training deep learning networks with annotations from multiple raters is a common practice that mitigates the model's bias towards a single expert. Reliable models generating calibrated outputs and reflecting the inter-rater disagreement are key to the integration of artificial intelligence in clinical practice. Various methods exist to take into account different expert labels. We focus on comparing three label fusion methods: STAPLE, average of the rater's segmentation, and random sampling of each rater's segmentation during training. Each label fusion method is studied using both the conventional training framework and the recently published SoftSeg framework that limits information loss by treating the segmentation task as a regression. Our results, across 10 data splittings on two public datasets, indicate that SoftSeg models, regardless of the ground truth fusion method, had better calibration and preservation of the inter-rater rater variability compared with their conventional counterparts without impacting the segmentation performance. Conventional models, i.e., trained with a Dice loss, with binary inputs, and sigmoid/softmax final activate, were overconfident and underestimated the uncertainty associated with inter-rater variability. Conversely, fusing labels by averaging with the SoftSeg framework led to underconfident outputs and overestimation of the rater disagreement. In terms of segmentation performance, the best label fusion method was different for the two datasets studied, indicating this parameter might be task-dependent. However, SoftSeg had segmentation performance systematically superior or equal to the conventionally trained models and had the best calibration and preservation of the inter-rater variability. "
}