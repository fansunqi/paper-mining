{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Evaluation of Explanation Methods of AI",
    "CNNs in Image Classification"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "FEM",
    "MLFEM",
    "Grad-CAM",
    "Pearson correlation coefficient",
    "Similarity",
    "Gaze Fixation Density Map",
    "stability metric"
  ],
  "results": [
    "None"
  ],
  "paper_id": "638d612490e50fcafd14ae07",
  "title": "Evaluation of Explanation Methods of AI -- CNNs in Image Classification\n  Tasks with Reference-based and No-reference Metrics",
  "abstract": "  The most popular methods in AI-machine learning paradigm are mainly black boxes. This is why explanation of AI decisions is of emergency. Although dedicated explanation tools have been massively developed, the evaluation of their quality remains an open research question. In this paper, we generalize the methodologies of evaluation of post-hoc explainers of CNNs' decisions in visual classification tasks with reference and no-reference based metrics. We apply them on our previously developed explainers (FEM, MLFEM), and popular Grad-CAM. The reference-based metrics are Pearson correlation coefficient and Similarity computed between the explanation map and its ground truth represented by a Gaze Fixation Density Map obtained with a psycho-visual experiment. As a no-reference metric, we use stability metric, proposed by Alvarez-Melis and Jaakkola. We study its behaviour, consensus with reference-based metrics and show that in case of several kinds of degradation on input images, this metric is in agreement with reference-based ones. Therefore, it can be used for evaluation of the quality of explainers when the ground truth is not available. "
}