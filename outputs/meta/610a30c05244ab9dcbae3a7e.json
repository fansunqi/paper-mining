{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Reinforcement Learning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Variational Actor-Critic Algorithms",
    "Clipping Method",
    "Flipping Method"
  ],
  "results": [
    "Fixed point of the algorithm is close to the optimal policy when the prefactor of the Bellman residual is sufficiently large"
  ],
  "paper_id": "610a30c05244ab9dcbae3a7e",
  "title": "Variational Actor-Critic Algorithms",
  "abstract": "  We introduce a class of variational actor-critic algorithms based on a variational formulation over both the value function and the policy. The objective function of the variational formulation consists of two parts: one for maximizing the value function and the other for minimizing the Bellman residual. Besides the vanilla gradient descent with both the value function and the policy updates, we propose two variants, the clipping method and the flipping method, in order to speed up the convergence. We also prove that, when the prefactor of the Bellman residual is sufficiently large, the fixed point of the algorithm is close to the optimal policy. "
}