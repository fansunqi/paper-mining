{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Semantic image segmentation",
    "Unsupervised domain adaptation"
  ],
  "datasets": [
    "On-board semantic segmentation datasets"
  ],
  "methods": [
    "Co-training procedure",
    "Self-training stage",
    "Model collaboration loop"
  ],
  "results": [
    "Improvements ranging from ~13 to ~26 mIoU points over baselines"
  ],
  "paper_id": "6296d9145aee126c0f731247",
  "title": "Co-Training for Unsupervised Domain Adaptation of Semantic Segmentation\n  Models",
  "abstract": "  Semantic image segmentation is a central and challenging task in autonomous driving, addressed by training deep models. Since this training draws to a curse of human-based image labeling, using synthetic images with automatically generated labels together with unlabeled real-world images is a promising alternative. This implies to address an unsupervised domain adaptation (UDA) problem. In this paper, we propose a new co-training procedure for synth-to-real UDA of semantic segmentation models. It consists of a self-training stage, which provides two domain-adapted models, and a model collaboration loop for the mutual improvement of these two models. These models are then used to provide the final semantic segmentation labels (pseudo-labels) for the real-world images. The overall procedure treats the deep models as black boxes and drives their collaboration at the level of pseudo-labeled target images, i.e., neither modifying loss functions is required, nor explicit feature alignment. We test our proposal on standard synthetic and real-world datasets for on-board semantic segmentation. Our procedure shows improvements ranging from ~13 to ~26 mIoU points over baselines, so establishing new state-of-the-art results. "
}