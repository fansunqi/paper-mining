{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Optimization problems with constraints"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Kernel Sum-Of-Squares (kSoS) approximations"
  ],
  "results": [
    "Unified theorem for convergence guarantees",
    "Use of scattering inequalities to mitigate curse of dimensionality"
  ],
  "paper_id": "63c8b56b90e50fcafd905f30",
  "title": "Approximation of optimization problems with constraints through kernel\n  Sum-Of-Squares",
  "abstract": "Handling an infinite number of inequality constraints in infinite-dimensional\nspaces occurs in many fields, from global optimization to optimal transport.\nThese problems have been tackled individually in several previous articles\nthrough kernel Sum-Of-Squares (kSoS) approximations. We propose here a unified\ntheorem to prove convergence guarantees for these schemes. Pointwise\ninequalities are turned into equalities within a class of nonnegative kSoS\nfunctions. Assuming further that the functions appearing in the problem are\nsmooth, focusing on pointwise equality constraints enables the use of\nscattering inequalities to mitigate the curse of dimensionality in sampling the\nconstraints. Our approach is illustrated in learning vector fields with side\ninformation, here the invariance of a set."
}