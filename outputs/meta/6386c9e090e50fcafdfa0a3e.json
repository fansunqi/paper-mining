{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Geospatial Exploration"
  ],
  "datasets": [
    "Satellite Imagery Datasets"
  ],
  "methods": [
    "Reinforcement Learning",
    "Domain Adaptation"
  ],
  "results": [
    "Significant outperformance over several strong baselines"
  ],
  "paper_id": "6386c9e090e50fcafdfa0a3e",
  "title": "A Visual Active Search Framework for Geospatial Exploration",
  "abstract": "  Many problems can be viewed as forms of geospatial search aided by aerial imagery, with examples ranging from detecting poaching activity to human trafficking. We model this class of problems in a visual active search (VAS) framework, which takes as input an image of a broad area, and aims to identify as many examples of a target object as possible. It does this through a limited sequence of queries, each of which verifies whether an example is present in a given region. A crucial feature of VAS is that each such query is informative about the spatial distribution of target objects beyond what is captured visually (for example, due to spatial correlation). We propose a reinforcement learning approach for VAS that leverages a collection of fully annotated search tasks as training data to learn a search policy, and combines features of the input image with a natural representation of active search state. Additionally, we propose domain adaptation techniques to improve the policy at decision time when training data is not fully reflective of the test-time distribution of VAS tasks. Through extensive experiments on several satellite imagery datasets, we show that the proposed approach significantly outperforms several strong baselines. Code and data will be made public. "
}