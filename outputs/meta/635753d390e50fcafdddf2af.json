{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Dimension Reduction Methods on Machine Learning Algorithms",
    "Psychometrics"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Exploratory Graph Analysis (EGA)",
    "Unique Variable Analysis (UVA)",
    "Principal Component Analysis",
    "Independent Component Analysis"
  ],
  "results": [
    "EGA and UVA perform as well as other reduction techniques or no reduction",
    "Dimension reduction tends to lead to better performance for classification tasks"
  ],
  "paper_id": "635753d390e50fcafdddf2af",
  "title": "An Experimental Study of Dimension Reduction Methods on Machine Learning\n  Algorithms with Applications to Psychometrics",
  "abstract": "  Developing interpretable machine learning models has become an increasingly important issue. One way in which data scientists have been able to develop interpretable models has been to use dimension reduction techniques. In this paper, we examine several dimension reduction techniques including two recent approaches developed in the network psychometrics literature called exploratory graph analysis (EGA) and unique variable analysis (UVA). We compared EGA and UVA with two other dimension reduction techniques common in the machine learning literature (principal component analysis and independent component analysis) as well as no reduction to the variables real data. We show that EGA and UVA perform as well as the other reduction techniques or no reduction. Consistent with previous literature, we show that dimension reduction can decrease, increase, or provide the same accuracy as no reduction of variables. Our tentative results find that dimension reduction tends to lead to better performance when used for classification tasks. "
}