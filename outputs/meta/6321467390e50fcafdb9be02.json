{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Deep learning training performance analysis"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Multi-Instance GPU (MIG)",
    "Multi-Process Service (MPS)"
  ],
  "results": [
    "Up to four times training throughput",
    "Sub-optimal GPU utilization for dynamic mixed workloads"
  ],
  "paper_id": "6321467390e50fcafdb9be02",
  "title": "An Analysis of Collocation on GPUs for Deep Learning Training",
  "abstract": "  Deep learning training is an expensive process that extensively uses GPUs, but not all model training saturates modern powerful GPUs. Multi-Instance GPU (MIG) is a new technology introduced by NVIDIA that can partition a GPU to better-fit workloads that do not require all the memory and compute resources of a full GPU. In this paper, we examine the performance of a MIG-enabled A100 GPU under deep learning workloads containing various sizes and combinations of models. We contrast the benefits of MIG to older workload collocation methods on GPUs: na\\\"ively submitting multiple processes on the same GPU and utilizing Multi-Process Service (MPS). Our results demonstrate that collocating multiple model training runs may yield significant benefits. In certain cases, it can lead up to four times training throughput despite increased epoch time. On the other hand, the aggregate memory footprint and compute needs of the models trained in parallel must fit the available memory and compute resources of the GPU. MIG can be beneficial thanks to its interference-free partitioning, especially when the sizes of the models align with the MIG partitioning options. MIG's rigid partitioning, however, may create sub-optimal GPU utilization for more dynamic mixed workloads. In general, we recommend MPS as the best performing and most flexible form of collocation for model training for a single user submitting training jobs. "
}