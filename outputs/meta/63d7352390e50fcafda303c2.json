{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Contrastive Learning",
    "Self-Supervised Learning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "BCL loss",
    "Bayesian Framework"
  ],
  "results": [
    "Effectiveness and superiority of the BCL loss"
  ],
  "paper_id": "63d7352390e50fcafda303c2",
  "title": "Bayesian Self-Supervised Contrastive Learning",
  "abstract": "Recent years have witnessed many successful applications of contrastive\nlearning in diverse domains, yet its self-supervised version still remains many\nexciting challenges. As the negative samples are drawn from unlabeled datasets,\na randomly selected sample may be actually a false negative to an anchor,\nleading to incorrect encoder training. This paper proposes a new\nself-supervised contrastive loss called the BCL loss that still uses random\nsamples from the unlabeled data while correcting the resulting bias with\nimportance weights. The key idea is to design the desired sampling distribution\nfor sampling hard true negative samples under the Bayesian framework. The\nprominent advantage lies in that the desired sampling distribution is a\nparametric structure, with a location parameter for debiasing false negative\nand concentration parameter for mining hard negative, respectively. Experiments\nvalidate the effectiveness and superiority of the BCL loss."
}