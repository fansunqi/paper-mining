{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Optimization of SAM (Sharpness-Aware Minimization)"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Stochastic Differential Equations (SDEs) for SAM and its variants"
  ],
  "results": [
    "SDEs are rigorous approximations of the real discrete-time algorithms",
    "Explanation of SAM's preference for flat minima",
    "SAM is attracted to saddle points under certain conditions"
  ],
  "paper_id": "63ca06a190e50fcafd68385a",
  "title": "An SDE for Modeling SAM: Theory and Insights",
  "abstract": "  We study the SAM (Sharpness-Aware Minimization) optimizer which has recently attracted a lot of interest due to its increased performance over more classical variants of stochastic gradient descent. Our main contribution is the derivation of continuous-time models (in the form of SDEs) for SAM and two of its variants, both for the full-batch and mini-batch settings. We demonstrate that these SDEs are rigorous approximations of the real discrete-time algorithms (in a weak sense, scaling linearly with the step size). Using these models, we then offer an explanation of why SAM prefers flat minima over sharp ones~--~by showing that it minimizes an implicitly regularized loss with a Hessian-dependent noise structure. Finally, we prove that perhaps unexpectedly SAM is attracted to saddle points under some realistic conditions. Our theoretical results are supported by detailed experiments. "
}