{
  "code_links": [
    "https://youtu.be/I0i8uN2oOP0"
  ],
  "tasks": [
    "End-to-End Autonomous Driving"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "White-box targeted attacks",
    "Image-specific attack",
    "Image-agnostic attack"
  ],
  "results": [
    "Image-specific attack deviates steering angle by 0.478",
    "Image-agnostic attack deviates steering angle by 0.111",
    "Random noises deviate steering angle by 0.002"
  ],
  "paper_id": "6051db9291e011c24e5991f5",
  "title": "Adversarial Driving: Attacking End-to-End Autonomous Driving",
  "abstract": "  As research in deep neural networks advances, deep convolutional networks become promising for autonomous driving tasks. In particular, there is an emerging trend of employing end-to-end neural network models for autonomous driving. However, previous research has shown that deep neural network classifiers are vulnerable to adversarial attacks. While for regression tasks, the effect of adversarial attacks is not as well understood. In this research, we devise two white-box targeted attacks against end-to-end autonomous driving models. Our attacks manipulate the behavior of the autonomous driving system by perturbing the input image. In an average of 800 attacks with the same attack strength (epsilon=1), the image-specific and image-agnostic attack deviates the steering angle from the original output by 0.478 and 0.111, respectively, which is much stronger than random noises that only perturbs the steering angle by 0.002 (The steering angle ranges from [-1, 1]). Both attacks can be initiated in real-time on CPUs without employing GPUs. Demo video: https://youtu.be/I0i8uN2oOP0. "
}