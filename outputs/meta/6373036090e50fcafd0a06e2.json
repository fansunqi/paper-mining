{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Semantic Parsing",
    "Confidence Estimation"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Sequence generation models",
    "Calibration metrics computation"
  ],
  "results": [
    "Calibration varies across models and datasets",
    "New confidence-based challenge splits released"
  ],
  "paper_id": "6373036090e50fcafd0a06e2",
  "title": "Calibrated Interpretation: Confidence Estimation in Semantic Parsing",
  "abstract": "  Sequence generation models are increasingly being used to translate language into executable programs, i.e. to perform executable semantic parsing. The fact that semantic parsing aims to execute actions in the real world motivates developing safe systems, which in turn makes measuring calibration -- a central component to safety -- particularly important. We investigate the calibration of common generation models across four popular semantic parsing datasets, finding that it varies across models and datasets. We then analyze factors associated with calibration error and release new confidence-based challenge splits of two parsing datasets. To facilitate the inclusion of calibration in semantic parsing evaluations, we release a library for computing calibration metrics. "
}