{
  "code_links": [
    "https://nardien.github.io/grad-stylespeech-demo"
  ],
  "tasks": [
    "Text-To-Speech (TTS) synthesis",
    "Any-speaker adaptive TTS"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Diffusion models"
  ],
  "results": [
    "Grad-StyleSpeech significantly outperforms recent speaker-adaptive TTS baselines on English benchmarks"
  ],
  "paper_id": "6376f7df90e50fcafdb9c2d4",
  "title": "Grad-StyleSpeech: Any-speaker Adaptive Text-to-Speech Synthesis with\n  Diffusion Models",
  "abstract": "  There has been a significant progress in Text-To-Speech (TTS) synthesis technology in recent years, thanks to the advancement in neural generative modeling. However, existing methods on any-speaker adaptive TTS have achieved unsatisfactory performance, due to their suboptimal accuracy in mimicking the target speakers' styles. In this work, we present Grad-StyleSpeech, which is an any-speaker adaptive TTS framework that is based on a diffusion model that can generate highly natural speech with extremely high similarity to target speakers' voice, given a few seconds of reference speech. Grad-StyleSpeech significantly outperforms recent speaker-adaptive TTS baselines on English benchmarks. Audio samples are available at https://nardien.github.io/grad-stylespeech-demo. "
}