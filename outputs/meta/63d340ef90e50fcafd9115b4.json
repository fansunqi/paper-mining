{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Risk-sensitive reinforcement learning",
    "Sequential decision-making problems"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Expected Conditional Risk Measures (ECRMs)",
    "Policy gradient methods",
    "REINFORCE algorithm"
  ],
  "results": [
    "Global convergence of risk-averse policy gradient algorithms",
    "Risk control importance"
  ],
  "paper_id": "63d340ef90e50fcafd9115b4",
  "title": "On the Global Convergence of Risk-Averse Policy Gradient Methods with\n  Dynamic Time-Consistent Risk Measures",
  "abstract": "  Risk-sensitive reinforcement learning (RL) has become a popular tool to control the risk of uncertain outcomes and ensure reliable performance in various sequential decision-making problems. While policy gradient methods have been developed for risk-sensitive RL, it remains unclear if these methods enjoy the same global convergence guarantees as in the risk-neutral case. In this paper, we consider a class of dynamic time-consistent risk measures, called Expected Conditional Risk Measures (ECRMs), and derive policy gradient updates for ECRM-based objective functions. Under both constrained direct parameterization and unconstrained softmax parameterization, we provide global convergence of the corresponding risk-averse policy gradient algorithms. We further test a risk-averse variant of REINFORCE algorithm on a stochastic Cliffwalk environment to demonstrate the efficacy of our algorithm and the importance of risk control. "
}