{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Channel Pruning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Rewarded meta-pruning"
  ],
  "results": [
    "Superior performances over state-of-the-art methods in pruning ResNet-50, MobileNetV1, and MobileNetV2 networks"
  ],
  "paper_id": "63d340ef90e50fcafd91166e",
  "title": "Rewarded meta-pruning: Meta Learning with Rewards for Channel Pruning",
  "abstract": "  Convolutional Neural Networks (CNNs) have a large number of parameters and take significantly large hardware resources to compute, so edge devices struggle to run high-level networks. This paper proposes a novel method to reduce the parameters and FLOPs for computational efficiency in deep learning models. We introduce accuracy and efficiency coefficients to control the trade-off between the accuracy of the network and its computing efficiency. The proposed Rewarded meta-pruning algorithm trains a network to generate weights for a pruned model chosen based on the approximate parameters of the final model by controlling the interactions using a reward function. The reward function allows more control over the metrics of the final pruned model. Extensive experiments demonstrate superior performances of the proposed method over the state-of-the-art methods in pruning ResNet-50, MobileNetV1, and MobileNetV2 networks. "
}