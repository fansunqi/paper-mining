{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Goal-directed reading comprehension",
    "Attention allocation in complex tasks"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Transformer-based deep neural networks (DNNs)",
    "Eye-tracking"
  ],
  "results": [
    "Reading time on each word predicted by attention weights in DNNs",
    "Separate attention to text features and question-relevant info during first-pass and rereading",
    "Text features and question relevance modulate attention weights in shallow and deep DNN layers",
    "Reading time without a question predicted by DNNs optimized for word prediction"
  ],
  "paper_id": "60efa6d25244ab9dcbc47dce",
  "title": "Human Attention during Goal-directed Reading Comprehension Relies on\n  Task Optimization",
  "abstract": "  The computational principles underlying attention allocation in complex goal-directed tasks remain elusive. Goal-directed reading, i.e., reading a passage to answer a question in mind, is a common real-world task that strongly engages attention. Here, we investigate what computational models can explain attention distribution in this complex task. We show that the reading time on each word is predicted by the attention weights in transformer-based deep neural networks (DNNs) optimized to perform the same reading task. Eye-tracking further reveals that readers separately attend to basic text features and question-relevant information during first-pass reading and rereading, respectively. Similarly, text features and question relevance separately modulate attention weights in shallow and deep DNN layers. Furthermore, when readers scan a passage without a question in mind, their reading time is predicted by DNNs optimized for a word prediction task. Therefore, attention during real-world reading can be interpreted as the consequence of task optimization. "
}