{
  "code_links": [
    "https://github.com/microsoft/symbolic-robot-teaching-interface"
  ],
  "tasks": [
    "Learning-from-Observation (LfO)",
    "Robot teaching"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Interactive task encoding system (ITES)",
    "Multimodal LfO"
  ],
  "results": [
    "Successful teaching of multiple operations through multimodal demonstrations",
    "Usefulness of ITES for multimodal LfO"
  ],
  "paper_id": "63a3cae990e50fcafdeb7477",
  "title": "Interactive Task Encoding System for Learning-from-Observation",
  "abstract": "  We introduce a practical pipeline that interactively encodes multimodal human demonstrations for robot teaching. This pipeline is designed as an input system for a framework called Learning-from-Observation (LfO), which aims to program household robots with manipulative tasks through few-shots human demonstration without coding. While most previous LfO systems run with visual demonstration, recent research on robot teaching has shown the effectiveness of verbal instruction in making recognition robust and teaching interactive. To the best of our knowledge, however, no LfO system has yet been proposed that utilizes both verbal instruction and interaction, namely \\textit{multimodal LfO}. This paper proposes the interactive task encoding system (ITES) as an input pipeline for multimodal LfO. ITES assumes that the user teaches step-by-step, pausing hand movements in order to match the granularity of human instructions with the granularity of robot execution. ITES recognizes tasks based on step-by-step verbal instructions that accompany the hand movements. Additionally, the recognition is made robust through interactions with the user. We test ITES on a real robot and show that the user can successfully teach multiple operations through multimodal demonstrations. The results suggest the usefulness of ITES for multimodal LfO. The source code is available at https://github.com/microsoft/symbolic-robot-teaching-interface. "
}