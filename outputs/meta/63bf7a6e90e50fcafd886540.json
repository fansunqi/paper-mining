{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Neural network pruning",
    "Computer vision",
    "Efficient inference"
  ],
  "datasets": [
    "ImageNet"
  ],
  "methods": [
    "Model pruning",
    "Neural Architecture Search (NAS)",
    "FBNetV3"
  ],
  "results": [
    "Better performance than existing FBNetV3 models",
    "State-of-the-art results on ImageNet",
    "Reduced GPU-hours for NAS"
  ],
  "paper_id": "63bf7a6e90e50fcafd886540",
  "title": "Pruning Compact ConvNets for Efficient Inference",
  "abstract": "  Neural network pruning is frequently used to compress over-parameterized networks by large amounts, while incurring only marginal drops in generalization performance. However, the impact of pruning on networks that have been highly optimized for efficient inference has not received the same level of attention. In this paper, we analyze the effect of pruning for computer vision, and study state-of-the-art ConvNets, such as the FBNetV3 family of models. We show that model pruning approaches can be used to further optimize networks trained through NAS (Neural Architecture Search). The resulting family of pruned models can consistently obtain better performance than existing FBNetV3 models at the same level of computation, and thus provide state-of-the-art results when trading off between computational complexity and generalization performance on the ImageNet benchmark. In addition to better generalization performance, we also demonstrate that when limited computation resources are available, pruning FBNetV3 models incur only a fraction of GPU-hours involved in running a full-scale NAS. "
}