{
  "code_links": [
    "https://github.com/Guiliang/ICRL-benchmarks-public"
  ],
  "tasks": [
    "Inverse Reinforcement Learning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Variational ICRL",
    "Constraint Inference"
  ],
  "results": [
    "None"
  ],
  "paper_id": "62b2889d5aee126c0fbd3675",
  "title": "Benchmarking Constraint Inference in Inverse Reinforcement Learning",
  "abstract": "  When deploying Reinforcement Learning (RL) agents into a physical system, we must ensure that these agents are well aware of the underlying constraints. In many real-world problems, however, the constraints are often hard to specify mathematically and unknown to the RL agents. To tackle these issues, Inverse Constrained Reinforcement Learning (ICRL) empirically estimates constraints from expert demonstrations. As an emerging research topic, ICRL does not have common benchmarks, and previous works tested algorithms under hand-crafted environments with manually-generated expert demonstrations. In this paper, we construct an ICRL benchmark in the context of RL application domains, including robot control, and autonomous driving. For each environment, we design relevant constraints and train expert agents to generate demonstration data. Besides, unlike existing baselines that learn a deterministic constraint, we propose a variational ICRL method to model a posterior distribution of candidate constraints. We conduct extensive experiments on these algorithms under our benchmark and show how they can facilitate studying important research challenges for ICRL. The benchmark, including the instructions for reproducing ICRL algorithms, is available at https://github.com/Guiliang/ICRL-benchmarks-public. "
}