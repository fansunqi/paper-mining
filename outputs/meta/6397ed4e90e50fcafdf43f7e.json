{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Self-supervised learning of depth and pose"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Constrained Bidirectional Weighted Loss",
    "Affine transformation",
    "View synthesis",
    "Feature difference measurement"
  ],
  "results": [
    "Outperforms existing state-of-the-art self-supervised methods"
  ],
  "paper_id": "6397ed4e90e50fcafdf43f7e",
  "title": "CbwLoss: Constrained Bidirectional Weighted Loss for Self-supervised\n  Learning of Depth and Pose",
  "abstract": "  Photometric differences are widely used as supervision signals to train neural networks for estimating depth and camera pose from unlabeled monocular videos. However, this approach is detrimental for model optimization because occlusions and moving objects in a scene violate the underlying static scenario assumption. In addition, pixels in textureless regions or less discriminative pixels hinder model training. To solve these problems, in this paper, we deal with moving objects and occlusions utilizing the difference of the flow fields and depth structure generated by affine transformation and view synthesis, respectively. Secondly, we mitigate the effect of textureless regions on model optimization by measuring differences between features with more semantic and contextual information without adding networks. In addition, although the bidirectionality component is used in each sub-objective function, a pair of images are reasoned about only once, which helps reduce overhead. Extensive experiments and visual analysis demonstrate the effectiveness of the proposed method, which outperform existing state-of-the-art self-supervised methods under the same conditions and without introducing additional auxiliary information. "
}