{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Watermarking for large language models"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Watermarking framework",
    "Green token selection",
    "Statistical test for watermark detection",
    "Information-theoretic framework"
  ],
  "results": [
    "Watermark can be embedded with negligible impact on text quality",
    "Efficient open-source algorithm for detection",
    "Interpretable p-values for watermark detection",
    "Robustness and security analysis"
  ],
  "paper_id": "63d340e890e50fcafd91093c",
  "title": "A Watermark for Large Language Models",
  "abstract": "  Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of \"green\" tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security. "
}