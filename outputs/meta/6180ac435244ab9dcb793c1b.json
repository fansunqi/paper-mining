{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Understanding AI/ML model behavior",
    "Analyzing robustness of local explanations"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Feature importance explanations",
    "Counterfactual explanations",
    "Survey of robustness analysis"
  ],
  "results": [
    "Unifies existing definitions of robustness",
    "Introduces taxonomy for robustness approaches",
    "Discusses interesting results",
    "Provides pointers for extending robustness analysis"
  ],
  "paper_id": "6180ac435244ab9dcb793c1b",
  "title": "A Survey on the Robustness of Feature Importance and Counterfactual\n  Explanations",
  "abstract": "  There exist several methods that aim to address the crucial task of understanding the behaviour of AI/ML models. Arguably, the most popular among them are local explanations that focus on investigating model behaviour for individual instances. Several methods have been proposed for local analysis, but relatively lesser effort has gone into understanding if the explanations are robust and accurately reflect the behaviour of underlying models. In this work, we present a survey of the works that analysed the robustness of two classes of local explanations (feature importance and counterfactual explanations) that are popularly used in analysing AI/ML models in finance. The survey aims to unify existing definitions of robustness, introduces a taxonomy to classify different robustness approaches, and discusses some interesting results. Finally, the survey introduces some pointers about extending current robustness analysis approaches so as to identify reliable explainability methods. "
}