{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Training Ensemble Models Under Misspecification and Outliers"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Robust PAC$^m$",
    "Combines generalized logarithm score function with PAC$^m$ ensemble bounds"
  ],
  "results": [
    "Enhanced performance under misspecification and in presence of outliers",
    "Produces predictive distributions that counteract effects of misspecification and outliers"
  ],
  "paper_id": "622183525aee126c0f23c7cd",
  "title": "Robust PAC$^m$: Training Ensemble Models Under Misspecification and\n  Outliers",
  "abstract": "  Standard Bayesian learning is known to have suboptimal generalization capabilities under misspecification and in the presence of outliers. PAC-Bayes theory demonstrates that the free energy criterion minimized by Bayesian learning is a bound on the generalization error for Gibbs predictors (i.e., for single models drawn at random from the posterior) under the assumption of sampling distributions uncontaminated by outliers. This viewpoint provides a justification for the limitations of Bayesian learning when the model is misspecified, requiring ensembling, and when data is affected by outliers. In recent work, PAC-Bayes bounds -- referred to as PAC$^m$ -- were derived to introduce free energy metrics that account for the performance of ensemble predictors, obtaining enhanced performance under misspecification. This work presents a novel robust free energy criterion that combines the generalized logarithm score function with PAC$^m$ ensemble bounds. The proposed free energy training criterion produces predictive distributions that are able to concurrently counteract the detrimental effects of misspecification -- with respect to both likelihood and prior distribution -- and outliers. "
}