{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Link between ecosystem simulators and general AI"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Ecotwin ecosystem simulator",
    "Reflex network",
    "Happiness network",
    "Policy network trained with reinforcement learning (RL)"
  ],
  "results": [
    "Emergence of Lotka-Volterra style population dynamics in terrestrial ecosystem",
    "Emergence of diel vertical migration behavior in marine ecosystem",
    "Agents combining RL with reflexes outperform pure RL agents in ecosystems with lethal dangers"
  ],
  "paper_id": "611c8ad55244ab9dcb60911f",
  "title": "The Ecosystem Path to General AI",
  "abstract": "  We start by discussing the link between ecosystem simulators and general AI. Then we present the open-source ecosystem simulator Ecotwin, which is based on the game engine Unity and operates on ecosystems containing inanimate objects like mountains and lakes, as well as organisms such as animals and plants. Animal cognition is modeled by integrating three separate networks: (i) a reflex network for hard-wired reflexes; (ii) a happiness network that maps sensory data such as oxygen, water, energy, and smells, to a scalar happiness value; and (iii) a policy network for selecting actions. The policy network is trained with reinforcement learning (RL), where the reward signal is defined as the happiness difference from one time step to the next. All organisms are capable of either sexual or asexual reproduction, and they die if they run out of critical resources. We report results from three studies with Ecotwin, in which natural phenomena emerge in the models without being hardwired. First, we study a terrestrial ecosystem with wolves, deer, and grass, in which a Lotka-Volterra style population dynamics emerges. Second, we study a marine ecosystem with phytoplankton, copepods, and krill, in which a diel vertical migration behavior emerges. Third, we study an ecosystem involving lethal dangers, in which certain agents that combine RL with reflexes outperform pure RL agents. "
}