{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Learning identity-preserving transformations"
  ],
  "datasets": [
    "MNIST",
    "Fashion MNIST",
    "CelebA"
  ],
  "methods": [
    "Learning strategy without transformation labels",
    "Method for learning local regions for each operator"
  ],
  "results": [
    "Model's ability to learn identity-preserving transformations on multi-class datasets",
    "Ability to learn semantically meaningful transformations on complex datasets in an unsupervised manner"
  ],
  "paper_id": "60d537f891e01153881e8456",
  "title": "Learning Identity-Preserving Transformations on Data Manifolds",
  "abstract": "  Many machine learning techniques incorporate identity-preserving transformations into their models to generalize their performance to previously unseen data. These transformations are typically selected from a set of functions that are known to maintain the identity of an input when applied (e.g., rotation, translation, flipping, and scaling). However, there are many natural variations that cannot be labeled for supervision or defined through examination of the data. As suggested by the manifold hypothesis, many of these natural variations live on or near a low-dimensional, nonlinear manifold. Several techniques represent manifold variations through a set of learned Lie group operators that define directions of motion on the manifold. However, these approaches are limited because they require transformation labels when training their models and they lack a method for determining which regions of the manifold are appropriate for applying each specific operator. We address these limitations by introducing a learning strategy that does not require transformation labels and developing a method that learns the local regions where each operator is likely to be used while preserving the identity of inputs. Experiments on MNIST and Fashion MNIST highlight our model's ability to learn identity-preserving transformations on multi-class datasets. Additionally, we train on CelebA to showcase our model's ability to learn semantically meaningful transformations on complex datasets in an unsupervised manner. "
}