{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Distributed learning with parameter-server setting"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Adaptive Bounded Staleness (ABS)"
  ],
  "results": [
    "Wall-clock convergence time and communication rounds improved"
  ],
  "paper_id": "63d9d85d90e50fcafd5785c0",
  "title": "ABS: Adaptive Bounded Staleness Converges Faster and Communicates Less",
  "abstract": "Wall-clock convergence time and communication rounds are critical performance\nmetrics in distributed learning with parameter-server setting. While\nsynchronous methods converge fast but are not robust to stragglers; and\nasynchronous ones can reduce the wall-clock time per round but suffers from\ndegraded convergence rate due to the staleness of gradients, it is natural to\ncombine the two methods to achieve a balance. In this work, we develop a novel\nasynchronous strategy that leverages the advantages of both synchronous methods\nand asynchronous ones, named adaptive bounded staleness (ABS). The key enablers\nof ABS are two-fold. First, the number of workers that the PS waits for per\nround for gradient aggregation is adaptively selected to strike a\nstraggling-staleness balance. Second, the workers with relatively high\nstaleness are required to start a new round of computation to alleviate the\nnegative effect of staleness. Simulation results are provided to demonstrate\nthe superiority of ABS over state-of-the-art schemes in terms of wall-clock\ntime and communication rounds."
}