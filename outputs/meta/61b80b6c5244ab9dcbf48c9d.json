{
  "code_links": [
    "Github.com/amazon-research/fast-rl-with-slow-updates"
  ],
  "tasks": [
    "Deep reinforcement learning value function optimization"
  ],
  "datasets": [
    "Atari benchmark"
  ],
  "methods": [
    "DQN Pro",
    "Rainbow Pro",
    "Incentivize online network to remain close to target network"
  ],
  "results": [
    "Significant performance improvements over DQN and Rainbow on Atari benchmark"
  ],
  "paper_id": "61b80b6c5244ab9dcbf48c9d",
  "title": "Faster Deep Reinforcement Learning with Slower Online Network",
  "abstract": "  Deep reinforcement learning algorithms often use two networks for value function optimization: an online network, and a target network that tracks the online network with some delay. Using two separate networks enables the agent to hedge against issues that arise when performing bootstrapping. In this paper we endow two popular deep reinforcement learning algorithms, namely DQN and Rainbow, with updates that incentivize the online network to remain in the proximity of the target network. This improves the robustness of deep reinforcement learning in presence of noisy updates. The resultant agents, called DQN Pro and Rainbow Pro, exhibit significant performance improvements over their original counterparts on the Atari benchmark demonstrating the effectiveness of this simple idea in deep reinforcement learning. The code for our paper is available here: Github.com/amazon-research/fast-rl-with-slow-updates. "
}