{
  "code_links": [
    "None"
  ],
  "tasks": [
    "3D articulated objects perception and manipulation",
    "Motion planning for articulated objects"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Neural network for predicting motion vector field",
    "Analytical motion planner"
  ],
  "results": [
    "State-of-the-art performance in simulated and real-world experiments"
  ],
  "paper_id": "6279c9c65aee126c0fdae846",
  "title": "FlowBot3D: Learning 3D Articulation Flow to Manipulate Articulated\n  Objects",
  "abstract": "  We explore a novel method to perceive and manipulate 3D articulated objects that generalizes to enable a robot to articulate unseen classes of objects. We propose a vision-based system that learns to predict the potential motions of the parts of a variety of articulated objects to guide downstream motion planning of the system to articulate the objects. To predict the object motions, we train a neural network to output a dense vector field representing the point-wise motion direction of the points in the point cloud under articulation. We then deploy an analytical motion planner based on this vector field to achieve a policy that yields maximum articulation. We train the vision system entirely in simulation, and we demonstrate the capability of our system to generalize to unseen object instances and novel categories in both simulation and the real world, deploying our policy on a Sawyer robot with no finetuning. Results show that our system achieves state-of-the-art performance in both simulated and real-world experiments. "
}