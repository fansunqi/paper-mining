{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Direct Evolutionary Optimization of Variational Autoencoders"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Discrete variational method with truncated posteriors",
    "Evolutionary algorithms for discrete optimization",
    "Gradient ascent for network weights",
    "Decoder-based latent state selection"
  ],
  "results": [
    "Efficiently scalable to hundreds of latents",
    "Highly competitive in `zero-shot' learning",
    "Effective denoising without prior training on clean data",
    "Competitive performance in zero-shot settings against non-generative approaches"
  ],
  "paper_id": "5fc4dcd391e011abfa2fb08c",
  "title": "Direct Evolutionary Optimization of Variational Autoencoders With Binary\n  Latents",
  "abstract": "  Discrete latent variables are considered important for real world data, which has motivated research on Variational Autoencoders (VAEs) with discrete latents. However, standard VAE training is not possible in this case, which has motivated different strategies to manipulate discrete distributions in order to train discrete VAEs similarly to conventional ones. Here we ask if it is also possible to keep the discrete nature of the latents fully intact by applying a direct discrete optimization for the encoding model. The approach is consequently strongly diverting from standard VAE-training by sidestepping sampling approximation, reparameterization trick and amortization. Discrete optimization is realized in a variational setting using truncated posteriors in conjunction with evolutionary algorithms. For VAEs with binary latents, we (A) show how such a discrete variational method ties into gradient ascent for network weights, and (B) how the decoder is used to select latent states for training. Conventional amortized training is more efficient and applicable to large neural networks. However, using smaller networks, we here find direct discrete optimization to be efficiently scalable to hundreds of latents. More importantly, we find the effectiveness of direct optimization to be highly competitive in `zero-shot' learning. In contrast to large supervised networks, the here investigated VAEs can, e.g., denoise a single image without previous training on clean data and/or training on large image datasets. More generally, the studied approach shows that training of VAEs is indeed possible without sampling-based approximation and reparameterization, which may be interesting for the analysis of VAE-training in general. For `zero-shot' settings a direct optimization, furthermore, makes VAEs competitive where they have previously been outperformed by non-generative approaches. "
}