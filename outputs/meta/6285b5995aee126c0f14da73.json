{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Reinforcement learning",
    "Bandit problems",
    "Discounted MDPs"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Slowly changing adversarial bandit algorithms",
    "Optimal regret",
    "Bellman backups"
  ],
  "results": [
    "Optimal expected regret in infinite-horizon discounted MDPs",
    "Applicability to EXP3 algorithm"
  ],
  "paper_id": "6285b5995aee126c0f14da73",
  "title": "Slowly Changing Adversarial Bandit Algorithms are Efficient for\n  Discounted MDPs",
  "abstract": "  Reinforcement learning generalizes bandit problems with additional difficulties on longer planning horizon and unknown transition kernel. We show that, under some mild assumptions, *any* slowly changing adversarial bandit algorithm enjoys optimal regret in adversarial bandits can achieve optimal (in the dependency of $T$) expected regret in infinite-horizon discounted MDPs, without the presence of Bellman backups. The slowly changing property required by our generalization is mild, which is also marked by the online Markov decision process literature. We also examine the applicability of our reduction to a well-known adversarial bandit algorithm, EXP3. "
}