{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Alzheimer's Disease diagnosis and prognosis"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Ensemble of 125 U-Nets",
    "Graph convolutional neural network classifier"
  ],
  "results": [
    "Comparative performance to the state-of-the-art methods",
    "Better generalization capacity"
  ],
  "paper_id": "62a013785aee126c0ff69442",
  "title": "Towards better Interpretable and Generalizable AD detection using\n  Collective Artificial Intelligence",
  "abstract": "  Alzheimer's Disease is the most common cause of dementia. Accurate diagnosis and prognosis of this disease are essential to design an appropriate treatment plan, increasing the life expectancy of the patient. Intense research has been conducted on the use of machine learning to identify Alzheimer's Disease from neuroimaging data, such as structural magnetic resonance imaging. In recent years, advances of deep learning in computer vision suggest a new research direction for this problem. Current deep learning-based approaches in this field, however, have a number of drawbacks, including the interpretability of model decisions, a lack of generalizability information and a lower performance compared to traditional machine learning techniques. In this paper, we design a two-stage framework to overcome these limitations. In the first stage, an ensemble of 125 U-Nets is used to grade the input image, producing a 3D map that reflects the disease severity at voxel-level. This map can help to localize abnormal brain areas caused by the disease. In the second stage, we model a graph per individual using the generated grading map and other information about the subject. We propose to use a graph convolutional neural network classifier for the final classification. As a result, our framework demonstrates comparative performance to the state-of-the-art methods in different datasets for both diagnosis and prognosis. We also demonstrate that the use of a large ensemble of U-Nets offers a better generalization capacity for our framework. "
}