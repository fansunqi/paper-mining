{
  "code_links": [
    "github.com/microsoft/binder"
  ],
  "tasks": [
    "Named Entity Recognition"
  ],
  "datasets": [
    "ACE2004",
    "ACE2005",
    "GENIA",
    "NCBI",
    "BC5CDR",
    "JNLPBA"
  ],
  "methods": [
    "Bi-Encoder",
    "Contrastive Learning",
    "Dynamic thresholding loss"
  ],
  "results": [
    "New state of the art across standard datasets"
  ],
  "paper_id": "6310233d90e50fcafdc2f5b3",
  "title": "Optimizing Bi-Encoder for Named Entity Recognition via Contrastive\n  Learning",
  "abstract": "  We present a bi-encoder framework for named entity recognition (NER), which applies contrastive learning to map candidate text spans and entity types into the same vector representation space. Prior work predominantly approaches NER as sequence labeling or span classification. We instead frame NER as a representation learning problem that maximizes the similarity between the vector representations of an entity mention and its type. This makes it easy to handle nested and flat NER alike, and can better leverage noisy self-supervision signals. A major challenge to this bi-encoder formulation for NER lies in separating non-entity spans from entity mentions. Instead of explicitly labeling all non-entity spans as the same class $\\texttt{Outside}$ ($\\texttt{O}$) as in most prior methods, we introduce a novel dynamic thresholding loss. Experiments show that our method performs well in both supervised and distantly supervised settings, for nested and flat NER alike, establishing new state of the art across standard datasets in the general domain (e.g., ACE2004, ACE2005) and high-value verticals such as biomedicine (e.g., GENIA, NCBI, BC5CDR, JNLPBA). We release the code at github.com/microsoft/binder. "
}