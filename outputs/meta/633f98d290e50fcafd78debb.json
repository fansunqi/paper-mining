{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Neural Network Interpretability",
    "Transformer Characterization"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "First-Order Logic with Majority",
    "Boolean Threshold Circuits",
    "Circuit-Logic Connections"
  ],
  "results": [
    "Transformer behavior captured in simple formalism",
    "Integer division is 'transformer-hard'",
    "First-order logic with majority as a useful language for transformer programs"
  ],
  "paper_id": "633f98d290e50fcafd78debb",
  "title": "Transformers Can Be Expressed In First-Order Logic with Majority",
  "abstract": "  Characterizing the implicit structure of the computation within neural networks is a foundational problem in the area of deep learning interpretability. Can the inner decision process of neural networks be captured symbolically in some familiar logic? We show that any fixed-precision transformer neural network can be translated into an equivalent fixed-size $\\mathsf{FO}(\\mathsf{M})$ formula, i.e., a first-order logic formula that, in addition to standard universal and existential quantifiers, may also contain majority-vote quantifiers. The proof idea is to design highly uniform boolean threshold circuits that can simulate transformers, and then leverage known theoretical connections between circuits and logic. Our results reveal a surprisingly simple formalism for capturing the behavior of transformers, show that simple problems like integer division are \"transformer-hard\", and provide valuable insights for comparing transformers to other models like RNNs. Our results suggest that first-order logic with majority may be a useful language for expressing programs extracted from transformers. "
}