{
  "code_links": [
    "https://github.com/Boyiliee/SITTA"
  ],
  "tasks": [
    "Single Image Texture Translation",
    "Data Augmentation",
    "Long-tailed Image Classification",
    "Few-shot Image Classification"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "SITTA (Single Image Texture Translation for data Augmentation)",
    "Lightweight yet efficient model for texture translation"
  ],
  "results": [
    "Consistently improved image recognition performance",
    "Capable of translating texture of input data into a target domain"
  ],
  "paper_id": "60da97b391e0112af847ea8c",
  "title": "SITTA: Single Image Texture Translation for Data Augmentation",
  "abstract": "  Recent advances in data augmentation enable one to translate images by learning the mapping between a source domain and a target domain. Existing methods tend to learn the distributions by training a model on a variety of datasets, with results evaluated largely in a subjective manner. Relatively few works in this area, however, study the potential use of image synthesis methods for recognition tasks. In this paper, we propose and explore the problem of image translation for data augmentation. We first propose a lightweight yet efficient model for translating texture to augment images based on a single input of source texture, allowing for fast training and testing, referred to as Single Image Texture Translation for data Augmentation (SITTA). Then we explore the use of augmented data in long-tailed and few-shot image classification tasks. We find the proposed augmentation method and workflow is capable of translating the texture of input data into a target domain, leading to consistently improved image recognition performance. Finally, we examine how SITTA and related image translation methods can provide a basis for a data-efficient, \"augmentation engineering\" approach to model training. Codes are available at https://github.com/Boyiliee/SITTA. "
}