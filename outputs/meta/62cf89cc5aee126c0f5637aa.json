{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Unsupervised Salient Object Detection"
  ],
  "datasets": [
    "RGB",
    "RGB-D",
    "RGB-T",
    "video SOD benchmarks"
  ],
  "methods": [
    "Confidence-aware Saliency Distilling",
    "Boundary-aware Texture Matching"
  ],
  "results": [
    "State-of-the-art USOD performance"
  ],
  "paper_id": "62cf89cc5aee126c0f5637aa",
  "title": "Texture-guided Saliency Distilling for Unsupervised Salient Object\n  Detection",
  "abstract": "  Deep Learning-based Unsupervised Salient Object Detection (USOD) mainly relies on the noisy saliency pseudo labels that have been generated from traditional handcraft methods or pre-trained networks. To cope with the noisy labels problem, a class of methods focus on only easy samples with reliable labels but ignore valuable knowledge in hard samples. In this paper, we propose a novel USOD method to mine rich and accurate saliency knowledge from both easy and hard samples. First, we propose a Confidence-aware Saliency Distilling (CSD) strategy that scores samples conditioned on samples' confidences, which guides the model to distill saliency knowledge from easy samples to hard samples progressively. Second, we propose a Boundary-aware Texture Matching (BTM) strategy to refine the boundaries of noisy labels by matching the textures around the predicted boundary. Extensive experiments on RGB, RGB-D, RGB-T, and video SOD benchmarks prove that our method achieves state-of-the-art USOD performance. "
}