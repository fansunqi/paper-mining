{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Approximation of functions by tensor networks"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Tensor product structure for Lebesgue $L^p$-spaces",
    "Rank-structured functions of finite representation complexity",
    "Approximation classes of tensor networks",
    "Feed-forward sum-product networks with sparse architecture"
  ],
  "results": [
    "Identification of Lebesgue $L^p$-spaces with tensor product spaces",
    "Definition of quasi-normed linear spaces for approximation classes",
    "Continuous embedding of classical smoothness spaces into approximation classes",
    "Lack of Besov smoothness in approximation classes without depth restriction"
  ],
  "paper_id": "5efdabf191e01191d3d281d8",
  "title": "Approximation Theory of Tree Tensor Networks: Tensorized Univariate\n  Functions -- Part I",
  "abstract": "  We study the approximation of functions by tensor networks (TNs). We show that Lebesgue $L^p$-spaces in one dimension can be identified with tensor product spaces of arbitrary order through tensorization. We use this tensor product structure to define subsets of $L^p$ of rank-structured functions of finite representation complexity. These subsets are then used to define different approximation classes of tensor networks, associated with different measures of complexity. These approximation classes are shown to be quasi-normed linear spaces. We study some elementary properties and relationships of said spaces. In part II of this work, we will show that classical smoothness (Besov) spaces are continuously embedded into these approximation classes. We will also show that functions in these approximation classes do not possess any Besov smoothness, unless one restricts the depth of the tensor networks. The results of this work are both an analysis of the approximation spaces of TNs and a study of the expressivity of a particular type of neural networks (NN) -- namely feed-forward sum-product networks with sparse architecture. The input variables of this network result from the tensorization step, interpreted as a particular featuring step which can also be implemented with a neural network with a specific architecture. We point out interesting parallels to recent results on the expressivity of rectified linear unit (ReLU) networks -- currently one of the most popular type of NNs. "
}