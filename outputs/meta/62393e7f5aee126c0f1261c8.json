{
  "code_links": [
    "None"
  ],
  "tasks": [
    "End-to-End Text-to-Speech Synthesis"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Differentiable Duration Modeling",
    "Soft-duration Mechanism",
    "Adversarial Training"
  ],
  "results": [
    "Competitive results with simpler training pipeline"
  ],
  "paper_id": "62393e7f5aee126c0f1261c8",
  "title": "AutoTTS: End-to-End Text-to-Speech Synthesis through Differentiable\n  Duration Modeling",
  "abstract": "  Parallel text-to-speech (TTS) models have recently enabled fast and highly-natural speech synthesis. However, they typically require external alignment models, which are not necessarily optimized for the decoder as they are not jointly trained. In this paper, we propose a differentiable duration method for learning monotonic alignments between input and output sequences. Our method is based on a soft-duration mechanism that optimizes a stochastic process in expectation. Using this differentiable duration method, we introduce AutoTTS, a direct text-to-waveform speech synthesis model. AutoTTS enables high-fidelity speech synthesis through a combination of adversarial training and matching the total ground-truth duration. Experimental results show that our model obtains competitive results while enjoying a much simpler training pipeline. Audio samples are available online. "
}