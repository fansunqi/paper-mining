{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Student modeling in block-based visual programming",
    "Synthesizing student attempts"
  ],
  "datasets": [
    "StudentSyn"
  ],
  "methods": [
    "TutorSS (human experts)",
    "NeurSS (neuro/symbolic technique)",
    "SymSS (neuro/symbolic technique)"
  ],
  "results": [
    "Human experts (TutorSS) achieve high performance on the benchmark",
    "Simple baselines perform poorly"
  ],
  "paper_id": "6271e0e75aee126c0f574731",
  "title": "From {Solution Synthesis} to {Student Attempt Synthesis} for Block-Based\n  Visual Programming Tasks",
  "abstract": "  Block-based visual programming environments are increasingly used to introduce computing concepts to beginners. Given that programming tasks are open-ended and conceptual, novice students often struggle when learning in these environments. AI-driven programming tutors hold great promise in automatically assisting struggling students, and need several components to realize this potential. We investigate the crucial component of student modeling, in particular, the ability to automatically infer students' misconceptions for predicting (synthesizing) their behavior. We introduce a novel benchmark, StudentSyn, centered around the following challenge: For a given student, synthesize the student's attempt on a new target task after observing the student's attempt on a fixed reference task. This challenge is akin to that of program synthesis; however, instead of synthesizing a {solution} (i.e., program an expert would write), the goal here is to synthesize a {student attempt} (i.e., program that a given student would write). We first show that human experts (TutorSS) can achieve high performance on the benchmark, whereas simple baselines perform poorly. Then, we develop two neuro/symbolic techniques (NeurSS and SymSS) in a quest to close this gap with TutorSS. "
}