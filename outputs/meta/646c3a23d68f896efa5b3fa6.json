{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Nystr\"om approximation",
    "Kernel quadrature"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Improved error bound for Nystr\"om approximation",
    "Refined selection of subspaces",
    "Application to convex kernel quadrature"
  ],
  "results": [
    "Improved error bound",
    "Theoretical guarantees",
    "Novel theoretical guarantees",
    "Numerical observations"
  ],
  "paper_id": "646c3a23d68f896efa5b3fa6",
  "title": "Sampling-based Nystr\\\"om Approximation and Kernel Quadrature",
  "abstract": "  We analyze the Nystr\\\"om approximation of a positive definite kernel associated with a probability measure. We first prove an improved error bound for the conventional Nystr\\\"om approximation with i.i.d. sampling and singular-value decomposition in the continuous regime; the proof techniques are borrowed from statistical learning theory. We further introduce a refined selection of subspaces in Nystr\\\"om approximation with theoretical guarantees that is applicable to non-i.i.d. landmark points. Finally, we discuss their application to convex kernel quadrature and give novel theoretical guarantees as well as numerical observations. "
}