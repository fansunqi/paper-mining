{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Data-driven control",
    "Deep reinforcement learning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Generalized Policy Improvement algorithms"
  ],
  "results": [
    "Balances practical performance guarantees and data efficiency",
    "Improves on-policy methods with sample reuse efficiency"
  ],
  "paper_id": "62bbc3875aee126c0fa68761",
  "title": "Generalized Policy Improvement Algorithms with Theoretically Supported\n  Sample Reuse",
  "abstract": "  Data-driven, learning-based control methods offer the potential to improve operations in complex systems, and model-free deep reinforcement learning represents a popular approach to data-driven control. However, existing classes of algorithms present a trade-off between two important deployment requirements for real-world control: (i) practical performance guarantees and (ii) data efficiency. Off-policy algorithms make efficient use of data through sample reuse but lack theoretical guarantees, while on-policy algorithms guarantee approximate policy improvement throughout training but suffer from high sample complexity. In order to balance these competing goals, we develop a class of Generalized Policy Improvement algorithms that combines the policy improvement guarantees of on-policy methods with the efficiency of sample reuse. We demonstrate the benefits of this new class of algorithms through extensive experimental analysis on a variety of continuous control tasks from the DeepMind Control Suite. "
}