{
  "code_links": [
    "None"
  ],
  "tasks": [
    "dynamic semantic SLAM"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "TwistSLAM++, fusion of stereo images and LiDAR information, semantic information for object tracking, registration on consecutive object scans, object scans for shape estimation"
  ],
  "results": [
    "improved accuracy of object tracking"
  ],
  "paper_id": "6327dda790e50fcafd67e1a5",
  "title": "TwistSLAM++: Fusing multiple modalities for accurate dynamic semantic\n  SLAM",
  "abstract": "  Most classical SLAM systems rely on the static scene assumption, which limits their applicability in real world scenarios. Recent SLAM frameworks have been proposed to simultaneously track the camera and moving objects. However they are often unable to estimate the canonical pose of the objects and exhibit a low object tracking accuracy. To solve this problem we propose TwistSLAM++, a semantic, dynamic, SLAM system that fuses stereo images and LiDAR information. Using semantic information, we track potentially moving objects and associate them to 3D object detections in LiDAR scans to obtain their pose and size. Then, we perform registration on consecutive object scans to refine object pose estimation. Finally, object scans are used to estimate the shape of the object and constrain map points to lie on the estimated surface within the BA. We show on classical benchmarks that this fusion approach based on multimodal information improves the accuracy of object tracking. "
}