{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Image-and-Language Understanding",
    "Multimodal Tasks",
    "Retrieval",
    "Zero-shot Image Classification",
    "Natural Language Understanding",
    "Visual Question Answering",
    "Multilingual Multimodal Retrieval"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "CLIPPO",
    "Contrastive Loss",
    "Single Encoder",
    "Image-Text Contrastive Learning",
    "Next-sentence Contrastive Learning"
  ],
  "results": [
    "Half the number of parameters compared to CLIP-style models",
    "Outperforms pixel-based prior work on natural language understanding tasks",
    "Good accuracy in visual question answering",
    "Strong performance on multilingual multimodal retrieval without modifications"
  ],
  "paper_id": "639be1d090e50fcafd578da9",
  "title": "CLIPPO: Image-and-Language Understanding from Pixels Only",
  "abstract": "  Multimodal models are becoming increasingly effective, in part due to unified components, such as the Transformer architecture. However, multimodal models still often consist of many task- and modality-specific pieces and training procedures. For example, CLIP (Radford et al., 2021) trains independent text and image towers via a contrastive loss. We explore an additional unification: the use of a pure pixel-based model to perform image, text, and multimodal tasks. Our model is trained with contrastive loss alone, so we call it CLIP-Pixels Only (CLIPPO). CLIPPO uses a single encoder that processes both regular images and text rendered as images. CLIPPO performs image-based tasks such as retrieval and zero-shot image classification almost as well as CLIP-style models, with half the number of parameters and no text-specific tower or embedding. When trained jointly via image-text contrastive learning and next-sentence contrastive learning, CLIPPO can perform well on natural language understanding tasks, without any word-level loss (language modelling or masked language modelling), outperforming pixel-based prior work. Surprisingly, CLIPPO can obtain good accuracy in visual question answering, simply by rendering the question and image together. Finally, we exploit the fact that CLIPPO does not require a tokenizer to show that it can achieve strong performance on multilingual multimodal retrieval without modifications. "
}