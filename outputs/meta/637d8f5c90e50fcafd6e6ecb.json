{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Vision Learners"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Implicit Neural Representation Network (INRN)"
  ],
  "results": [
    "Effectiveness on low-level tasks (image fitting) and high-level vision tasks (image classification, object detection, instance segmentation)"
  ],
  "paper_id": "637d8f5c90e50fcafd6e6ecb",
  "title": "Rethinking Implicit Neural Representations for Vision Learners",
  "abstract": "  Implicit Neural Representations (INRs) are powerful to parameterize continuous signals in computer vision. However, almost all INRs methods are limited to low-level tasks, e.g., image/video compression, super-resolution, and image generation. The questions on how to explore INRs to high-level tasks and deep networks are still under-explored. Existing INRs methods suffer from two problems: 1) narrow theoretical definitions of INRs are inapplicable to high-level tasks; 2) lack of representation capabilities to deep networks. Motivated by the above facts, we reformulate the definitions of INRs from a novel perspective and propose an innovative Implicit Neural Representation Network (INRN), which is the first study of INRs to tackle both low-level and high-level tasks. Specifically, we present three key designs for basic blocks in INRN along with two different stacking ways and corresponding loss functions. Extensive experiments with analysis on both low-level tasks (image fitting) and high-level vision tasks (image classification, object detection, instance segmentation) demonstrate the effectiveness of the proposed method. "
}