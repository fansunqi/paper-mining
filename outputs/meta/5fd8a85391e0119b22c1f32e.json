{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Conservative Bandit Problems"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "One-Size-Fits-All solution to CBPs",
    "Conservative Multi-Armed Bandits (CMAB)",
    "Conservative Linear Bandits (CLB)",
    "Conservative Contextual Combinatorial Bandits (CCCB)",
    "Conservative Mean-Variance Bandit Problem (MV-CBP) algorithm"
  ],
  "results": [
    "T-independent additive regrets for CBPs",
    "O(1/T) normalized additive regrets for MV-CBP"
  ],
  "paper_id": "5fd8a85391e0119b22c1f32e",
  "title": "A One-Size-Fits-All Solution to Conservative Bandit Problems",
  "abstract": "  In this paper, we study a family of conservative bandit problems (CBPs) with sample-path reward constraints, i.e., the learner's reward performance must be at least as well as a given baseline at any time. We propose a One-Size-Fits-All solution to CBPs and present its applications to three encompassed problems, i.e. conservative multi-armed bandits (CMAB), conservative linear bandits (CLB) and conservative contextual combinatorial bandits (CCCB). Different from previous works which consider high probability constraints on the expected reward, we focus on a sample-path constraint on the actually received reward, and achieve better theoretical guarantees ($T$-independent additive regrets instead of $T$-dependent) and empirical performance. Furthermore, we extend the results and consider a novel conservative mean-variance bandit problem (MV-CBP), which measures the learning performance with both the expected reward and variability. For this extended problem, we provide a novel algorithm with $O(1/T)$ normalized additive regrets ($T$-independent in the cumulative form) and validate this result through empirical evaluation. "
}