{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Few-shot learning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Interpretable model based on human-friendly attributes",
    "Online attribute selection mechanism",
    "Attribute detection and compensation mechanism"
  ],
  "results": [
    "Results on par with black-box few-shot-learning models"
  ],
  "paper_id": "6375a67190e50fcafd3e1d59",
  "title": "Interpretable Few-shot Learning with Online Attribute Selection",
  "abstract": "  Few-shot learning (FSL) is a challenging learning problem in which only a few samples are available for each class. Decision interpretation is more important in few-shot classification since there is a greater chance of error than in traditional classification. However, most of the previous FSL methods are black-box models. In this paper, we propose an inherently interpretable model for FSL based on human-friendly attributes. Moreover, we propose an online attribute selection mechanism that can effectively filter out irrelevant attributes in each episode. The attribute selection mechanism improves the accuracy and helps with interpretability by reducing the number of participated attributes in each episode. We propose a mechanism that automatically detects the episodes where the pool of human-friendly attributes are not adequate, and compensates by engaging learned unknown attributes. We demonstrate that the proposed method achieves results on par with black-box few-shot-learning models on four widely used datasets. "
}