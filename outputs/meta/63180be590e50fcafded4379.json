{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Neural Network Generalization"
  ],
  "datasets": [
    "ImageNet"
  ],
  "methods": [
    "Sample Generalization",
    "Distribution Generalization",
    "Domain Generalization",
    "Task Generalization",
    "Modality Generalization",
    "Scope Generalization"
  ],
  "results": [
    "Training error reduction",
    "Overfitting reduction",
    "Transfer learning",
    "Few-shot meta-learning",
    "BERT NLP engine",
    "Biologically-inspired network",
    "Knowledge graph integration"
  ],
  "paper_id": "63180be590e50fcafded4379",
  "title": "Generalization in Neural Networks: A Broad Survey",
  "abstract": "  This paper reviews concepts, modeling approaches, and recent findings along a spectrum of different levels of abstraction of neural network models including generalization across (1) Samples, (2) Distributions, (3) Domains, (4) Tasks, (5) Modalities, and (6) Scopes. Results on (1) sample generalization show that, in the case of ImageNet, nearly all the recent improvements reduced training error while overfitting stayed flat; with nearly all the training error eliminated, future progress will require a focus on reducing overfitting. Perspectives from statistics highlight how (2) distribution generalization can be viewed alternately as a change in sample weights or a change in the input-output relationship; thus, techniques that have been successful in domain generalization have the potential to be applied to difficult forms of sample or distribution generalization. Transfer learning approaches to (3) domain generalization are summarized, as are recent advances and the wealth of domain adaptation benchmark datasets available. Recent breakthroughs surveyed in (4) task generalization include few-shot meta-learning approaches and the BERT NLP engine, and recent (5) modality generalization studies are discussed that integrate image and text data and that apply a biologically-inspired network across olfactory, visual, and auditory modalities. Recent (6) scope generalization results are reviewed that embed knowledge graphs into deep NLP approaches. Additionally, concepts from neuroscience are discussed on the modular architecture of brains and the steps by which dopamine-driven conditioning leads to abstract thinking. "
}