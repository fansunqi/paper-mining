{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Maintenance Planning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Deep Reinforcement Learning (DRL)",
    "Deep Q-learning (DQN)",
    "Conservative Q-learning"
  ],
  "results": [
    "DRL-based policies improve over standard preventive, corrective, and greedy planning alternatives",
    "Learning from the fixed DQN replay dataset in an offline setting further improves the performance"
  ],
  "paper_id": "62ea18e05aee126c0fca2ad4",
  "title": "A Maintenance Planning Framework using Online and Offline Deep\n  Reinforcement Learning",
  "abstract": "  Cost-effective asset management is an area of interest across several industries. Specifically, this paper develops a deep reinforcement learning (DRL) solution to automatically determine an optimal rehabilitation policy for continuously deteriorating water pipes. We approach the problem of rehabilitation planning in an online and offline DRL setting. In online DRL, the agent interacts with a simulated environment of multiple pipes with distinct lengths, materials, and failure rate characteristics. We train the agent using deep Q-learning (DQN) to learn an optimal policy with minimal average costs and reduced failure probability. In offline learning, the agent uses static data, e.g., DQN replay data, to learn an optimal policy via a conservative Q-learning algorithm without further interactions with the environment. We demonstrate that DRL-based policies improve over standard preventive, corrective, and greedy planning alternatives. Additionally, learning from the fixed DQN replay dataset in an offline setting further improves the performance. The results warrant that the existing deterioration profiles of water pipes consisting of large and diverse states and action trajectories provide a valuable avenue to learn rehabilitation policies in the offline setting, which can be further fine-tuned using the simulator. "
}