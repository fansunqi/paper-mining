{
  "code_links": [
    "None"
  ],
  "tasks": [
    "online stochastic optimization",
    "distributed learning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "communication-efficient adaptive algorithm"
  ],
  "results": [
    "order-optimal cumulative regret",
    "low communication cost"
  ],
  "paper_id": "64800067d68f896efaa0efcc",
  "title": "A Communication-Efficient Adaptive Algorithm for Federated Learning\n  under Cumulative Regret",
  "abstract": "  We consider the problem of online stochastic optimization in a distributed setting with $M$ clients connected through a central server. We develop a distributed online learning algorithm that achieves order-optimal cumulative regret with low communication cost measured in the total number of bits transmitted over the entire learning horizon. This is in contrast to existing studies which focus on the offline measure of simple regret for learning efficiency. The holistic measure for communication cost also departs from the prevailing approach that \\emph{separately} tackles the communication frequency and the number of bits in each communication round. "
}