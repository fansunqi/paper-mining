{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Investigation of contraction coefficients for $E_\\gamma$-divergence",
    "Privacy analysis in local differential privacy",
    "Privacy guarantees of online algorithms"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Generalization of Dobrushin's coefficient to $E_\\gamma$-divergence",
    "Closed-form expression for contraction of $E_\\gamma$-divergence",
    "Information-theoretic technique for privacy analysis",
    "Composition of amplitude-constrained Gaussian channels"
  ],
  "results": [
    "Local differential privacy expressed in terms of $E_\\gamma$-divergence contraction",
    "Quantification of local privacy impact in estimation and hypothesis testing",
    "New technique for analyzing privacy guarantees of online algorithms",
    "Differential privacy parameters of gradient descent derived"
  ],
  "paper_id": "5fe1d3b891e0119a161ede00",
  "title": "Contraction of $E_\\gamma$-Divergence and Its Applications to Privacy",
  "abstract": "  We investigate the contraction coefficients derived from strong data processing inequalities for the $E_\\gamma$-divergence. By generalizing the celebrated Dobrushin's coefficient from total variation distance to $E_\\gamma$-divergence, we derive a closed-form expression for the contraction of $E_\\gamma$-divergence. This result has fundamental consequences in two privacy settings. First, it implies that local differential privacy can be equivalently expressed in terms of the contraction of $E_\\gamma$-divergence. This equivalent formula can be used to precisely quantify the impact of local privacy in (Bayesian and minimax) estimation and hypothesis testing problems in terms of the reduction of effective sample size. Second, it leads to a new information-theoretic technique for analyzing privacy guarantees of online algorithms. In this technique, we view such algorithms as a composition of amplitude-constrained Gaussian channels and then relate their contraction coefficients under $E_\\gamma$-divergence to the overall differential privacy guarantees. As an example, we apply our technique to derive the differential privacy parameters of gradient descent. Moreover, we also show that this framework can be tailored to batch learning algorithms that can be implemented with one pass over the training dataset. "
}