{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Byzantine-robust decentralized training"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "ClippedGossip algorithm for Byzantine-robust consensus and optimization"
  ],
  "results": [
    "Provably converges to a $O(\\delta_{\\max}\\zeta^2/\\gamma^2)$ neighborhood of the stationary point for non-convex objectives",
    "Encouraging empirical performance under a large number of attacks"
  ],
  "paper_id": "61fc99475aee126c0fcdcd8a",
  "title": "Byzantine-Robust Decentralized Learning via ClippedGossip",
  "abstract": "  In this paper, we study the challenging task of Byzantine-robust decentralized training on arbitrary communication graphs. Unlike federated learning where workers communicate through a server, workers in the decentralized environment can only talk to their neighbors, making it harder to reach consensus and benefit from collaborative training. To address these issues, we propose a ClippedGossip algorithm for Byzantine-robust consensus and optimization, which is the first to provably converge to a $O(\\delta_{\\max}\\zeta^2/\\gamma^2)$ neighborhood of the stationary point for non-convex objectives under standard assumptions. Finally, we demonstrate the encouraging empirical performance of ClippedGossip under a large number of attacks. "
}