{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Training of convolutional neural networks"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Newton methods for convolutional neural networks",
    "Complete data instead of sub-sampled",
    "Parallel processing in mini-batch computations"
  ],
  "results": [
    "Outperforms the time taken by the previous approach"
  ],
  "paper_id": "61a98aff5244ab9dcb955e12",
  "title": "Newton methods based convolution neural networks using parallel\n  processing",
  "abstract": "  Training of convolutional neural networks is a high dimensional and a non-convex optimization problem. At present, it is inefficient in situations where parametric learning rates can not be confidently set. Some past works have introduced Newton methods for training deep neural networks. Newton methods for convolutional neural networks involve complicated operations. Finding the Hessian matrix in second-order methods becomes very complex as we mainly use the finite differences method with the image data. Newton methods for convolutional neural networks deals with this by using the sub-sampled Hessian Newton methods. In this paper, we have used the complete data instead of the sub-sampled methods that only handle partial data at a time. Further, we have used parallel processing instead of serial processing in mini-batch computations. The results obtained using parallel processing in this study, outperform the time taken by the previous approach. "
}