{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Modeling (semi) structured object sequences"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Temporal Value Modeling (TVM)",
    "Key Aggregation (KA)"
  ],
  "results": [
    "Better performance than flattening sequence objects",
    "Ability to operate on significantly larger sequences than existing methods"
  ],
  "paper_id": "63b63fd190e50fcafd8f578e",
  "title": "Semi-Structured Object Sequence Encoders",
  "abstract": "  In this paper we explore the task of modeling (semi) structured object sequences; in particular we focus our attention on the problem of developing a structure-aware input representation for such sequences. In such sequences, we assume that each structured object is represented by a set of key-value pairs which encode the attributes of the structured object. Given a universe of keys, a sequence of structured objects can then be viewed as an evolution of the values for each key, over time. We encode and construct a sequential representation using the values for a particular key (Temporal Value Modeling - TVM) and then self-attend over the set of key-conditioned value sequences to a create a representation of the structured object sequence (Key Aggregation - KA). We pre-train and fine-tune the two components independently and present an innovative training schedule that interleaves the training of both modules with shared attention heads. We find that this iterative two part-training results in better performance than a unified network with hierarchical encoding as well as over, other methods that use a {\\em record-view} representation of the sequence \\cite{de2021transformers4rec} or a simple {\\em flattened} representation of the sequence. We conduct experiments using real-world data to demonstrate the advantage of interleaving TVM-KA on multiple tasks and detailed ablation studies motivating our modeling choices. We find that our approach performs better than flattening sequence objects and also allows us to operate on significantly larger sequences than existing methods. "
}