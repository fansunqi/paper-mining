{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Minimizing convex function under distributional drift",
    "Concept drift",
    "Stochastic tracking",
    "Performative prediction"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Stochastic algorithms with iterate averaging",
    "Proximal stochastic gradient method with step decay schedule"
  ],
  "results": [
    "Non-asymptotic convergence guarantees",
    "Decoupled contributions of optimization error, gradient noise, and time drift",
    "Improved tracking efficiency in low drift-to-noise regime"
  ],
  "paper_id": "611c81a45244ab9dcb5bfa3c",
  "title": "Stochastic Optimization under Distributional Drift",
  "abstract": "  We consider the problem of minimizing a convex function that is evolving according to unknown and possibly stochastic dynamics, which may depend jointly on time and on the decision variable itself. Such problems abound in the machine learning and signal processing literature, under the names of concept drift, stochastic tracking, and performative prediction. We provide novel non-asymptotic convergence guarantees for stochastic algorithms with iterate averaging, focusing on bounds valid both in expectation and with high probability. The efficiency estimates we obtain clearly decouple the contributions of optimization error, gradient noise, and time drift. Notably, we identify a low drift-to-noise regime in which the tracking efficiency of the proximal stochastic gradient method benefits significantly from a step decay schedule. Numerical experiments illustrate our results. "
}