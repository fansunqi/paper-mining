{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Surveillance-evading path-planning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Bayesian reinforcement learning",
    "Gaussian Process regression",
    "Hamilton-Jacobi PDEs",
    "Confidence Bounds"
  ],
  "results": [
    "Significant advantages over traditional graph-based algorithms",
    "Highlighted by numerical experiments and regret metrics"
  ],
  "paper_id": "61567c655244ab9dcb07f080",
  "title": "Surveillance Evasion Through Bayesian Reinforcement Learning",
  "abstract": "  We consider a task of surveillance-evading path-planning in a continuous setting. An Evader strives to escape from a 2D domain while minimizing the risk of detection (and immediate capture). The probability of detection is path-dependent and determined by the spatially inhomogeneous surveillance intensity, which is fixed but a priori unknown and gradually learned in the multi-episodic setting. We introduce a Bayesian reinforcement learning algorithm that relies on a Gaussian Process regression (to model the surveillance intensity function based on the information from prior episodes), numerical methods for Hamilton-Jacobi PDEs (to plan the best continuous trajectories based on the current model), and Confidence Bounds (to balance the exploration vs exploitation). We use numerical experiments and regret metrics to highlight the significant advantages of our approach compared to traditional graph-based algorithms of reinforcement learning. "
}