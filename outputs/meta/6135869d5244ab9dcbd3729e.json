{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Concept-based Explanation",
    "Model Interpretability"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Neighbor Shapley",
    "Instance-wise Explanation",
    "Class-wise Explanation"
  ],
  "results": [
    "None"
  ],
  "paper_id": "6135869d5244ab9dcbd3729e",
  "title": "Instance-wise or Class-wise? A Tale of Neighbor Shapley for\n  Concept-based Explanation",
  "abstract": "  Deep neural networks have demonstrated remarkable performance in many data-driven and prediction-oriented applications, and sometimes even perform better than humans. However, their most significant drawback is the lack of interpretability, which makes them less attractive in many real-world applications. When relating to the moral problem or the environmental factors that are uncertain such as crime judgment, financial analysis, and medical diagnosis, it is essential to mine the evidence for the model's prediction (interpret model knowledge) to convince humans. Thus, investigating how to interpret model knowledge is of paramount importance for both academic research and real applications. "
}