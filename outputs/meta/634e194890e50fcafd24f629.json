{
  "code_links": [
    "https://github.com/ENSTA-U2IS/torch-uncertainty"
  ],
  "tasks": [
    "Uncertainty Estimation",
    "Out-of-distribution Detection",
    "Robustness to Distribution Shift"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Packed-Ensembles",
    "Grouped Convolutions"
  ],
  "results": [
    "Preserves properties of Deep Ensembles",
    "Equal performance in accuracy, calibration, out-of-distribution detection, and robustness"
  ],
  "paper_id": "634e194890e50fcafd24f629",
  "title": "Packed-Ensembles for Efficient Uncertainty Estimation",
  "abstract": "  Deep Ensembles (DE) are a prominent approach for achieving excellent performance on key metrics such as accuracy, calibration, uncertainty estimation, and out-of-distribution detection. However, hardware limitations of real-world systems constrain to smaller ensembles and lower-capacity networks, significantly deteriorating their performance and properties. We introduce Packed-Ensembles (PE), a strategy to design and train lightweight structured ensembles by carefully modulating the dimension of their encoding space. We leverage grouped convolutions to parallelize the ensemble into a single shared backbone and forward pass to improve training and inference speeds. PE is designed to operate within the memory limits of a standard neural network. Our extensive research indicates that PE accurately preserves the properties of DE, such as diversity, and performs equally well in terms of accuracy, calibration, out-of-distribution detection, and robustness to distribution shift. We make our code available at https://github.com/ENSTA-U2IS/torch-uncertainty. "
}