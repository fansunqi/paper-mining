{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Bayesian inference",
    "Sampling high-dimensional probability distributions"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Stein variational gradient descent",
    "Iterated steepest descent with reproducing kernel Hilbert space norm",
    "Nondifferentiable kernels with adjusted tails"
  ],
  "results": [
    "Significant performance gains in numerical experiments"
  ],
  "paper_id": "5de632593a55ac4f55c25762",
  "title": "On the geometry of Stein variational gradient descent",
  "abstract": "  Bayesian inference problems require sampling or approximating high-dimensional probability distributions. The focus of this paper is on the recently introduced Stein variational gradient descent methodology, a class of algorithms that rely on iterated steepest descent steps with respect to a reproducing kernel Hilbert space norm. This construction leads to interacting particle systems, the mean-field limit of which is a gradient flow on the space of probability distributions equipped with a certain geometrical structure. We leverage this viewpoint to shed some light on the convergence properties of the algorithm, in particular addressing the problem of choosing a suitable positive definite kernel function. Our analysis leads us to considering certain nondifferentiable kernels with adjusted tails. We demonstrate significant performance gains of these in various numerical experiments. "
}