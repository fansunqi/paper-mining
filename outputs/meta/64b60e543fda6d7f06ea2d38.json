{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Audio-Visual Classification",
    "Audio-Visual Retrieval"
  ],
  "datasets": [
    "AudioSet",
    "VGGSound"
  ],
  "methods": [
    "Masked Audio-Video Learners",
    "Self-supervised Learning",
    "Contrastive Learning",
    "Reconstruction"
  ],
  "results": [
    "AudioSet: 53.1 mAP",
    "VGGSound: 67.1% accuracy"
  ],
  "paper_id": "64b60e543fda6d7f06ea2d38",
  "title": "MAViL: Masked Audio-Video Learners",
  "abstract": "  We present Masked Audio-Video Learners (MAViL) to train audio-visual representations. Our approach learns with three complementary forms of self-supervision: (1) reconstruction of masked audio and video input data, (2) intra- and inter-modal contrastive learning with masking, and (3) self-training by reconstructing joint audio-video contextualized features learned from the first two objectives. Pre-training with MAViL not only enables the model to perform well in audio-visual classification and retrieval tasks but also improves representations of each modality in isolation, without using information from the other modality for fine-tuning or inference. Empirically, MAViL sets a new state-of-the-art on AudioSet (53.1 mAP) and VGGSound (67.1% accuracy). For the first time, a self-supervised audio-visual model outperforms ones that use external supervision on these benchmarks. "
}