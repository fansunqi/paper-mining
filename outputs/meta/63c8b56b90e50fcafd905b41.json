{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Team competition in multi-agent Markov games",
    "Multi-agent reinforcement learning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "RAC",
    "actor-critic framework with role encoder and opponent role predictors"
  ],
  "results": [
    "Higher rewards than state-of-the-art baselines",
    "Agents learn diverse and opponent-aware policies"
  ],
  "paper_id": "63c8b56b90e50fcafd905b41",
  "title": "Opponent-aware Role-based Learning in Team Competitive Markov Games",
  "abstract": "  Team competition in multi-agent Markov games is an increasingly important setting for multi-agent reinforcement learning, due to its general applicability in modeling many real-life situations. Multi-agent actor-critic methods are the most suitable class of techniques for learning optimal policies in the team competition setting, due to their flexibility in learning agent-specific critic functions, which can also learn from other agents. In many real-world team competitive scenarios, the roles of the agents naturally emerge, in order to aid in coordination and collaboration within members of the teams. However, existing methods for learning emergent roles rely heavily on the Q-learning setup which does not allow learning of agent-specific Q-functions. In this paper, we propose RAC, a novel technique for learning the emergent roles of agents within a team that are diverse and dynamic. In the proposed method, agents also benefit from predicting the roles of the agents in the opponent team. RAC uses the actor-critic framework with role encoder and opponent role predictors for learning an optimal policy. Experimentation using 2 games demonstrates that the policies learned by RAC achieve higher rewards than those learned using state-of-the-art baselines. Moreover, experiments suggest that the agents in a team learn diverse and opponent-aware policies. "
}