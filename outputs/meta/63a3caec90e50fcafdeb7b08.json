{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Neural image classifiers training",
    "Text-to-Image (T2I) generators for robust representations"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Text-to-Image (T2I) generators",
    "Prompt-based interventions"
  ],
  "results": [
    "State-of-the-art performance on Single-Domain Generalization benchmarks",
    "Reduced dependency on spurious features",
    "Facilitated Multi-Domain Generalization techniques"
  ],
  "paper_id": "63a3caec90e50fcafdeb7b08",
  "title": "Not Just Pretty Pictures: Text-to-Image Generators Enable Interpretable\n  Interventions for Robust Representations",
  "abstract": "  Neural image classifiers are known to undergo severe performance degradation when exposed to input that exhibits covariate shift with respect to the training distribution. In this paper, we show that recent Text-to-Image (T2I) generators' ability to edit images to approximate interventions via natural-language prompts is a promising technology to train more robust classifiers. Using current open-source models, we find that a variety of prompting strategies are effective for producing augmented training datasets sufficient to achieve state-of-the-art performance (1) in widely adopted Single-Domain Generalization benchmarks, (2) in reducing classifiers' dependency on spurious features and (3) facilitating the application of Multi-Domain Generalization techniques when fewer training domains are available. "
}