{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Multi-objective Policy Optimization",
    "Offline RL",
    "Finetuning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Distillation of a Mixture of Experts (DiME)",
    "Multi-objective (MO) optimization",
    "Linear scalarization"
  ],
  "results": [
    "DiME outperforms linear scalarization",
    "For offline RL, DiME leads to a new algorithm that outperforms state-of-the-art",
    "For finetuning, new algorithms learn to outperform the teacher policy"
  ],
  "paper_id": "64c9d4b03fda6d7f06370520",
  "title": "On Multi-objective Policy Optimization as a Tool for Reinforcement\n  Learning: Case Studies in Offline RL and Finetuning",
  "abstract": "  Many advances that have improved the robustness and efficiency of deep reinforcement learning (RL) algorithms can, in one way or another, be understood as introducing additional objectives or constraints in the policy optimization step. This includes ideas as far ranging as exploration bonuses, entropy regularization, and regularization toward teachers or data priors. Often, the task reward and auxiliary objectives are in conflict, and in this paper we argue that this makes it natural to treat these cases as instances of multi-objective (MO) optimization problems. We demonstrate how this perspective allows us to develop novel and more effective RL algorithms. In particular, we focus on offline RL and finetuning as case studies, and show that existing approaches can be understood as MO algorithms relying on linear scalarization. We hypothesize that replacing linear scalarization with a better algorithm can improve performance. We introduce Distillation of a Mixture of Experts (DiME), a new MORL algorithm that outperforms linear scalarization and can be applied to these non-standard MO problems. We demonstrate that for offline RL, DiME leads to a simple new algorithm that outperforms state-of-the-art. For finetuning, we derive new algorithms that learn to outperform the teacher policy. "
}