{
  "code_links": [
    "https://github.com/google/belief-localization"
  ],
  "tasks": [
    "Knowledge editing in language models",
    "Representation denoising (Causal Tracing)"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Weight editing",
    "Representation denoising (Causal Tracing)",
    "Editing problem variants"
  ],
  "results": [
    "Localization conclusions from representation denoising do not predict editing performance",
    "Which layer to edit is a better predictor of performance than localization results"
  ],
  "paper_id": "63bf7a6990e50fcafd885aa7",
  "title": "Does Localization Inform Editing? Surprising Differences in\n  Causality-Based Localization vs. Knowledge Editing in Language Models",
  "abstract": "  Language models are known to learn a great quantity of factual information during pretraining, and recent work localizes this information to specific model weights like mid-layer MLP weights (Meng et al., 2022). In this paper, we find that we can change how a fact is stored in a model by editing weights that are in a different location than where existing methods suggest that the fact is stored. This is surprising because we would expect that localizing facts to specific parameters in models would tell us where to manipulate knowledge in models, and this assumption has motivated past work on model editing methods. Specifically, we show that localization conclusions from representation denoising (also known as Causal Tracing) do not provide any insight into which model MLP layer would be best to edit in order to override an existing stored fact with a new one. This finding raises questions about how past work relies on Causal Tracing to select which model layers to edit (Meng et al., 2022). Next, to better understand the discrepancy between representation denoising and weight editing, we develop several variants of the editing problem that appear more and more like representation denoising in their design and objective. Experiments show that, for one of our editing problems, editing performance does relate to localization results from representation denoising, but we find that which layer we edit is a far better predictor of performance. Our results suggest, counterintuitively, that better mechanistic understanding of how pretrained language models work may not always translate to insights about how to best change their behavior. Code is available at: https://github.com/google/belief-localization "
}