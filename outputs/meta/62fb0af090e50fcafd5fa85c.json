{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Domain Invariant Supervised Representation Learning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Causal structure analysis",
    "Invariant structure learning"
  ],
  "results": [
    "None"
  ],
  "paper_id": "62fb0af090e50fcafd5fa85c",
  "title": "The Causal Structure of Domain Invariant Supervised Representation\n  Learning",
  "abstract": "  Machine learning methods can be unreliable when deployed in domains that differ from the domains on which they were trained. There are a wide range of proposals for mitigating this problem by learning representations that are ``invariant'' in some sense.However, these methods generally contradict each other, and none of them consistently improve performance on real-world domain shift benchmarks. There are two main questions that must be addressed to understand when, if ever, we should use each method. First, how does each ad hoc notion of ``invariance'' relate to the structure of real-world problems? And, second, when does learning invariant representations actually yield robust models? To address these issues, we introduce a broad formal notion of what it means for a real-world domain shift to admit invariant structure. Then, we characterize the causal structures that are compatible with this notion of invariance.With this in hand, we find conditions under which method-specific invariance notions correspond to real-world invariant structure, and we clarify the relationship between invariant structure and robustness to domain shifts. For both questions, we find that the true underlying causal structure of the data plays a critical role. "
}