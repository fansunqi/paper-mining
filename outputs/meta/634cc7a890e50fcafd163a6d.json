{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Vehicle Routing Problems"
  ],
  "datasets": [
    "TSPLIB",
    "CVRPLIB"
  ],
  "methods": [
    "Knowledge Distillation",
    "Adaptive Multi-Distribution Knowledge Distillation (AMDKD)"
  ],
  "results": [
    "Competitive results on both unseen in-distribution and out-of-distribution instances",
    "Less computational resources for inference"
  ],
  "paper_id": "634cc7a890e50fcafd163a6d",
  "title": "Learning Generalizable Models for Vehicle Routing Problems via Knowledge\n  Distillation",
  "abstract": "  Recent neural methods for vehicle routing problems always train and test the deep models on the same instance distribution (i.e., uniform). To tackle the consequent cross-distribution generalization concerns, we bring the knowledge distillation to this field and propose an Adaptive Multi-Distribution Knowledge Distillation (AMDKD) scheme for learning more generalizable deep models. Particularly, our AMDKD leverages various knowledge from multiple teachers trained on exemplar distributions to yield a light-weight yet generalist student model. Meanwhile, we equip AMDKD with an adaptive strategy that allows the student to concentrate on difficult distributions, so as to absorb hard-to-master knowledge more effectively. Extensive experimental results show that, compared with the baseline neural methods, our AMDKD is able to achieve competitive results on both unseen in-distribution and out-of-distribution instances, which are either randomly synthesized or adopted from benchmark datasets (i.e., TSPLIB and CVRPLIB). Notably, our AMDKD is generic, and consumes less computational resources for inference. "
}