{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Neural Networks Adversarial Attack"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "ExploreADV",
    "DeepFool",
    "Brendel&Bethge Attack",
    "mask-constrained adversarial attack"
  ],
  "results": [
    "None"
  ],
  "paper_id": "63b63fd190e50fcafd8f596d",
  "title": "ExploreADV: Towards exploratory attack for Neural Networks",
  "abstract": "  Although deep learning has made remarkable progress in processing various types of data such as images, text and speech, they are known to be susceptible to adversarial perturbations: perturbations specifically designed and added to the input to make the target model produce erroneous output. Most of the existing studies on generating adversarial perturbations attempt to perturb the entire input indiscriminately. In this paper, we propose ExploreADV, a general and flexible adversarial attack system that is capable of modeling regional and imperceptible attacks, allowing users to explore various kinds of adversarial examples as needed. We adapt and combine two existing boundary attack methods, DeepFool and Brendel\\&Bethge Attack, and propose a mask-constrained adversarial attack system, which generates minimal adversarial perturbations under the pixel-level constraints, namely ``mask-constraints''. We study different ways of generating such mask-constraints considering the variance and importance of the input features, and show that our adversarial attack system offers users good flexibility to focus on sub-regions of inputs, explore imperceptible perturbations and understand the vulnerability of pixels/regions to adversarial attacks. We demonstrate our system to be effective based on extensive experiments and user study. "
}