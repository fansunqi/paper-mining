{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Collaborative learning among distributed clients"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Kernel-based bandit framework",
    "Surrogate Gaussian process (GP) models"
  ],
  "results": [
    "Order-optimal regret performance",
    "Sparse approximations to reduce communication overhead"
  ],
  "paper_id": "62d620f45aee126c0fad3dcb",
  "title": "Collaborative Learning in Kernel-based Bandits for Distributed Users",
  "abstract": "  We study collaborative learning among distributed clients facilitated by a central server. Each client is interested in maximizing a personalized objective function that is a weighted sum of its local objective and a global objective. Each client has direct access to random bandit feedback on its local objective, but only has a partial view of the global objective and relies on information exchange with other clients for collaborative learning. We adopt the kernel-based bandit framework where the objective functions belong to a reproducing kernel Hilbert space. We propose an algorithm based on surrogate Gaussian process (GP) models and establish its order-optimal regret performance (up to polylogarithmic factors). We also show that the sparse approximations of the GP models can be employed to reduce the communication overhead across clients. "
}