{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Unsupervised Mandarin-Cantonese Machine Translation"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Character-based tokenization",
    "Transformer architecture"
  ],
  "results": [
    "Character-level BLEU: 25.1 (Mandarin to Cantonese)",
    "Character-level BLEU: 24.4 (Cantonese to Mandarin)"
  ],
  "paper_id": "63be28d490e50fcafdf52dfc",
  "title": "Unsupervised Mandarin-Cantonese Machine Translation",
  "abstract": "  Advancements in unsupervised machine translation have enabled the development of machine translation systems that can translate between languages for which there is not an abundance of parallel data available. We explored unsupervised machine translation between Mandarin Chinese and Cantonese. Despite the vast number of native speakers of Cantonese, there is still no large-scale corpus for the language, due to the fact that Cantonese is primarily used for oral communication. The key contributions of our project include: 1. The creation of a new corpus containing approximately 1 million Cantonese sentences, and 2. A large-scale comparison across different model architectures, tokenization schemes, and embedding structures. Our best model trained with character-based tokenization and a Transformer architecture achieved a character-level BLEU of 25.1 when translating from Mandarin to Cantonese and of 24.4 when translating from Cantonese to Mandarin. In this paper we discuss our research process, experiments, and results. "
}