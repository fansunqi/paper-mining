{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Screen correspondence",
    "UI understanding"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Machine learning",
    "Element semantics",
    "Appearance",
    "Text"
  ],
  "results": [
    "Improves upon baselines",
    "Facilitates better UI understanding"
  ],
  "paper_id": "63cdfab690e50fcafd106e4d",
  "title": "Screen Correspondence: Mapping Interchangeable Elements between UIs",
  "abstract": "  Understanding user interface (UI) functionality is a useful yet challenging task for both machines and people. In this paper, we investigate a machine learning approach for screen correspondence, which allows reasoning about UIs by mapping their elements onto previously encountered examples with known functionality and properties. We describe and implement a model that incorporates element semantics, appearance, and text to support correspondence computation without requiring any labeled examples. Through a comprehensive performance evaluation, we show that our approach improves upon baselines by incorporating multi-modal properties of UIs. Finally, we show three example applications where screen correspondence facilitates better UI understanding for humans and machines: (i) instructional overlay generation, (ii) semantic UI element search, and (iii) automated interface testing. "
}