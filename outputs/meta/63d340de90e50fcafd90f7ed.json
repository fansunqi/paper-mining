{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Explainable AI (XAI)",
    "Selective explanations"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Selective explanation framework",
    "Human input on a small sample"
  ],
  "results": [
    "Reduces over-reliance on AI",
    "Improves decision outcomes",
    "Enhances subjective perceptions of the AI"
  ],
  "paper_id": "63d340de90e50fcafd90f7ed",
  "title": "Selective Explanations: Leveraging Human Input to Align Explainable AI",
  "abstract": "  While a vast collection of explainable AI (XAI) algorithms have been developed in recent years, they are often criticized for significant gaps with how humans produce and consume explanations. As a result, current XAI techniques are often found to be hard to use and lack effectiveness. In this work, we attempt to close these gaps by making AI explanations selective -- a fundamental property of human explanations -- by selectively presenting a subset from a large set of model reasons based on what aligns with the recipient's preferences. We propose a general framework for generating selective explanations by leveraging human input on a small sample. This framework opens up a rich design space that accounts for different selectivity goals, types of input, and more. As a showcase, we use a decision-support task to explore selective explanations based on what the decision-maker would consider relevant to the decision task. We conducted two experimental studies to examine three out of a broader possible set of paradigms based on our proposed framework: in Study 1, we ask the participants to provide their own input to generate selective explanations, with either open-ended or critique-based input. In Study 2, we show participants selective explanations based on input from a panel of similar users (annotators). Our experiments demonstrate the promise of selective explanations in reducing over-reliance on AI and improving decision outcomes and subjective perceptions of the AI, but also paint a nuanced picture that attributes some of these positive effects to the opportunity to provide one's own input to augment AI explanations. Overall, our work proposes a novel XAI framework inspired by human communication behaviors and demonstrates its potentials to encourage future work to better align AI explanations with human production and consumption of explanations. "
}