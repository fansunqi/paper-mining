{
  "code_links": [
    "https://github.com/MCG-NJU/PointTAD"
  ],
  "tasks": [
    "Multi-label Temporal Action Detection"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "PointTAD",
    "Learnable Query Points",
    "Multi-level Interactive Module",
    "End-to-end trainable framework"
  ],
  "results": [
    "Outperforms all previous methods under detection-mAP metric",
    "Achieves promising results under segmentation-mAP metric"
  ],
  "paper_id": "63520de390e50fcafd60ed1a",
  "title": "PointTAD: Multi-Label Temporal Action Detection with Learnable Query\n  Points",
  "abstract": "  Traditional temporal action detection (TAD) usually handles untrimmed videos with small number of action instances from a single label (e.g., ActivityNet, THUMOS). However, this setting might be unrealistic as different classes of actions often co-occur in practice. In this paper, we focus on the task of multi-label temporal action detection that aims to localize all action instances from a multi-label untrimmed video. Multi-label TAD is more challenging as it requires for fine-grained class discrimination within a single video and precise localization of the co-occurring instances. To mitigate this issue, we extend the sparse query-based detection paradigm from the traditional TAD and propose the multi-label TAD framework of PointTAD. Specifically, our PointTAD introduces a small set of learnable query points to represent the important frames of each action instance. This point-based representation provides a flexible mechanism to localize the discriminative frames at boundaries and as well the important frames inside the action. Moreover, we perform the action decoding process with the Multi-level Interactive Module to capture both point-level and instance-level action semantics. Finally, our PointTAD employs an end-to-end trainable framework simply based on RGB input for easy deployment. We evaluate our proposed method on two popular benchmarks and introduce the new metric of detection-mAP for multi-label TAD. Our model outperforms all previous methods by a large margin under the detection-mAP metric, and also achieves promising results under the segmentation-mAP metric. Code is available at https://github.com/MCG-NJU/PointTAD. "
}