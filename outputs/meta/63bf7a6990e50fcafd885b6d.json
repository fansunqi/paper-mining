{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Deep Neural Networks Parallelism"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "TAPS: Topology-Aware Intra-Operator Parallelism strategy Searching algorithm"
  ],
  "results": [
    "Up to 85% fewer communication costs compared to latest baselines"
  ],
  "paper_id": "63bf7a6990e50fcafd885b6d",
  "title": "TAPS: Topology-Aware Intra-Operator Parallelism Strategy Searching\n  Algorithm for Deep Neural Networks",
  "abstract": "  TAPS is a Topology-Aware intra-operator Parallelism strategy Searching algorithm that generates intra-operator parallelism strategies by considering both intra-node and inter-node bandwidth. Most of the existing auto-parallelism works use the communication volume as the communication cost directly when generating strategies, which we prove to be sub-optimal in multi-nodes cases. We design a topology-aware cost model for multi-node intra-operator parallelism strategy searching. Numerical experiments demonstrate that TAPS can generate strategies with up to 85% fewer communication costs, which outperform the latest baselines. "
}