{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Multi-distribution learning paradigms"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Stochastic optimization techniques for solving stochastic zero-sum games",
    "Stochastic Mirror Descent"
  ],
  "results": [
    "Optimal sample complexity bounds",
    "Improvement over known sample complexities"
  ],
  "paper_id": "635753cc90e50fcafddddbea",
  "title": "On-Demand Sampling: Learning Optimally from Multiple Distributions",
  "abstract": "  Social and real-world considerations such as robustness, fairness, social welfare and multi-agent tradeoffs have given rise to multi-distribution learning paradigms, such as collaborative, group distributionally robust, and fair federated learning. In each of these settings, a learner seeks to minimize its worst-case loss over a set of $n$ predefined distributions, while using as few samples as possible. In this paper, we establish the optimal sample complexity of these learning paradigms and give algorithms that meet this sample complexity. Importantly, our sample complexity bounds exceed that of the sample complexity of learning a single distribution only by an additive factor of $n \\log(n) / \\epsilon^2$. These improve upon the best known sample complexity of agnostic federated learning by Mohri et al. by a multiplicative factor of $n$, the sample complexity of collaborative learning by Nguyen and Zakynthinou by a multiplicative factor $\\log n / \\epsilon^3$, and give the first sample complexity bounds for the group DRO objective of Sagawa et al. To achieve optimal sample complexity, our algorithms learn to sample and learn from distributions on demand. Our algorithm design and analysis is enabled by our extensions of stochastic optimization techniques for solving stochastic zero-sum games. In particular, we contribute variants of Stochastic Mirror Descent that can trade off between players' access to cheap one-off samples or more expensive reusable ones. "
}