{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Random feature regression",
    "Overparameterized learning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Random feature ridge regression (RFRR)",
    "Kernel ridge regression (KRR)",
    "Nonlinear random feature map",
    "Hermite polynomial expansion"
  ],
  "results": [
    "Non-asymptotic concentration results for training errors, cross-validations, and generalization errors",
    "Lower bound for the generalization error of RFRR for a nonlinear student-teacher model"
  ],
  "paper_id": "6371b1a090e50fcafdb2e4d0",
  "title": "Overparameterized random feature regression with nearly orthogonal data",
  "abstract": "  We investigate the properties of random feature ridge regression (RFRR) given by a two-layer neural network with random Gaussian initialization. We study the non-asymptotic behaviors of the RFRR with nearly orthogonal deterministic unit-length input data vectors in the overparameterized regime, where the width of the first layer is much larger than the sample size. Our analysis shows high-probability non-asymptotic concentration results for the training errors, cross-validations, and generalization errors of RFRR centered around their respective values for a kernel ridge regression (KRR). This KRR is derived from an expected kernel generated by a nonlinear random feature map. We then approximate the performance of the KRR by a polynomial kernel matrix obtained from the Hermite polynomial expansion of the activation function, whose degree only depends on the orthogonality among different data points. This polynomial kernel determines the asymptotic behavior of the RFRR and the KRR. Our results hold for a wide variety of activation functions and input data sets that exhibit nearly orthogonal properties. Based on these approximations, we obtain a lower bound for the generalization error of the RFRR for a nonlinear student-teacher model. "
}