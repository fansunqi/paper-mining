{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Federated Learning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Network Adaptive Compression (NAC-FL)"
  ],
  "results": [
    "Asymptotically optimal in terms of directly minimizing the expected wall clock training time",
    "Robust performance improvements with higher gains in settings with positively correlated delays across time"
  ],
  "paper_id": "63bf7a6e90e50fcafd8864c8",
  "title": "Network Adaptive Federated Learning: Congestion and Lossy Compression",
  "abstract": "  In order to achieve the dual goals of privacy and learning across distributed data, Federated Learning (FL) systems rely on frequent exchanges of large files (model updates) between a set of clients and the server. As such FL systems are exposed to, or indeed the cause of, congestion across a wide set of network resources. Lossy compression can be used to reduce the size of exchanged files and associated delays, at the cost of adding noise to model updates. By judiciously adapting clients' compression to varying network congestion, an FL application can reduce wall clock training time. To that end, we propose a Network Adaptive Compression (NAC-FL) policy, which dynamically varies the client's lossy compression choices to network congestion variations. We prove, under appropriate assumptions, that NAC-FL is asymptotically optimal in terms of directly minimizing the expected wall clock training time. Further, we show via simulation that NAC-FL achieves robust performance improvements with higher gains in settings with positively correlated delays across time. "
}