{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Anomaly Detection"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Logarithmic Mean Squared Error (LMSE)"
  ],
  "results": [
    "LMSE superior to MSE in loss convergence and anomaly detection performance"
  ],
  "paper_id": "61e781645244ab9dcbf9a22f",
  "title": "Concise Logarithmic Loss Function for Robust Training of Anomaly\n  Detection Model",
  "abstract": "  Recently, deep learning-based algorithms are widely adopted due to the advantage of being able to establish anomaly detection models without or with minimal domain knowledge of the task. Instead, to train the artificial neural network more stable, it should be better to define the appropriate neural network structure or the loss function. For the training anomaly detection model, the mean squared error (MSE) function is adopted widely. On the other hand, the novel loss function, logarithmic mean squared error (LMSE), is proposed in this paper to train the neural network more stable. This study covers a variety of comparisons from mathematical comparisons, visualization in the differential domain for backpropagation, loss convergence in the training process, and anomaly detection performance. In an overall view, LMSE is superior to the existing MSE function in terms of strongness of loss convergence, anomaly detection performance. The LMSE function is expected to be applicable for training not only the anomaly detection model but also the general generative neural network. "
}