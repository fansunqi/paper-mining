{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Offline Reinforcement Learning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Unifying taxonomy for offline RL methods",
    "Comprehensive review of latest algorithmic breakthroughs",
    "Unified notation for reviewing algorithms",
    "Review of existing benchmarks' properties and shortcomings",
    "Summary figure of method performance on different dataset properties"
  ],
  "results": [
    "Identification of open problems in offline RL",
    "Proposal of future research directions"
  ],
  "paper_id": "6221834e5aee126c0f23c17d",
  "title": "A Survey on Offline Reinforcement Learning: Taxonomy, Review, and Open\n  Problems",
  "abstract": "  With the widespread adoption of deep learning, reinforcement learning (RL) has experienced a dramatic increase in popularity, scaling to previously intractable problems, such as playing complex games from pixel observations, sustaining conversations with humans, and controlling robotic agents. However, there is still a wide range of domains inaccessible to RL due to the high cost and danger of interacting with the environment. Offline RL is a paradigm that learns exclusively from static datasets of previously collected interactions, making it feasible to extract policies from large and diverse training datasets. Effective offline RL algorithms have a much wider range of applications than online RL, being particularly appealing for real-world applications, such as education, healthcare, and robotics. In this work, we contribute with a unifying taxonomy to classify offline RL methods. Furthermore, we provide a comprehensive review of the latest algorithmic breakthroughs in the field using a unified notation as well as a review of existing benchmarks' properties and shortcomings. Additionally, we provide a figure that summarizes the performance of each method and class of methods on different dataset properties, equipping researchers with the tools to decide which type of algorithm is best suited for the problem at hand and identify which classes of algorithms look the most promising. Finally, we provide our perspective on open problems and propose future research directions for this rapidly growing field. "
}