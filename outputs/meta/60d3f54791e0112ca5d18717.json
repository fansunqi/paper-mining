{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Active Learning for DNNs on Edge Devices",
    "Classification",
    "Object Detection"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "General task-agnostic framework",
    "Stream submodular maximization"
  ],
  "results": [
    "Outperforms all other methods in both tasks",
    "Runs at practical speed on real devices"
  ],
  "paper_id": "60d3f54791e0112ca5d18717",
  "title": "Active Learning for Deep Neural Networks on Edge Devices",
  "abstract": "  When dealing with deep neural network (DNN) applications on edge devices, continuously updating the model is important. Although updating a model with real incoming data is ideal, using all of them is not always feasible due to limits, such as labeling and communication costs. Thus, it is necessary to filter and select the data to use for training (i.e., active learning) on the device. In this paper, we formalize a practical active learning problem for DNNs on edge devices and propose a general task-agnostic framework to tackle this problem, which reduces it to a stream submodular maximization. This framework is light enough to be run with low computational resources, yet provides solutions whose quality is theoretically guaranteed thanks to the submodular property. Through this framework, we can configure data selection criteria flexibly, including using methods proposed in previous active learning studies. We evaluate our approach on both classification and object detection tasks in a practical setting to simulate a real-life scenario. The results of our study show that the proposed framework outperforms all other methods in both tasks, while running at a practical speed on real devices. "
}