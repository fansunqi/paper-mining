{
  "code_links": [
    "https://github.com/Trustworthy-ML-Lab/CLIP-dissect"
  ],
  "tasks": [
    "Automatic description of neuron representations in deep vision networks"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "CLIP-Dissect",
    "Multimodal vision/language models"
  ],
  "results": [
    "More accurate descriptions than existing methods for last layer neurons",
    "Qualitatively good descriptions for hidden layer neurons",
    "Label all neurons from five layers of ResNet-50 in 4 minutes",
    "More than 10 times faster than existing methods"
  ],
  "paper_id": "626754bb5aee126c0fbccbe5",
  "title": "CLIP-Dissect: Automatic Description of Neuron Representations in Deep\n  Vision Networks",
  "abstract": "  In this paper, we propose CLIP-Dissect, a new technique to automatically describe the function of individual hidden neurons inside vision networks. CLIP-Dissect leverages recent advances in multimodal vision/language models to label internal neurons with open-ended concepts without the need for any labeled data or human examples. We show that CLIP-Dissect provides more accurate descriptions than existing methods for last layer neurons where the ground-truth is available as well as qualitatively good descriptions for hidden layer neurons. In addition, our method is very flexible: it is model agnostic, can easily handle new concepts and can be extended to take advantage of better multimodal models in the future. Finally CLIP-Dissect is computationally efficient and can label all neurons from five layers of ResNet-50 in just 4 minutes, which is more than 10 times faster than existing methods. Our code is available at https://github.com/Trustworthy-ML-Lab/CLIP-dissect. "
}