{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Produce robust and efficient deep neural networks"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Convergent relaxation quantization algorithm (Binary-Relax)",
    "ResNets Ensemble via Feynman-Kac Formalism (EnResNet)",
    "Trade-off loss function for preserving natural accuracy and improving channel sparsity"
  ],
  "results": [
    "No reduction of resistance under weak attacks",
    "Very minor reduction of resistance under strong attacks",
    "High efficiency with robust models"
  ],
  "paper_id": "5f4e209291e011084df59cd9",
  "title": "An Integrated Approach to Produce Robust Models with High Efficiency",
  "abstract": "  Deep Neural Networks (DNNs) needs to be both efficient and robust for practical uses. Quantization and structure simplification are promising ways to adapt DNNs to mobile devices, and adversarial training is the most popular method to make DNNs robust. In this work, we try to obtain both features by applying a convergent relaxation quantization algorithm, Binary-Relax (BR), to a robust adversarial-trained model, ResNets Ensemble via Feynman-Kac Formalism (EnResNet). We also discover that high precision, such as ternary (tnn) and 4-bit, quantization will produce sparse DNNs. However, this sparsity is unstructured under advarsarial training. To solve the problems that adversarial training jeopardizes DNNs' accuracy on clean images and the struture of sparsity, we design a trade-off loss function that helps DNNs preserve their natural accuracy and improve the channel sparsity. With our trade-off loss function, we achieve both goals with no reduction of resistance under weak attacks and very minor reduction of resistance under strong attcks. Together with quantized EnResNet with trade-off loss function, we provide robust models that have high efficiency. "
}