{
  "code_links": [
    "https://sites.google.com/view/tax-pose/home"
  ],
  "tasks": [
    "Robot Manipulation"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Cross-Pose Estimation",
    "Vision-Based System",
    "Motion Planner"
  ],
  "results": [
    "State-of-the-art performance in simulated and real-world experiments",
    "Generalizes to unseen objects after training on 10 demonstrations"
  ],
  "paper_id": "6376f7df90e50fcafdb9c252",
  "title": "TAX-Pose: Task-Specific Cross-Pose Estimation for Robot Manipulation",
  "abstract": "  How do we imbue robots with the ability to efficiently manipulate unseen objects and transfer relevant skills based on demonstrations? End-to-end learning methods often fail to generalize to novel objects or unseen configurations. Instead, we focus on the task-specific pose relationship between relevant parts of interacting objects. We conjecture that this relationship is a generalizable notion of a manipulation task that can transfer to new objects in the same category; examples include the relationship between the pose of a pan relative to an oven or the pose of a mug relative to a mug rack. We call this task-specific pose relationship \"cross-pose\" and provide a mathematical definition of this concept. We propose a vision-based system that learns to estimate the cross-pose between two objects for a given manipulation task using learned cross-object correspondences. The estimated cross-pose is then used to guide a downstream motion planner to manipulate the objects into the desired pose relationship (placing a pan into the oven or the mug onto the mug rack). We demonstrate our method's capability to generalize to unseen objects, in some cases after training on only 10 demonstrations in the real world. Results show that our system achieves state-of-the-art performance in both simulated and real-world experiments across a number of tasks. Supplementary information and videos can be found at https://sites.google.com/view/tax-pose/home. "
}