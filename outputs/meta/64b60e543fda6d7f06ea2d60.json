{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Complex Reasoning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Unified learning framework",
    "Structured queries",
    "Contextual representations",
    "Step-by-step reasoning"
  ],
  "results": [
    "Significant improvements in complex reasoning tasks",
    "Transferability to downstream tasks",
    "Effectiveness for complex reasoning of KGs modality"
  ],
  "paper_id": "64b60e543fda6d7f06ea2d60",
  "title": "Unifying Structure Reasoning and Language Model Pre-training for Complex\n  Reasoning",
  "abstract": "  Recent pre-trained language models (PLMs) equipped with foundation reasoning skills have shown remarkable performance on downstream complex tasks. However, the significant structure reasoning skill has been rarely studied, which involves modeling implicit structure information within the text and performing explicit logical reasoning over them to deduce the conclusion. This paper proposes a unified learning framework that combines explicit structure reasoning and language pre-training to endow PLMs with the structure reasoning skill. It first identifies several elementary structures within contexts to construct structured queries and performs step-by-step reasoning along the queries to identify the answer entity. The fusion of textual semantics and structure reasoning is achieved by using contextual representations learned by PLMs to initialize the representation space of structures, and performing stepwise reasoning on this semantic representation space. Experimental results on four datasets demonstrate that the proposed model achieves significant improvements in complex reasoning tasks involving diverse structures, and shows transferability to downstream tasks with limited training data and effectiveness for complex reasoning of KGs modality. "
}