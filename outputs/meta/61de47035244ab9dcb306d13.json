{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Deep Multi-Task Learning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Unitary Scalarization with standard regularization and stabilization techniques"
  ],
  "results": [
    "Matches or improves upon the performance of complex multi-task optimizers",
    "Many specialized multi-task optimizers can be interpreted as forms of regularization"
  ],
  "paper_id": "61de47035244ab9dcb306d13",
  "title": "In Defense of the Unitary Scalarization for Deep Multi-Task Learning",
  "abstract": "  Recent multi-task learning research argues against unitary scalarization, where training simply minimizes the sum of the task losses. Several ad-hoc multi-task optimization algorithms have instead been proposed, inspired by various hypotheses about what makes multi-task settings difficult. The majority of these optimizers require per-task gradients, and introduce significant memory, runtime, and implementation overhead. We show that unitary scalarization, coupled with standard regularization and stabilization techniques from single-task learning, matches or improves upon the performance of complex multi-task optimizers in popular supervised and reinforcement learning settings. We then present an analysis suggesting that many specialized multi-task optimizers can be partly interpreted as forms of regularization, potentially explaining our surprising results. We believe our results call for a critical reevaluation of recent research in the area. "
}