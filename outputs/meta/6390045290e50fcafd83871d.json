{
  "code_links": [
    "https://github.com/astra-vision/PODA"
  ],
  "tasks": [
    "Zero-shot Domain Adaptation"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Prompt-driven",
    "Contrastive Vision-Language Model (CLIP)",
    "Affine Transformations"
  ],
  "results": [
    "Significantly outperforms CLIP-based style transfer baselines",
    "Outperforms one-shot unsupervised domain adaptation on some datasets",
    "Comparable results on others"
  ],
  "paper_id": "6390045290e50fcafd83871d",
  "title": "P{\\O}DA: Prompt-driven Zero-shot Domain Adaptation",
  "abstract": "  Domain adaptation has been vastly investigated in computer vision but still requires access to target images at train time, which might be intractable in some uncommon conditions. In this paper, we propose the task of `Prompt-driven Zero-shot Domain Adaptation', where we adapt a model trained on a source domain using only a single general textual description of the target domain, i.e., a prompt. First, we leverage a pretrained contrastive vision-language model (CLIP) to optimize affine transformations of source features, steering them towards target text embeddings, while preserving their content and semantics. Second, we show that augmented features can be used to perform zero-shot domain adaptation for semantic segmentation. Experiments demonstrate that our method significantly outperforms CLIP-based style transfer baselines on several datasets for the downstream task at hand. Our prompt-driven approach even outperforms one-shot unsupervised domain adaptation on some datasets, and gives comparable results on others. Our code is available at https://github.com/astra-vision/PODA. "
}