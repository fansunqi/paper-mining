{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Faithful Interpretation",
    "Knowledge Injection in CNNs"
  ],
  "datasets": [
    "Imagenet"
  ],
  "methods": [
    "Local Attention Pooling (LAP)",
    "Weakly-supervised knowledge injection"
  ],
  "results": [
    "More valid human-understandable interpretations",
    "More faithful-to-the-model interpretations than white-box explainer methods"
  ],
  "paper_id": "61f753205aee126c0f9c2062",
  "title": "LAP: An Attention-Based Module for Faithful Interpretation and Knowledge\n  Injection in Convolutional Neural Networks",
  "abstract": "  Despite the state-of-the-art performance of deep convolutional neural networks, they are susceptible to bias and malfunction in unseen situations. The complex computation behind their reasoning is not sufficiently human-understandable to develop trust. External explainer methods have tried to interpret the network decisions in a human-understandable way, but they are accused of fallacies due to their assumptions and simplifications. On the other side, the inherent self-interpretability of models, while being more robust to the mentioned fallacies, cannot be applied to the already trained models. In this work, we propose a new attention-based pooling layer, called Local Attention Pooling (LAP), that accomplishes self-interpretability and the possibility for knowledge injection while improving the model's performance. Moreover, several weakly-supervised knowledge injection methodologies are provided to enhance the process of training. We verified our claims by evaluating several LAP-extended models on three different datasets, including Imagenet. The proposed framework offers more valid human-understandable and more faithful-to-the-model interpretations than the commonly used white-box explainer methods. "
}