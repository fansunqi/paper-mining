{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Conditional Time Series Generation"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Conditional Neural Stochastic Differential Equations",
    "Signature-Wasserstein-1 metric"
  ],
  "results": [
    "More efficient than other classical approaches",
    "Outperforms them in terms of performance"
  ],
  "paper_id": "63b63fd290e50fcafd8f5a81",
  "title": "Neural SDEs for Conditional Time Series Generation and the\n  Signature-Wasserstein-1 metric",
  "abstract": "  (Conditional) Generative Adversarial Networks (GANs) have found great success in recent years, due to their ability to approximate (conditional) distributions over extremely high dimensional spaces. However, they are highly unstable and computationally expensive to train, especially in the time series setting. Recently, it has been proposed the use of a key object in rough path theory, called the signature of a path, which is able to convert the min-max formulation given by the (conditional) GAN framework into a classical minimization problem. However, this method is extremely expensive in terms of memory cost, sometimes even becoming prohibitive. To overcome this, we propose the use of \\textit{Conditional Neural Stochastic Differential Equations}, which have a constant memory cost as a function of depth, being more memory efficient than traditional deep learning architectures. We empirically test that this proposed model is more efficient than other classical approaches, both in terms of memory cost and computational time, and that it usually outperforms them in terms of performance. "
}