{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Optimization under uncertainty",
    "Two-stage stochastic optimization",
    "Probability functions analysis",
    "Expectation constraints",
    "Outlier analysis"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Rockafellian relaxation",
    "Phi-divergence",
    "Asymptotically exact Rockafellians",
    "Negative regularization"
  ],
  "results": [
    "Addresses challenging problems with ambiguous probability distributions",
    "Circumvents convergence issues in stochastic optimization",
    "Illustrated in computer vision with label noise"
  ],
  "paper_id": "62563f825aee126c0f6f288c",
  "title": "Rockafellian Relaxation in Optimization under Uncertainty:\n  Asymptotically Exact Formulations",
  "abstract": "  In practice, optimization models are often prone to unavoidable inaccuracies due to dubious assumptions and corrupted data. Traditionally, this placed special emphasis on risk-based and robust formulations, and their focus on ``conservative\" decisions. We develop, in contrast, an ``optimistic\" framework based on Rockafellian relaxations in which optimization is conducted not only over the original decision space but also jointly with a choice of model perturbation. The framework enables us to address challenging problems with ambiguous probability distributions from the areas of two-stage stochastic optimization without relatively complete recourse, probability functions lacking continuity properties, expectation constraints, and outlier analysis. We are also able to circumvent the fundamental difficulty in stochastic optimization that convergence of distributions fails to guarantee convergence of expectations. The framework centers on the novel concepts of exact and asymptotically exact Rockafellians, with interpretations of ``negative'' regularization emerging in certain settings. We illustrate the role of Phi-divergence, examine rates of convergence under changing distributions, and explore extensions to first-order optimality conditions. The main development is free of assumptions about convexity, smoothness, and even continuity of objective functions. Numerical results in the setting of computer vision with label noise illustrate the framework. "
}