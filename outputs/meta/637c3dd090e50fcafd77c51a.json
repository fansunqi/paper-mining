{
  "code_links": [
    "https://github.com/ViTAE-Transformer/DeepSolo"
  ],
  "tasks": [
    "Text spotting"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "DETR-like baseline",
    "Explicit points",
    "Text-matching criterion"
  ],
  "results": [
    "Outperforms previous state-of-the-art methods",
    "Better training efficiency",
    "Compatible with line annotations"
  ],
  "paper_id": "637c3dd090e50fcafd77c51a",
  "title": "DeepSolo: Let Transformer Decoder with Explicit Points Solo for Text\n  Spotting",
  "abstract": "  End-to-end text spotting aims to integrate scene text detection and recognition into a unified framework. Dealing with the relationship between the two sub-tasks plays a pivotal role in designing effective spotters. Although Transformer-based methods eliminate the heuristic post-processing, they still suffer from the synergy issue between the sub-tasks and low training efficiency. In this paper, we present DeepSolo, a simple DETR-like baseline that lets a single Decoder with Explicit Points Solo for text detection and recognition simultaneously. Technically, for each text instance, we represent the character sequence as ordered points and model them with learnable explicit point queries. After passing a single decoder, the point queries have encoded requisite text semantics and locations, thus can be further decoded to the center line, boundary, script, and confidence of text via very simple prediction heads in parallel. Besides, we also introduce a text-matching criterion to deliver more accurate supervisory signals, thus enabling more efficient training. Quantitative experiments on public benchmarks demonstrate that DeepSolo outperforms previous state-of-the-art methods and achieves better training efficiency. In addition, DeepSolo is also compatible with line annotations, which require much less annotation cost than polygons. The code is available at https://github.com/ViTAE-Transformer/DeepSolo. "
}