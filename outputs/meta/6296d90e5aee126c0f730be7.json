{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Machine Unlearning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Model Inversion",
    "Few-Shot Unlearning"
  ],
  "results": [
    "Outperforms state-of-the-art unlearning methods with few samples"
  ],
  "paper_id": "6296d90e5aee126c0f730be7",
  "title": "Few-Shot Unlearning by Model Inversion",
  "abstract": "  We consider a practical scenario of machine unlearning to erase a target dataset, which causes unexpected behavior from the trained model. The target dataset is often assumed to be fully identifiable in a standard unlearning scenario. Such a flawless identification, however, is almost impossible if the training dataset is inaccessible at the time of unlearning. Unlike previous approaches requiring a complete set of targets, we consider few-shot unlearning scenario when only a few samples of target data are available. To this end, we formulate the few-shot unlearning problem specifying intentions behind the unlearning request (e.g., purely unlearning, mislabel correction, privacy protection), and we devise a straightforward framework that (i) retrieves a proxy of the training data via model inversion fully exploiting information available in the context of unlearning; (ii) adjusts the proxy according to the unlearning intention; and (iii) updates the model with the adjusted proxy. We demonstrate that our method using only a subset of target data can outperform the state-of-the-art unlearning methods even with a complete indication of target data. "
}