{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Contrastive learning",
    "Self-supervised representations",
    "Kernel functions",
    "Approximate view-invariant functions"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Contrastive learning",
    "PCA",
    "Kernel PCA",
    "Markov chain eigenfunctions"
  ],
  "results": [
    "Minimizes worst-case approximation error",
    "Generalization bounds for linear prediction",
    "Empirical recovery of Markov chain eigenfunctions"
  ],
  "paper_id": "633e476490e50fcafde5909f",
  "title": "Contrastive Learning Can Find An Optimal Basis For Approximately\n  View-Invariant Functions",
  "abstract": "  Contrastive learning is a powerful framework for learning self-supervised representations that generalize well to downstream supervised tasks. We show that multiple existing contrastive learning methods can be reinterpreted as learning kernel functions that approximate a fixed positive-pair kernel. We then prove that a simple representation obtained by combining this kernel with PCA provably minimizes the worst-case approximation error of linear predictors, under a straightforward assumption that positive pairs have similar labels. Our analysis is based on a decomposition of the target function in terms of the eigenfunctions of a positive-pair Markov chain, and a surprising equivalence between these eigenfunctions and the output of Kernel PCA. We give generalization bounds for downstream linear prediction using our Kernel PCA representation, and show empirically on a set of synthetic tasks that applying Kernel PCA to contrastive learning models can indeed approximately recover the Markov chain eigenfunctions, although the accuracy depends on the kernel parameterization as well as on the augmentation strength. "
}