{
  "code_links": [
    "None"
  ],
  "tasks": [
    "text classification"
  ],
  "datasets": [
    "wide range of text classification datasets"
  ],
  "methods": [
    "TextRGNN with residual connections",
    "probabilistic language model for node embedding initialization"
  ],
  "results": [
    "Improved classification accuracy",
    "Reduced node feature oversmoothing",
    "State-of-the-art performance on multiple datasets"
  ],
  "paper_id": "61d269325244ab9dcbc5ea7f",
  "title": "TextRGNN: Residual Graph Neural Networks for Text Classification",
  "abstract": "  Recently, text classification model based on graph neural network (GNN) has attracted more and more attention. Most of these models adopt a similar network paradigm, that is, using pre-training node embedding initialization and two-layer graph convolution. In this work, we propose TextRGNN, an improved GNN structure that introduces residual connection to deepen the convolution network depth. Our structure can obtain a wider node receptive field and effectively suppress the over-smoothing of node features. In addition, we integrate the probabilistic language model into the initialization of graph node embedding, so that the non-graph semantic information of can be better extracted. The experimental results show that our model is general and efficient. It can significantly improve the classification accuracy whether in corpus level or text level, and achieve SOTA performance on a wide range of text classification datasets. "
}