{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Personalized Federated Learning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Shared data representation learning across clients",
    "Unique local heads for each client",
    "Distributed computational power for local-updates"
  ],
  "results": [
    "Linear convergence to ground-truth representation",
    "Near-optimal sample complexity in linear setting",
    "Efficient reduction of problem dimension for each client",
    "Empirical improvement over alternative personalized federated learning approaches"
  ],
  "paper_id": "602b9d8f91e0113d72356c6f",
  "title": "Exploiting Shared Representations for Personalized Federated Learning",
  "abstract": "  Deep neural networks have shown the ability to extract universal feature representations from data such as images and text that have been useful for a variety of learning tasks. However, the fruits of representation learning have yet to be fully-realized in federated settings. Although data in federated settings is often non-i.i.d. across clients, the success of centralized deep learning suggests that data often shares a global feature representation, while the statistical heterogeneity across clients or tasks is concentrated in the labels. Based on this intuition, we propose a novel federated learning framework and algorithm for learning a shared data representation across clients and unique local heads for each client. Our algorithm harnesses the distributed computational power across clients to perform many local-updates with respect to the low-dimensional local parameters for every update of the representation. We prove that this method obtains linear convergence to the ground-truth representation with near-optimal sample complexity in a linear setting, demonstrating that it can efficiently reduce the problem dimension for each client. This result is of interest beyond federated learning to a broad class of problems in which we aim to learn a shared low-dimensional representation among data distributions, for example in meta-learning and multi-task learning. Further, extensive experimental results show the empirical improvement of our method over alternative personalized federated learning approaches in federated environments with heterogeneous data. "
}