{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Radar parameter estimation"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Shannon's information theory",
    "A posteriori probability density function",
    "Range information (RI)",
    "Entropy error (EE)",
    "Closed-form approximation for EE"
  ],
  "results": [
    "1 bit of RI reduces estimation deviation by half",
    "EE degenerates to MSE at high SNR",
    "Theoretical RI is achievable",
    "Theoretical EE is tighter than Cram\u00e9r-Rao and ZivZakai bounds"
  ],
  "paper_id": "60ee300b91e01102f8efa379",
  "title": "Theoretical Performance Limit for Radar Parameter Estimation",
  "abstract": "  In this paper, we employ the thoughts and methodologies of Shannon's information theory to solve the problem of the optimal radar parameter estimation. Based on a general radar system model, the \\textit{a posteriori} probability density function of targets' parameters is derived. Range information (RI) and entropy error (EE) are defined to evaluate the performance. It is proved that acquiring 1 bit of the range information is equivalent to reducing estimation deviation by half. The closed-form approximation for the EE is deduced in all signal-to-noise ratio (SNR) regions, which demonstrates that the EE degenerates to the mean square error (MSE) when the SNR is tending to infinity. Parameter estimation theorem is then proved, which claims that the theoretical RI is achievable.   The converse claims that there exists no unbiased estimator whose empirical RI is larger than the theoretical RI. Simulation result demonstrates that the theoretical EE is tighter than the commonly used Cram\\'er-Rao bound and the ZivZakai bound. "
}