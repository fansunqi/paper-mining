{
  "code_links": [
    "https://github.com/ilyassmoummad/scl_icbhi2017"
  ],
  "tasks": [
    "Audio Feature Learning",
    "Classification"
  ],
  "datasets": [
    "ICBHI"
  ],
  "methods": [
    "Supervised Contrastive Learning",
    "Metadata Integration"
  ],
  "results": [
    "State-of-the-art score",
    "Similar performance with metadata-only learning"
  ],
  "paper_id": "635f3ca990e50fcafd3f60c5",
  "title": "Learning Audio Features with Metadata and Contrastive Learning",
  "abstract": "  Methods based on supervised learning using annotations in an end-to-end fashion have been the state-of-the-art for classification problems. However, they may be limited in their generalization capability, especially in the low data regime. In this study, we address this issue using supervised contrastive learning combined with available metadata to solve multiple pretext tasks that learn a good representation of data. We apply our approach on ICBHI, a respiratory sound classification dataset suited for this setting. We show that learning representations using only metadata, without class labels, obtains similar performance as using cross entropy with those labels only. In addition, we obtain state-of-the-art score when combining class labels with metadata using multiple supervised contrastive learning. This work suggests the potential of using multiple metadata sources in supervised contrastive settings, in particular in settings with class imbalance and few data. Our code is released at https://github.com/ilyassmoummad/scl_icbhi2017 "
}