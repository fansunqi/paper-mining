{
  "code_links": [
    "https://sites.google.com/berkeley.edu/mop-rl"
  ],
  "tasks": [
    "Preference-based reinforcement learning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Learned dynamics models"
  ],
  "results": [
    "Safer and more sample efficient than prior preference learning approaches"
  ],
  "paper_id": "63c0cc6490e50fcafd2a8971",
  "title": "Efficient Preference-Based Reinforcement Learning Using Learned Dynamics\n  Models",
  "abstract": "Preference-based reinforcement learning (PbRL) can enable robots to learn to\nperform tasks based on an individual's preferences without requiring a\nhand-crafted reward function. However, existing approaches either assume access\nto a high-fidelity simulator or analytic model or take a model-free approach\nthat requires extensive, possibly unsafe online environment interactions. In\nthis paper, we study the benefits and challenges of using a learned dynamics\nmodel when performing PbRL. In particular, we provide evidence that a learned\ndynamics model offers the following benefits when performing PbRL: (1)\npreference elicitation and policy optimization require significantly fewer\nenvironment interactions than model-free PbRL, (2) diverse preference queries\ncan be synthesized safely and efficiently as a byproduct of standard\nmodel-based RL, and (3) reward pre-training based on suboptimal demonstrations\ncan be performed without any environmental interaction. Our paper provides\nempirical evidence that learned dynamics models enable robots to learn\ncustomized policies based on user preferences in ways that are safer and more\nsample efficient than prior preference learning approaches. Supplementary\nmaterials and code are available at\nhttps://sites.google.com/berkeley.edu/mop-rl."
}