{
  "code_links": [
    "https://asap-benchmark.github.io/"
  ],
  "tasks": [
    "Video understanding benchmarks"
  ],
  "datasets": [
    "LCric"
  ],
  "methods": [
    "ASAP (Annotation and Video Stream Alignment Pipeline)"
  ],
  "results": [
    "LCric: over 1000 hours of densely annotated long Cricket videos",
    "Human studies indicate high fidelity, precision, and speed in ASAP alignment"
  ],
  "paper_id": "63c8b59590e50fcafd90b738",
  "title": "Building Scalable Video Understanding Benchmarks through Sports",
  "abstract": "  Existing benchmarks for evaluating long video understanding falls short on two critical aspects, either lacking in scale or quality of annotations. These limitations arise from the difficulty in collecting dense annotations for long videos, which often require manually labeling each frame. In this work, we introduce an automated Annotation and Video Stream Alignment Pipeline (abbreviated ASAP). We demonstrate the generality of ASAP by aligning unlabeled videos of four different sports with corresponding freely available dense web annotations (i.e. commentary). We then leverage ASAP scalability to create LCric, a large-scale long video understanding benchmark, with over 1000 hours of densely annotated long Cricket videos (with an average sample length of ~50 mins) collected at virtually zero annotation cost. We benchmark and analyze state-of-the-art video understanding models on LCric through a large set of compositional multi-choice and regression queries. We establish a human baseline that indicates significant room for new research to explore. Our human studies indicate that ASAP can align videos and annotations with high fidelity, precision, and speed. The dataset along with the code for ASAP and baselines can be accessed here: https://asap-benchmark.github.io/. "
}