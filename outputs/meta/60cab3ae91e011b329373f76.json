{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Reversible Steganography"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Monolithic neural networks for reversible steganography",
    "Modular paradigm incorporating neural networks into traditional pipelines",
    "Prediction-error modulation with neural networks in analytics module",
    "Different training configurations for predictive accuracy",
    "Initialisation strategies for input images",
    "Dual-layer prediction training strategies",
    "Comparison of model architectures with different loss functions"
  ],
  "results": [
    "None"
  ],
  "paper_id": "60cab3ae91e011b329373f76",
  "title": "Deep Learning for Predictive Analytics in Reversible Steganography",
  "abstract": "  Deep learning is regarded as a promising solution for reversible steganography. There is an accelerating trend of representing a reversible steo-system by monolithic neural networks, which bypass intermediate operations in traditional pipelines of reversible steganography. This end-to-end paradigm, however, suffers from imperfect reversibility. By contrast, the modular paradigm that incorporates neural networks into modules of traditional pipelines can stably guarantee reversibility with mathematical explainability. Prediction-error modulation is a well-established reversible steganography pipeline for digital images. It consists of a predictive analytics module and a reversible coding module. Given that reversibility is governed independently by the coding module, we narrow our focus to the incorporation of neural networks into the analytics module, which serves the purpose of predicting pixel intensities and a pivotal role in determining capacity and imperceptibility. The objective of this study is to evaluate the impacts of different training configurations upon predictive accuracy of neural networks and provide practical insights. In particular, we investigate how different initialisation strategies for input images may affect the learning process and how different training strategies for dual-layer prediction respond to the problem of distributional shift. Furthermore, we compare steganographic performance of various model architectures with different loss functions. "
}