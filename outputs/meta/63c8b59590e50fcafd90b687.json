{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Emotion Recognition in Conversation"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "BERT-ERC: Fine-tuning BERT",
    "Exploring contextual information and dialogue structure information in the fine-tuning step",
    "Adapting the PLM to the ERC task",
    "Suggestive text",
    "Fine-grained classification module",
    "Two-stage training"
  ],
  "results": [
    "Substantial improvement on four datasets",
    "Significant outperformance of the previous paradigm",
    "Adaptability to various scenes"
  ],
  "paper_id": "63c8b59590e50fcafd90b687",
  "title": "BERT-ERC: Fine-tuning BERT is Enough for Emotion Recognition in\n  Conversation",
  "abstract": "  Previous works on emotion recognition in conversation (ERC) follow a two-step paradigm, which can be summarized as first producing context-independent features via fine-tuning pretrained language models (PLMs) and then analyzing contextual information and dialogue structure information among the extracted features. However, we discover that this paradigm has several limitations. Accordingly, we propose a novel paradigm, i.e., exploring contextual information and dialogue structure information in the fine-tuning step, and adapting the PLM to the ERC task in terms of input text, classification structure, and training strategy. Furthermore, we develop our model BERT-ERC according to the proposed paradigm, which improves ERC performance in three aspects, namely suggestive text, fine-grained classification module, and two-stage training. Compared to existing methods, BERT-ERC achieves substantial improvement on four datasets, indicating its effectiveness and generalization capability. Besides, we also set up the limited resources scenario and the online prediction scenario to approximate real-world scenarios. Extensive experiments demonstrate that the proposed paradigm significantly outperforms the previous one and can be adapted to various scenes. "
}