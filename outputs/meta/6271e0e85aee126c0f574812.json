{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Federated Learning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Hybrid Algorithm (Combines gradient descent and inexact alternating direction method of multipliers)"
  ],
  "results": [
    "More communication- and computation-efficient than several state-of-the-art algorithms",
    "Converges globally under mild conditions"
  ],
  "paper_id": "6271e0e85aee126c0f574812",
  "title": "FedGiA: An Efficient Hybrid Algorithm for Federated Learning",
  "abstract": "  Federated learning has shown its advances recently but is still facing many challenges, such as how algorithms save communication resources and reduce computational costs, and whether they converge. To address these critical issues, we propose a hybrid federated learning algorithm (FedGiA) that combines the gradient descent and the inexact alternating direction method of multipliers. The proposed algorithm is more communication- and computation-efficient than several state-of-the-art algorithms theoretically and numerically. Moreover, it also converges globally under mild conditions. "
}