{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Bias-scalable analog computing for machine learning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Shape-based analog computing (S-AC)",
    "Near-memory compute elements",
    "Non-linear activation functions",
    "Inner-product compute circuits",
    "Mixed-signal compressive memory"
  ],
  "results": [
    "Performance remains robust to transistor biasing and temperature variations",
    "Demonstrated effect of bias-scalability and computational accuracy on ML regression task"
  ],
  "paper_id": "6205d3ef5aee126c0ff1d0cb",
  "title": "Bias-Scalable Near-Memory CMOS Analog Processor for Machine Learning",
  "abstract": "  Bias-scalable analog computing is attractive for implementing machine learning (ML) processors with distinct power-performance specifications. For instance, ML implementations for server workloads are focused on higher computational throughput for faster training, whereas ML implementations for edge devices are focused on energy-efficient inference. In this paper, we demonstrate the implementation of bias-scalable approximate analog computing circuits using the generalization of the margin-propagation principle called shape-based analog computing (S-AC). The resulting S-AC core integrates several near-memory compute elements, which include: (a) non-linear activation functions; (b) inner-product compute circuits; and (c) a mixed-signal compressive memory, all of which can be scaled for performance or power while preserving its functionality. Using measured results from prototypes fabricated in a 180nm CMOS process, we demonstrate that the performance of computing modules remains robust to transistor biasing and variations in temperature. In this paper, we also demonstrate the effect of bias-scalability and computational accuracy on a simple ML regression task. "
}