{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Markov decision processes under risk and reward ambiguity"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "distributionally robust return-risk model",
    "tractable reformulation",
    "first-order algorithm"
  ],
  "results": [
    "None"
  ],
  "paper_id": "63b63fd190e50fcafd8f57f5",
  "title": "Risk-Averse MDPs under Reward Ambiguity",
  "abstract": "  We propose a distributionally robust return-risk model for Markov decision processes (MDPs) under risk and reward ambiguity. The proposed model optimizes the weighted average of mean and percentile performances, and it covers the distributionally robust MDPs and the distributionally robust chance-constrained MDPs (both under reward ambiguity) as special cases. By considering that the unknown reward distribution lies in a Wasserstein ambiguity set, we derive the tractable reformulation for our model. In particular, we show that that the return-risk model can also account for risk from uncertain transition kernel when one only seeks deterministic policies, and that a distributionally robust MDP under the percentile criterion can be reformulated as its nominal counterpart at an adjusted risk level. A scalable first-order algorithm is designed to solve large-scale problems, and we demonstrate the advantages of our proposed model and algorithm through numerical experiments. "
}