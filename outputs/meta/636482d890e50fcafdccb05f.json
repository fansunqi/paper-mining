{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Text-to-Speech System"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "FullConv-TTS",
    "CNN-based sequence synthesis",
    "Time Warping",
    "Frequency Mask",
    "Time Mask"
  ],
  "results": [
    "Reduces training time compared to classic TTS models like Tacotron",
    "Ensures quality of synthesized speech"
  ],
  "paper_id": "636482d890e50fcafdccb05f",
  "title": "Efficiently Trained Low-Resource Mongolian Text-to-Speech System Based\n  On FullConv-TTS",
  "abstract": "  Recurrent Neural Networks (RNNs) have become the standard modeling technique for sequence data, and are used in a number of novel text-to-speech models. However, training a TTS model including RNN components has certain requirements for GPU performance and takes a long time. In contrast, studies have shown that CNN-based sequence synthesis technology can greatly reduce training time in text-to-speech models while ensuring a certain performance due to its high parallelism. We propose a new text-to-speech system based on deep convolutional neural networks that does not employ any RNN components (recurrent units). At the same time, we improve the generality and robustness of our model through a series of data augmentation methods such as Time Warping, Frequency Mask, and Time Mask. The final experimental results show that the TTS model using only the CNN component can reduce the training time compared to the classic TTS models such as Tacotron while ensuring the quality of the synthesized speech. "
}