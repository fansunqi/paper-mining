{
  "code_links": [
    "https://github.com/Tai-Wang/Depth-from-Motion"
  ],
  "tasks": [
    "Monocular 3D Object Detection"
  ],
  "datasets": [
    "KITTI"
  ],
  "methods": [
    "Depth from Motion (DfM)",
    "Geometry-aware cost volume",
    "Monocular understanding"
  ],
  "results": [
    "Outperforms state-of-the-art methods on KITTI benchmark",
    "Pose-free DfM"
  ],
  "paper_id": "62e0acfd5aee126c0f209f69",
  "title": "Monocular 3D Object Detection with Depth from Motion",
  "abstract": "  Perceiving 3D objects from monocular inputs is crucial for robotic systems, given its economy compared to multi-sensor settings. It is notably difficult as a single image can not provide any clues for predicting absolute depth values. Motivated by binocular methods for 3D object detection, we take advantage of the strong geometry structure provided by camera ego-motion for accurate object depth estimation and detection. We first make a theoretical analysis on this general two-view case and notice two challenges: 1) Cumulative errors from multiple estimations that make the direct prediction intractable; 2) Inherent dilemmas caused by static cameras and matching ambiguity. Accordingly, we establish the stereo correspondence with a geometry-aware cost volume as the alternative for depth estimation and further compensate it with monocular understanding to address the second problem. Our framework, named Depth from Motion (DfM), then uses the established geometry to lift 2D image features to the 3D space and detects 3D objects thereon. We also present a pose-free DfM to make it usable when the camera pose is unavailable. Our framework outperforms state-of-the-art methods by a large margin on the KITTI benchmark. Detailed quantitative and qualitative analyses also validate our theoretical conclusions. The code will be released at https://github.com/Tai-Wang/Depth-from-Motion. "
}