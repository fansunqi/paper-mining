{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Open-Vocabulary Temporal Action Detection"
  ],
  "datasets": [
    "ActivityNet"
  ],
  "methods": [
    "Pretrained image-text co-embeddings",
    "Ensembling with motion features (e.g., optical flow)",
    "Audio features"
  ],
  "results": [
    "Competitive open-vocabulary performance with fully-supervised models",
    "Improved performance by combining image-text features with motion and audio features"
  ],
  "paper_id": "63a3cae890e50fcafdeb72a9",
  "title": "Open-Vocabulary Temporal Action Detection with Off-the-Shelf Image-Text\n  Features",
  "abstract": "  Detecting actions in untrimmed videos should not be limited to a small, closed set of classes. We present a simple, yet effective strategy for open-vocabulary temporal action detection utilizing pretrained image-text co-embeddings. Despite being trained on static images rather than videos, we show that image-text co-embeddings enable openvocabulary performance competitive with fully-supervised models. We show that the performance can be further improved by ensembling the image-text features with features encoding local motion, like optical flow based features, or other modalities, like audio. In addition, we propose a more reasonable open-vocabulary evaluation setting for the ActivityNet data set, where the category splits are based on similarity rather than random assignment. "
}