{
  "code_links": [
    "https://github.com/manestay/EcXTra"
  ],
  "tasks": [
    "Multilingual Bidirectional Unsupervised Translation"
  ],
  "datasets": [
    "parallel data in 40 languages to English",
    "monolingual datasets"
  ],
  "methods": [
    "multilingual fine-tuning",
    "back-translation",
    "EcXTra (English-centric Crosslingual Transfer)"
  ],
  "results": [
    "state-of-the-art for English-to-Kazakh (22.9 > 10.4 BLEU)"
  ],
  "paper_id": "63195d8190e50fcafde7d45d",
  "title": "Multilingual Bidirectional Unsupervised Translation Through Multilingual\n  Finetuning and Back-Translation",
  "abstract": "  We propose a two-stage approach for training a single NMT model to translate unseen languages both to and from English. For the first stage, we initialize an encoder-decoder model to pretrained XLM-R and RoBERTa weights, then perform multilingual fine-tuning on parallel data in 40 languages to English. We find this model can generalize to zero-shot translations on unseen languages. For the second stage, we leverage this generalization ability to generate synthetic parallel data from monolingual datasets, then bidirectionally train with successive rounds of back-translation.   Our approach, which we EcXTra (English-centric Crosslingual (X) Transfer), is conceptually simple, only using a standard cross-entropy objective throughout. It is also data-driven, sequentially leveraging auxiliary parallel data and monolingual data. We evaluate unsupervised NMT results for 7 low-resource languages, and find that each round of back-translation training further refines bidirectional performance. Our final single EcXTra-trained model achieves competitive translation performance in all translation directions, notably establishing a new state-of-the-art for English-to-Kazakh (22.9 > 10.4 BLEU). Our code is available at https://github.com/manestay/EcXTra . "
}