{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Multi-Modal Fact Checking"
  ],
  "datasets": [
    "De-Factify 2 challenge (DE-FACTIFY 2023)"
  ],
  "methods": [
    "Evidence Retrieval techniques",
    "Transformer Encoder (TE) architecture",
    "self-attention"
  ],
  "results": [
    "weighted avg. 0.79 on both val set and final blind test set",
    "3rd place among 9 participants"
  ],
  "paper_id": "63bcd73690e50fcafdefa164",
  "title": "Logically at Factify 2: A Multi-Modal Fact Checking System Based on\n  Evidence Retrieval techniques and Transformer Encoder Architecture",
  "abstract": "  In this paper, we present the Logically submissions to De-Factify 2 challenge (DE-FACTIFY 2023) on the task 1 of Multi-Modal Fact Checking. We describes our submissions to this challenge including explored evidence retrieval and selection techniques, pre-trained cross-modal and unimodal models, and a cross-modal veracity model based on the well established Transformer Encoder (TE) architecture which is heavily relies on the concept of self-attention. Exploratory analysis is also conducted on this Factify 2 data set that uncovers the salient multi-modal patterns and hypothesis motivating the architecture proposed in this work. A series of preliminary experiments were done to investigate and benchmarking different pre-trained embedding models, evidence retrieval settings and thresholds. The final system, a standard two-stage evidence based veracity detection system, yields weighted avg. 0.79 on both val set and final blind test set on the task 1, which achieves 3rd place with a small margin to the top performing system on the leaderboard among 9 participants. "
}