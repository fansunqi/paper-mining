{
  "code_links": [
    "None"
  ],
  "tasks": [
    "3D shape analysis",
    "scene understanding"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "PointVST",
    "cross-modal translation from 3D point clouds to 2D rendered images"
  ],
  "results": [
    "consistent and prominent performance superiority over current state-of-the-art methods"
  ],
  "paper_id": "63b24b1f90e50fcafdd370b1",
  "title": "Self-Supervised Pre-training for 3D Point Clouds via View-Specific\n  Point-to-Image Translation",
  "abstract": "  The past few years have witnessed the prevalence of self-supervised representation learning within the language and 2D vision communities. However, such advancements have not been fully migrated to the 3D point cloud learning community. Different from existing pre-training paradigms for 3D point clouds falling into the scope of generative modeling or contrastive learning, this paper proposes a translative pre-training framework, namely PointVST, driven by a novel self-supervised pretext task of cross-modal translation from 3D point clouds to their corresponding diverse forms of 2D rendered images. More specifically, we start by deducing view-conditioned point-wise embeddings via the insertion of a viewpoint indicator and then adaptively aggregate a view-specific global codeword fed into the subsequent 2D convolutional translation heads for image generation. Extensive experiments on various downstream tasks of 3D shape analysis and scene understanding demonstrate that PointVST shows consistent and prominent performance superiority over current state-of-the-art methods. Our code will be made publicly available. "
}