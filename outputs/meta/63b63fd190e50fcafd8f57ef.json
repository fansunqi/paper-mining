{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Few-shot learning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Variational autoencoder (VAE)",
    "Free Energy Principle",
    "Bayesian Program Learning"
  ],
  "results": [
    "Significantly better performance than baseline models for image recognition, low resource language processing, and character recognition"
  ],
  "paper_id": "63b63fd190e50fcafd8f57ef",
  "title": "A Theory of Human-Like Few-Shot Learning",
  "abstract": "  We aim to bridge the gap between our common-sense few-sample human learning and large-data machine learning. We derive a theory of human-like few-shot learning from von-Neuman-Landauer's principle. modelling human learning is difficult as how people learn varies from one to another. Under commonly accepted definitions, we prove that all human or animal few-shot learning, and major models including Free Energy Principle and Bayesian Program Learning that model such learning, approximate our theory, under Church-Turing thesis. We find that deep generative model like variational autoencoder (VAE) can be used to approximate our theory and perform significantly better than baseline models including deep neural networks, for image recognition, low resource language processing, and character recognition. "
}