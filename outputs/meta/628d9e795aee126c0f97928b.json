{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Conversational Recommendation",
    "Cold-Start Challenge"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Meta Policy Learning",
    "Meta-Exploration Policy",
    "Item Recommendation Module",
    "Transformer-based State Encoder"
  ],
  "results": [
    "Advantage in serving new users compared to state-of-the-art CRS solutions"
  ],
  "paper_id": "628d9e795aee126c0f97928b",
  "title": "Meta Policy Learning for Cold-Start Conversational Recommendation",
  "abstract": "  Conversational recommender systems (CRS) explicitly solicit users' preferences for improved recommendations on the fly. Most existing CRS solutions count on a single policy trained by reinforcement learning for a population of users. However, for users new to the system, such a global policy becomes ineffective to satisfy them, i.e., the cold-start challenge. In this paper, we study CRS policy learning for cold-start users via meta-reinforcement learning. We propose to learn a meta policy and adapt it to new users with only a few trials of conversational recommendations. To facilitate fast policy adaptation, we design three synergetic components. Firstly, we design a meta-exploration policy dedicated to identifying user preferences via a few exploratory conversations, which accelerates personalized policy adaptation from the meta policy. Secondly, we adapt the item recommendation module for each user to maximize the recommendation quality based on the collected conversation states during conversations. Thirdly, we propose a Transformer-based state encoder as the backbone to connect the previous two components. It provides comprehensive state representations by modeling complicated relations between positive and negative feedback during the conversation. Extensive experiments on three datasets demonstrate the advantage of our solution in serving new users, compared with a rich set of state-of-the-art CRS solutions. "
}