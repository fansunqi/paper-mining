{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Semi-supervised learning",
    "Backdoor attacks"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Gradient matching strategy",
    "Backdoor poisoning"
  ],
  "results": [
    "State-of-the-art attack success rates",
    "Bypasses modern backdoor defenses"
  ],
  "paper_id": "63b39cbf90e50fcafdd1e622",
  "title": "Trojaning semi-supervised learning model via poisoning wild images on\n  the web",
  "abstract": "  Wild images on the web are vulnerable to backdoor (also called trojan) poisoning, causing machine learning models learned on these images to be injected with backdoors. Most previous attacks assumed that the wild images are labeled. In reality, however, most images on the web are unlabeled. Specifically, we study the effects of unlabeled backdoor images under semi-supervised learning (SSL) on widely studied deep neural networks. To be realistic, we assume that the adversary is zero-knowledge and that the semi-supervised learning model is trained from scratch. Firstly, we find the fact that backdoor poisoning always fails when poisoned unlabeled images come from different classes, which is different from poisoning the labeled images. The reason is that the SSL algorithms always strive to correct them during training. Therefore, for unlabeled images, we implement backdoor poisoning on images from the target class. Then, we propose a gradient matching strategy to craft poisoned images such that their gradients match the gradients of target images on the SSL model, which can fit poisoned images to the target class and realize backdoor injection. To the best of our knowledge, this may be the first approach to backdoor poisoning on unlabeled images of trained-from-scratch SSL models. Experiments show that our poisoning achieves state-of-the-art attack success rates on most SSL algorithms while bypassing modern backdoor defenses. "
}