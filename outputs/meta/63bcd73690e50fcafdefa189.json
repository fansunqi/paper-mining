{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Platoon Leader Selection",
    "User Association",
    "Resource Allocation",
    "C-V2X based highway"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Distributed Multi-Agent Reinforcement Learning (MARL)",
    "Deep Q-learning"
  ],
  "results": [
    "MARL outperforms centralized hill-climbing algorithm",
    "Platoon leader selection improves V2V and V2I performance"
  ],
  "paper_id": "63bcd73690e50fcafdefa189",
  "title": "Platoon Leader Selection, User Association and Resource Allocation on a\n  C-V2X based highway: A Reinforcement Learning Approach",
  "abstract": "  We consider the problem of dynamic platoon leader selection, user association, channel assignment, and power allocation on a cellular vehicle-to-everything (C-V2X) based highway, where multiple vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) links share the frequency resources. There are multiple roadside units (RSUs) on a highway, and vehicles can form platoons, which has been identified as an advanced use case to increase road efficiency. The traditional optimization methods, requiring global channel information at a central controller, are not viable for high-mobility vehicular networks. To deal with this challenge, we propose a distributed multi-agent reinforcement learning (MARL) for resource allocation (RA). Each platoon leader, acting as an agent, can collaborate with other agents for joint sub-band selection and power allocation for its V2V links, and joint user association and power control for its V2I links. Moreover, each platoon can dynamically select the vehicle most suitable to be the platoon leader. We aim to maximize the V2V and V2I packet delivery probability in the desired latency using the deep Q-learning algorithm. Simulation results indicate that our proposed MARL outperforms the centralized hill-climbing algorithm, and platoon leader selection helps to improve both V2V and V2I performance. "
}