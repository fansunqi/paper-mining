{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Textual logical reasoning",
    "QA tasks with logical reasoning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "logic structural-constraint modeling",
    "discourse-aware graph networks (DAGNs)",
    "edge-reasoning mechanism"
  ],
  "results": [
    "reasonability of the logical structures built in DAGNs",
    "effectiveness of the learned logic features",
    "zero-shot transfer results"
  ],
  "paper_id": "62c3abc75aee126c0fc9a9a8",
  "title": "Discourse-Aware Graph Networks for Textual Logical Reasoning",
  "abstract": "  Textual logical reasoning, especially question-answering (QA) tasks with logical reasoning, requires awareness of particular logical structures. The passage-level logical relations represent entailment or contradiction between propositional units (e.g., a concluding sentence). However, such structures are unexplored as current QA systems focus on entity-based relations. In this work, we propose logic structural-constraint modeling to solve the logical reasoning QA and introduce discourse-aware graph networks (DAGNs). The networks first construct logic graphs leveraging in-line discourse connectives and generic logic theories, then learn logic representations by end-to-end evolving the logic relations with an edge-reasoning mechanism and updating the graph features. This pipeline is applied to a general encoder, whose fundamental features are joined with the high-level logic features for answer prediction. Experiments on three textual logical reasoning datasets demonstrate the reasonability of the logical structures built in DAGNs and the effectiveness of the learned logic features. Moreover, zero-shot transfer results show the features' generality to unseen logical texts. "
}