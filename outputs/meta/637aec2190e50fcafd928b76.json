{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Semi-Supervised Learning"
  ],
  "datasets": [
    "Text and image benchmark classification datasets"
  ],
  "methods": [
    "Contrastive Credibility Propagation (CCP)"
  ],
  "results": [
    "Boosts or matches performance over a supervised baseline in four common real-world SSL scenarios"
  ],
  "paper_id": "637aec2190e50fcafd928b76",
  "title": "Contrastive Credibility Propagation for Reliable Semi-Supervised\n  Learning",
  "abstract": "  Inferencing unlabeled data from labeled data is an error-prone process. Conventional neural network training is highly sensitive to supervision errors. These two realities make semi-supervised learning (SSL) troublesome. In practice, SSL approaches often fail to outperform their fully supervised baseline. Proposed is a novel framework for deep SSL via transductive pseudo-label refinement called Contrastive Credibility Propagation (CCP). Through an iterative process of refining soft pseudo-labels, CCP unifies a novel contrastive approach for generating pseudo-labels and a powerful technique to overcome instance-dependent label noise. The result is an SSL classification framework explicitly designed to overcome inevitable pseudo-label errors. Using standard text and image benchmark classification datasets, we show CCP reliably boosts or matches performance over a supervised baseline in four common real-world SSL scenarios: few-label, open-set, noisy-label, and class distribution misalignment. "
}