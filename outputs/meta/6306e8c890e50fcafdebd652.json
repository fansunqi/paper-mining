{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Bayesian neural networks",
    "Minibatch MCMC sampling for feedforward neural networks"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Blocked Gibbs sampling scheme",
    "Partitioning the parameter space",
    "Reducing the proposal variance in deeper layers",
    "Increasing the length of a non-convergent chain"
  ],
  "results": [
    "Alleviating vanishing acceptance rates",
    "Increasing predictive accuracy in classification tasks",
    "Quantification of predictive uncertainty"
  ],
  "paper_id": "6306e8c890e50fcafdebd652",
  "title": "Approximate blocked Gibbs sampling for Bayesian neural networks",
  "abstract": "  In this work, minibatch MCMC sampling for feedforward neural networks is made more feasible. To this end, it is proposed to sample subgroups of parameters via a blocked Gibbs sampling scheme. By partitioning the parameter space, sampling is possible irrespective of layer width. It is also possible to alleviate vanishing acceptance rates for increasing depth by reducing the proposal variance in deeper layers. Increasing the length of a non-convergent chain increases the predictive accuracy in classification tasks, so avoiding vanishing acceptance rates and consequently enabling longer chain runs have practical benefits. Moreover, non-convergent chain realizations aid in the quantification of predictive uncertainty. An open problem is how to perform minibatch MCMC sampling for feedforward neural networks in the presence of augmented data. "
}