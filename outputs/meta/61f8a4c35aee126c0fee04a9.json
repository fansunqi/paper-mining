{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Bayesian bandit algorithms with approximate Bayesian inference"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Enhanced Bayesian Upper Confidence Bound (EBUCB)"
  ],
  "results": [
    "EBUCB achieves optimal regret order O(log T) with constant inference error",
    "First theoretical regret bound better than o(T) for constant approximate inference error"
  ],
  "paper_id": "61f8a4c35aee126c0fee04a9",
  "title": "Optimal Regret Is Achievable With Constant Approximate Inference Error:\n  An Enhanced Bayesian Upper Confidence Bound Framework",
  "abstract": "  Bayesian bandit algorithms with approximate Bayesian inference have been widely used in real-world applications. However, there is a large discrepancy between the superior practical performance of these approaches and their theoretical justification. Previous research only indicates a negative theoretical result: Thompson sampling could have a worst-case linear regret $\\Omega(T)$ with a constant threshold on the inference error measured by one $\\alpha$-divergence. To bridge this gap, we propose an Enhanced Bayesian Upper Confidence Bound (EBUCB) framework that can efficiently accommodate bandit problems in the presence of approximate inference. Our theoretical analysis demonstrates that for Bernoulli multi-armed bandits, EBUCB can achieve the optimal regret order $O(\\log T)$ if the inference error measured by two different $\\alpha$-divergences is less than a constant, regardless of how large this constant is. Our study provides the first theoretical regret bound that is better than $o(T)$ in the setting of constant approximate inference error, to our best knowledge. Furthermore, in concordance with the negative results in previous studies, we show that only one bounded $\\alpha$-divergence is insufficient to guarantee a sub-linear regret. "
}