{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Learning Monotone Classes"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "PAC-learning",
    "randomized exponential time hypothesis",
    "set cover hardness conjecture"
  ],
  "results": [
    "Lower bounds for PAC-learning of Monotone DNF, Monotone Decision Tree, and Monotone Junta by DNF"
  ],
  "paper_id": "63cdfab690e50fcafd106f53",
  "title": "Superpolynomial Lower Bounds for Learning Monotone Classes",
  "abstract": "  Koch, Strassle, and Tan [SODA 2023], show that, under the randomized exponential time hypothesis, there is no distribution-free PAC-learning algorithm that runs in time $n^{\\tilde O(\\log\\log s)}$ for the classes of $n$-variable size-$s$ DNF, size-$s$ Decision Tree, and $\\log s$-Junta by DNF (that returns a DNF hypothesis). Assuming a natural conjecture on the hardness of set cover, they give the lower bound $n^{\\Omega(\\log s)}$. This matches the best known upper bound for $n$-variable size-$s$ Decision Tree, and $\\log s$-Junta.   In this paper, we give the same lower bounds for PAC-learning of $n$-variable size-$s$ Monotone DNF, size-$s$ Monotone Decision Tree, and Monotone $\\log s$-Junta by~DNF. This solves the open problem proposed by Koch, Strassle, and Tan and subsumes the above results.   The lower bound holds, even if the learner knows the distribution, can draw a sample according to the distribution in polynomial time, and can compute the target function on all the points of the support of the distribution in polynomial time. "
}