{
  "code_links": [
    "https://doi.org/10.5281/zenodo.7599667"
  ],
  "tasks": [
    "Semantic Textual Relatedness",
    "Sentence Representation Evaluation",
    "Downstream NLP Tasks"
  ],
  "datasets": [
    "STR-2022"
  ],
  "methods": [
    "Comparative Annotation Framework"
  ],
  "results": [
    "Human annotation correlation: 0.84",
    "Utility for evaluating sentence representation and downstream NLP tasks"
  ],
  "paper_id": "6164fcc15244ab9dcb24d144",
  "title": "What Makes Sentences Semantically Related: A Textual Relatedness Dataset\n  and Empirical Study",
  "abstract": "  The degree of semantic relatedness of two units of language has long been considered fundamental to understanding meaning. Additionally, automatically determining relatedness has many applications such as question answering and summarization. However, prior NLP work has largely focused on semantic similarity, a subset of relatedness, because of a lack of relatedness datasets. In this paper, we introduce a dataset for Semantic Textual Relatedness, STR-2022, that has 5,500 English sentence pairs manually annotated using a comparative annotation framework, resulting in fine-grained scores. We show that human intuition regarding relatedness of sentence pairs is highly reliable, with a repeat annotation correlation of 0.84. We use the dataset to explore questions on what makes sentences semantically related. We also show the utility of STR-2022 for evaluating automatic methods of sentence representation and for various downstream NLP tasks.   Our dataset, data statement, and annotation questionnaire can be found at: https://doi.org/10.5281/zenodo.7599667 "
}