{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Symmetries in the dynamics of wide two-layer neural networks"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Gradient flow on the population risk",
    "Study of symmetries",
    "Analysis of specific cases"
  ],
  "results": [
    "Reduction of dynamics to a linear predictor when f* is odd",
    "Gradient flow PDE reduces to a lower-dimensional PDE when f* has a low-dimensional structure",
    "Informal and numerical arguments suggesting input neurons align with the lower-dimensional structure"
  ],
  "paper_id": "6375a67090e50fcafd3e1b26",
  "title": "On the symmetries in the dynamics of wide two-layer neural networks",
  "abstract": "  We consider the idealized setting of gradient flow on the population risk for infinitely wide two-layer ReLU neural networks (without bias), and study the effect of symmetries on the learned parameters and predictors. We first describe a general class of symmetries which, when satisfied by the target function $f^*$ and the input distribution, are preserved by the dynamics. We then study more specific cases. When $f^*$ is odd, we show that the dynamics of the predictor reduces to that of a (non-linearly parameterized) linear predictor, and its exponential convergence can be guaranteed. When $f^*$ has a low-dimensional structure, we prove that the gradient flow PDE reduces to a lower-dimensional PDE. Furthermore, we present informal and numerical arguments that suggest that the input neurons align with the lower-dimensional structure of the problem. "
}