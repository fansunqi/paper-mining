{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Differentially-private learning in NLP"
  ],
  "datasets": [
    "Seven downstream datasets"
  ],
  "methods": [
    "DP-SGD",
    "BERT",
    "XtremeDistil architectures"
  ],
  "results": [
    "Privacy-preserving strategies do not exhibit a winning pattern",
    "Each task and privacy regime requires special treatment"
  ],
  "paper_id": "61baae6a5244ab9dcb6442e9",
  "title": "One size does not fit all: Investigating strategies for\n  differentially-private learning across NLP tasks",
  "abstract": "  Preserving privacy in contemporary NLP models allows us to work with sensitive data, but unfortunately comes at a price. We know that stricter privacy guarantees in differentially-private stochastic gradient descent (DP-SGD) generally degrade model performance. However, previous research on the efficiency of DP-SGD in NLP is inconclusive or even counter-intuitive. In this short paper, we provide an extensive analysis of different privacy preserving strategies on seven downstream datasets in five different `typical' NLP tasks with varying complexity using modern neural models based on BERT and XtremeDistil architectures. We show that unlike standard non-private approaches to solving NLP tasks, where bigger is usually better, privacy-preserving strategies do not exhibit a winning pattern, and each task and privacy regime requires a special treatment to achieve adequate performance. "
}