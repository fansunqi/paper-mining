{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Neural architecture similarity analysis"
  ],
  "datasets": [
    "ImageNet"
  ],
  "methods": [
    "Adversarial attack transferability",
    "Similarity function"
  ],
  "results": [
    "Model diversity leads to better performance on model ensembles and knowledge distillation"
  ],
  "paper_id": "63520de890e50fcafd60f4d8",
  "title": "Similarity of Neural Architectures Based on Input Gradient\n  Transferability",
  "abstract": "  In recent years, a huge amount of deep neural architectures have been developed for image classification. It remains curious whether these models are similar or different and what factors contribute to their similarities or differences. To address this question, we aim to design a quantitative and scalable similarity function between neural architectures. We utilize adversarial attack transferability, which has information related to input gradients and decision boundaries that are widely used to understand model behaviors. We conduct a large-scale analysis on 69 state-of-the-art ImageNet classifiers using our proposed similarity function to answer the question. Moreover, we observe neural architecture-related phenomena using model similarity that model diversity can lead to better performance on model ensembles and knowledge distillation under specific conditions. Our results provide insights into why the development of diverse neural architectures with distinct components is necessary. "
}