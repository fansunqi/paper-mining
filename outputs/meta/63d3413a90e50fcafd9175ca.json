{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Interpreting machine learning models",
    "Ensuring safety of deep networks in autonomous driving systems"
  ],
  "datasets": [
    "traffic sign data"
  ],
  "methods": [
    "Certifiably prOvable Robustness Guarantees for Interpretability mapping (CORGI)"
  ],
  "results": [
    "Certifiable lower bound for the robustness of the top k pixels of its CAM interpretability map",
    "Certifying lower bounds on the minimum adversarial perturbation not far from (4-5x) state-of-the-art attack methods"
  ],
  "paper_id": "63d3413a90e50fcafd9175ca",
  "title": "Certified Interpretability Robustness for Class Activation Mapping",
  "abstract": "  Interpreting machine learning models is challenging but crucial for ensuring the safety of deep networks in autonomous driving systems. Due to the prevalence of deep learning based perception models in autonomous vehicles, accurately interpreting their predictions is crucial. While a variety of such methods have been proposed, most are shown to lack robustness. Yet, little has been done to provide certificates for interpretability robustness. Taking a step in this direction, we present CORGI, short for Certifiably prOvable Robustness Guarantees for Interpretability mapping. CORGI is an algorithm that takes in an input image and gives a certifiable lower bound for the robustness of the top k pixels of its CAM interpretability map. We show the effectiveness of CORGI via a case study on traffic sign data, certifying lower bounds on the minimum adversarial perturbation not far from (4-5x) state-of-the-art attack methods. "
}