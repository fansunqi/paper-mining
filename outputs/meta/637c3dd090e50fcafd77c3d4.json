{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Stereo depth sensing",
    "3D computational photography",
    "Smart glasses"
  ],
  "datasets": [
    "Middlebury",
    "in-the-wild images captured from smart glasses"
  ],
  "methods": [
    "End-to-end stereo depth sensing system",
    "Online stereo rectification",
    "Stereo depth estimation",
    "Fallback to monocular depth estimation",
    "Robust calibration handling"
  ],
  "results": [
    "Models run in less than 1s on a six-year-old Samsung Galaxy S8 phone's CPU",
    "Good results on Middlebury and in-the-wild images"
  ],
  "paper_id": "637c3dd090e50fcafd77c3d4",
  "title": "A Practical Stereo Depth System for Smart Glasses",
  "abstract": "  We present the design of a productionized end-to-end stereo depth sensing system that does pre-processing, online stereo rectification, and stereo depth estimation with a fallback to monocular depth estimation when rectification is unreliable. The output of our depth sensing system is then used in a novel view generation pipeline to create 3D computational photography effects using point-of-view images captured by smart glasses. All these steps are executed on-device on the stringent compute budget of a mobile phone, and because we expect the users can use a wide range of smartphones, our design needs to be general and cannot be dependent on a particular hardware or ML accelerator such as a smartphone GPU. Although each of these steps is well studied, a description of a practical system is still lacking. For such a system, all these steps need to work in tandem with one another and fallback gracefully on failures within the system or less than ideal input data. We show how we handle unforeseen changes to calibration, e.g., due to heat, robustly support depth estimation in the wild, and still abide by the memory and latency constraints required for a smooth user experience. We show that our trained models are fast, and run in less than 1s on a six-year-old Samsung Galaxy S8 phone's CPU. Our models generalize well to unseen data and achieve good results on Middlebury and in-the-wild images captured from the smart glasses. "
}