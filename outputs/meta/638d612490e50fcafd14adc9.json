{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Continuous Speech Recognition"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Surrogate Gradient Spiking Neural Networks",
    "Recurrent Neural Networks",
    "LSTMs"
  ],
  "results": [
    "Replaces LSTMs in the encoder with only minor loss of performance",
    "Robustness to exploding gradient problems"
  ],
  "paper_id": "638d612490e50fcafd14adc9",
  "title": "Surrogate Gradient Spiking Neural Networks as Encoders for Large\n  Vocabulary Continuous Speech Recognition",
  "abstract": "  Compared to conventional artificial neurons that produce dense and real-valued responses, biologically-inspired spiking neurons transmit sparse and binary information, which can also lead to energy-efficient implementations. Recent research has shown that spiking neural networks can be trained like standard recurrent neural networks using the surrogate gradient method. They have shown promising results on speech command recognition tasks. Using the same technique, we show that they are scalable to large vocabulary continuous speech recognition, where they are capable of replacing LSTMs in the encoder with only minor loss of performance. This suggests that they may be applicable to more involved sequence-to-sequence tasks. Moreover, in contrast to their recurrent non-spiking counterparts, they show robustness to exploding gradient problems without the need to use gates. "
}