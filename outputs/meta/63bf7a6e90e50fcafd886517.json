{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Speech driven video editing"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Denoising diffusion model",
    "Audio spectral features"
  ],
  "results": [
    "Word error rate: 45% using an off the shelf lip reading model"
  ],
  "paper_id": "63bf7a6e90e50fcafd886517",
  "title": "Speech Driven Video Editing via an Audio-Conditioned Diffusion Model",
  "abstract": "  In this paper we propose a method for end-to-end speech driven video editing using a denoising diffusion model. Given a video of a person speaking, we aim to re-synchronise the lip and jaw motion of the person in response to a separate auditory speech recording without relying on intermediate structural representations such as facial landmarks or a 3D face model. We show this is possible by conditioning a denoising diffusion model with audio spectral features to generate synchronised facial motion. We achieve convincing results on the task of unstructured single-speaker video editing, achieving a word error rate of 45% using an off the shelf lip reading model. We further demonstrate how our approach can be extended to the multi-speaker domain. To our knowledge, this is the first work to explore the feasibility of applying denoising diffusion models to the task of audio-driven video editing. "
}