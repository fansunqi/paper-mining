{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Out-of-distribution detection",
    "Interpreting OOD detector decisions"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Concept-based explanations",
    "Two new metrics: detection completeness and concept separability",
    "Framework for learning concepts",
    "Modified Shapley value-based importance score"
  ],
  "results": [
    "Framework demonstrates effectiveness in providing concept-based explanations for diverse OOD techniques",
    "Identification of prominent concepts contributing to detection results"
  ],
  "paper_id": "6226c93c5aee126c0fd57792",
  "title": "Concept-based Explanations for Out-Of-Distribution Detectors",
  "abstract": "  Out-of-distribution (OOD) detection plays a crucial role in ensuring the safe deployment of deep neural network (DNN) classifiers. While a myriad of methods have focused on improving the performance of OOD detectors, a critical gap remains in interpreting their decisions. We help bridge this gap by providing explanations for OOD detectors based on learned high-level concepts. We first propose two new metrics for assessing the effectiveness of a particular set of concepts for explaining OOD detectors: 1) detection completeness, which quantifies the sufficiency of concepts for explaining an OOD-detector's decisions, and 2) concept separability, which captures the distributional separation between in-distribution and OOD data in the concept space. Based on these metrics, we propose a framework for learning a set of concepts that satisfy the desired properties of detection completeness and concept separability and demonstrate the framework's effectiveness in providing concept-based explanations for diverse OOD techniques. We also show how to identify prominent concepts that contribute to the detection results via a modified Shapley value-based importance score. "
}