{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Reinforcement Learning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Model-Based Reinforcement Learning",
    "Experience Replay",
    "Neural Network Function Approximation"
  ],
  "results": [
    "General theorem on model learning",
    "Empirical example of neural network function approximation",
    "Experiments on online RL with combinatorial complexity"
  ],
  "paper_id": "6368773090e50fcafd674d68",
  "title": "The Benefits of Model-Based Generalization in Reinforcement Learning",
  "abstract": "  Model-Based Reinforcement Learning (RL) is widely believed to have the potential to improve sample efficiency by allowing an agent to synthesize large amounts of imagined experience. Experience Replay (ER) can be considered a simple kind of model, which has proved extremely effective at improving the stability and efficiency of deep RL. In principle, a learned parametric model could improve on ER by generalizing from real experience to augment the dataset with additional plausible experience. However, owing to the many design choices involved in empirically successful algorithms, it can be very hard to establish where the benefits are actually coming from. Here, we provide theoretical and empirical insight into when, and how, we can expect data generated by a learned model to be useful. First, we provide a general theorem motivating how learning a model as an intermediate step can narrow down the set of possible value functions more than learning a value function directly from data using the Bellman equation. Second, we provide an illustrative example showing empirically how a similar effect occurs in a more concrete setting with neural network function approximation. Finally, we provide extensive experiments showing the benefit of model-based learning for online RL in environments with combinatorial complexity, but factored structure that allows a learned model to generalize. In these experiments, we take care to control for other factors in order to isolate, insofar as possible, the benefit of using experience generated by a learned model relative to ER alone. "
}