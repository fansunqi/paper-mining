{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Science Q&A"
  ],
  "datasets": [
    "Qasper"
  ],
  "methods": [
    "Iterated Decomposition",
    "ICE (open-source tool)"
  ],
  "results": [
    "25% to 65% improvement in describing the placebo",
    "53% to 70% improvement in evaluating participant adherence",
    "38% to 69% improvement in answering NLP questions"
  ],
  "paper_id": "63b63fd290e50fcafd8f5e01",
  "title": "Iterated Decomposition: Improving Science Q&A by Supervising Reasoning\n  Processes",
  "abstract": "  Language models (LMs) can perform complex reasoning either end-to-end, with hidden latent state, or compositionally, with transparent intermediate state. Composition offers benefits for interpretability and safety, but may need workflow support and infrastructure to remain competitive. We describe iterated decomposition, a human-in-the-loop workflow for developing and refining compositional LM programs. We improve the performance of compositions by zooming in on failing components and refining them through decomposition, additional context, chain of thought, etc. To support this workflow, we develop ICE, an open-source tool for visualizing the execution traces of LM programs. We apply iterated decomposition to three real-world tasks and improve the accuracy of LM programs over less compositional baselines: describing the placebo used in a randomized controlled trial (25% to 65%), evaluating participant adherence to a medical intervention (53% to 70%), and answering NLP questions on the Qasper dataset (38% to 69%). These applications serve as case studies for a workflow that, if automated, could keep ML systems interpretable and safe even as they scale to increasingly complex tasks. "
}