{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Out-of-distribution Detection in Deep Neural Networks"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "L2 normalization over feature space",
    "Neural Collapse"
  ],
  "results": [
    "Improved OoD detection scores and classification accuracy",
    "Comparable or superior OoD detection scores",
    "Substantially improves worst case OoD performance"
  ],
  "paper_id": "63292f6890e50fcafd2eb955",
  "title": "Linking Neural Collapse and L2 Normalization with Improved\n  Out-of-Distribution Detection in Deep Neural Networks",
  "abstract": "  We propose a simple modification to standard ResNet architectures--L2 normalization over feature space--that substantially improves out-of-distribution (OoD) performance on the previously proposed Deep Deterministic Uncertainty (DDU) benchmark. We show that this change also induces early Neural Collapse (NC), an effect linked to better OoD performance. Our method achieves comparable or superior OoD detection scores and classification accuracy in a small fraction of the training time of the benchmark. Additionally, it substantially improves worst case OoD performance over multiple, randomly initialized models. Though we do not suggest that NC is the sole mechanism or a comprehensive explanation for OoD behaviour in deep neural networks (DNN), we believe NC's simple mathematical and geometric structure can provide a framework for analysis of this complex phenomenon in future work. "
}