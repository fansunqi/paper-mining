{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Self-supervised speech learning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Adapters"
  ],
  "results": [
    "Over 90% parameter reduction",
    "Performance parity"
  ],
  "paper_id": "634781fe90e50fcafd2c1efa",
  "title": "Exploring Efficient-tuning Methods in Self-supervised Speech Models",
  "abstract": "  In this study, we aim to explore efficient tuning methods for speech self-supervised learning. Recent studies show that self-supervised learning (SSL) can learn powerful representations for different speech tasks. However, fine-tuning pre-trained models for each downstream task is parameter-inefficient since SSL models are notoriously large with millions of parameters. Adapters are lightweight modules commonly used in NLP to solve this problem. In downstream tasks, the parameters of SSL models are frozen, and only the adapters are trained. Given the lack of studies generally exploring the effectiveness of adapters for self-supervised speech tasks, we intend to fill this gap by adding various adapter modules in pre-trained speech SSL models. We show that the performance parity can be achieved with over 90% parameter reduction, and discussed the pros and cons of efficient tuning techniques. This is the first comprehensive investigation of various adapter types across speech tasks. "
}