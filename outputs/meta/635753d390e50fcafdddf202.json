{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Unsupervised learning",
    "Probabilistic neural network models"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Hierarchical Feature Model (HFM)"
  ],
  "results": [
    "Internal representation of learning machines can be independent of the data",
    "Only a finite number of features needed to describe datasets"
  ],
  "paper_id": "635753d390e50fcafdddf202",
  "title": "Occam learning",
  "abstract": "  We discuss probabilistic neural network models for unsupervised learning where the distribution of the hidden layer is fixed. We argue that learning machines with this architecture enjoy a number of desirable properties. For example, the model can be chosen as a simple and interpretable one, it does not need to be over-parametrised and training is argued to be efficient in a thermodynamic sense. When hidden units are binary variables, these models have a natural interpretation in terms of features. We show that the featureless state corresponds to a state of maximal ignorance about the features and that learning the first feature depends on non-Gaussian statistical properties of the data. We suggest that the distribution of hidden variables should be chosen according to the principle of maximal relevance. We introduce the Hierarchical Feature Model (HFM) as an example of a model that satisfies this principle, and that encodes a neutral a priori organisation of the feature space. We present extensive numerical experiments in order i) to test that the internal representation of learning machines can indeed be independent of the data with which they are trained and ii) that only a finite number of features are needed to describe a number of datasets. "
}