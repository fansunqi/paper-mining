{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Object Detection"
  ],
  "datasets": [
    "benchmark dataset"
  ],
  "methods": [
    "poison-only backdoor attack"
  ],
  "results": [
    "effectiveness in both digital and physical-world settings",
    "resistance to potential defenses"
  ],
  "paper_id": "636dbe6990e50fcafd79aade",
  "title": "Untargeted Backdoor Attack against Object Detection",
  "abstract": "  Recent studies revealed that deep neural networks (DNNs) are exposed to backdoor threats when training with third-party resources (such as training samples or backbones). The backdoored model has promising performance in predicting benign samples, whereas its predictions can be maliciously manipulated by adversaries based on activating its backdoors with pre-defined trigger patterns. Currently, most of the existing backdoor attacks were conducted on the image classification under the targeted manner. In this paper, we reveal that these threats could also happen in object detection, posing threatening risks to many mission-critical applications ($e.g.$, pedestrian detection and intelligent surveillance systems). Specifically, we design a simple yet effective poison-only backdoor attack in an untargeted manner, based on task characteristics. We show that, once the backdoor is embedded into the target model by our attack, it can trick the model to lose detection of any object stamped with our trigger patterns. We conduct extensive experiments on the benchmark dataset, showing its effectiveness in both digital and physical-world settings and its resistance to potential defenses. "
}