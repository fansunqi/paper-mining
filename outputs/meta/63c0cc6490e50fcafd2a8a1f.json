{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Entity resolution"
  ],
  "datasets": [
    "textual dataset",
    "online product domain",
    "citation datasets"
  ],
  "methods": [
    "Knowledge Augmented Entity Resolution",
    "pre-trained language models",
    "knowledge augmentation",
    "prompting methods"
  ],
  "results": [
    "KAER improves on Ditto",
    "more robust and better results on 'dirty data'",
    "outperforms baseline models on textual dataset and online product domain",
    "competitive results on domain-specific datasets"
  ],
  "paper_id": "63c0cc6490e50fcafd2a8a1f",
  "title": "KAER: A Knowledge Augmented Pre-Trained Language Model for Entity\n  Resolution",
  "abstract": "  Entity resolution has been an essential and well-studied task in data cleaning research for decades. Existing work has discussed the feasibility of utilizing pre-trained language models to perform entity resolution and achieved promising results. However, few works have discussed injecting domain knowledge to improve the performance of pre-trained language models on entity resolution tasks. In this study, we propose Knowledge Augmented Entity Resolution (KAER), a novel framework named for augmenting pre-trained language models with external knowledge for entity resolution. We discuss the results of utilizing different knowledge augmentation and prompting methods to improve entity resolution performance. Our model improves on Ditto, the existing state-of-the-art entity resolution method. In particular, 1) KAER performs more robustly and achieves better results on \"dirty data\", and 2) with more general knowledge injection, KAER outperforms the existing baseline models on the textual dataset and dataset from the online product domain. 3) KAER achieves competitive results on highly domain-specific datasets, such as citation datasets, requiring the injection of expert knowledge in future work. "
}