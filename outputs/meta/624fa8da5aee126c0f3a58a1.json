{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Nonlinear Generalized Nash Equilibrium Problems"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Quadratic Penalty Method (QPM)",
    "Augmented Lagrangian Method (ALM)",
    "Accelerated Mirror-Prox Algorithm"
  ],
  "results": [
    "Global convergence guarantee for monotone and strongly monotone NGNEPs",
    "Nonasymptotic complexity bounds in terms of gradient evaluations",
    "Experimental efficiency demonstration"
  ],
  "paper_id": "624fa8da5aee126c0f3a58a1",
  "title": "First-Order Algorithms for Nonlinear Generalized Nash Equilibrium\n  Problems",
  "abstract": "  We consider the problem of computing an equilibrium in a class of \\textit{nonlinear generalized Nash equilibrium problems (NGNEPs)} in which the strategy sets for each player are defined by equality and inequality constraints that may depend on the choices of rival players. While the asymptotic global convergence and local convergence rates of algorithms to solve this problem have been extensively investigated, the analysis of nonasymptotic iteration complexity is still in its infancy. This paper presents two first-order algorithms -- based on the quadratic penalty method (QPM) and augmented Lagrangian method (ALM), respectively -- with an accelerated mirror-prox algorithm as the solver in each inner loop. We establish a global convergence guarantee for solving monotone and strongly monotone NGNEPs and provide nonasymptotic complexity bounds expressed in terms of the number of gradient evaluations. Experimental results demonstrate the efficiency of our algorithms in practice. "
}