{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Language Modeling",
    "NLP tasks fine-tuning"
  ],
  "datasets": [
    "Large Arabic Corpus (over 500 GB)"
  ],
  "methods": [
    "Large pre-trained Language Models",
    "Fine-tuning on typical NLP tasks"
  ],
  "results": [
    "Significant boost from 4.5 to 8.5% on NLP tasks compared to mBERT"
  ],
  "paper_id": "61ef6ad75244ab9dcb67dce7",
  "title": "A Large and Diverse Arabic Corpus for Language Modeling",
  "abstract": "  Language models (LMs) have introduced a major paradigm shift in Natural Language Processing (NLP) modeling where large pre-trained LMs became integral to most of the NLP tasks. The LMs are intelligent enough to find useful and relevant representations of the language without any supervision. Perhaps, these models are used to fine-tune typical NLP tasks with significantly high accuracy as compared to the traditional approaches. Conversely, the training of these models requires a massively large corpus that is a good representation of the language. English LMs generally perform better than their other language counterparts, due to the availability of massive English corpora. This work elaborates on the design and development of a large Arabic corpus. It consists of over 500 GB of Arabic cleaned text targeted at improving cross-domain knowledge and downstream generalization capability of large-scale language models. Moreover, the corpus is utilized in the training of a large Arabic LM. In order to evaluate the effectiveness of the LM, a number of typical NLP tasks are fine-tuned. The tasks demonstrate a significant boost from 4.5 to 8.5% when compared to tasks fine-tuned on multi-lingual BERT (mBERT). To the best of my knowledge, this is currently the largest clean and diverse Arabic corpus ever collected. "
}