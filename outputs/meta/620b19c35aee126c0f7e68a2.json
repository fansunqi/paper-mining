{
  "code_links": [
    "https://github.com/ZJLAB-AMMI/PFGE"
  ],
  "tasks": [
    "Improving generalization in deep learning systems"
  ],
  "datasets": [
    "CIFAR-10",
    "CIFAR-100",
    "ImageNet"
  ],
  "methods": [
    "Parsimonious Fast Geometric Ensembling (PFGE)",
    "Stochastic Weight Averaging"
  ],
  "results": [
    "5x memory efficiency than prior art methods",
    "No compromise in generalization performance"
  ],
  "paper_id": "620b19c35aee126c0f7e68a2",
  "title": "PFGE: Parsimonious Fast Geometric Ensembling of DNNs",
  "abstract": "  Ensemble methods have been widely used to improve the performance of machine learning methods in terms of generalization, while they are hard to use in deep learning systems, as training an ensemble of deep neural networks (DNNs) incurs an extremely higher computational overhead of model training. Recently, advanced techniques such as fast geometric ensembling (FGE) and snapshot ensemble have been proposed. These methods can train the model ensembles in the same time as a single model, thus getting around the hurdle of training time. However, their memory overhead for test-time inference remains much higher than single model based methods. Here we propose a parsimonious FGE (PFGE) that employs a lightweight ensemble of higher-performing DNNs, generated by successively-performed stochastic weight averaging procedures. Experimental results across different modern DNN architectures on widely used image datasets CIFAR-$\\{10,100\\}$ and Imagenet, demonstrate that PFGE can achieve 5x memory efficiency than prior art methods, yet without compromise in generalization performance. Our code is available at https://github.com/ZJLAB-AMMI/PFGE. "
}