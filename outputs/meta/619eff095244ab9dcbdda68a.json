{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Single-source Domain Generalization for Medical Image Segmentation"
  ],
  "datasets": [
    "cross-modality (CT-MRI) abdominal image segmentation",
    "cross-sequence (bSSFP-LGE) cardiac MRI segmentation",
    "cross-center prostate MRI segmentation"
  ],
  "methods": [
    "Causality-inspired data augmentation",
    "Randomly-weighted shallow networks for appearance transformations",
    "Causal intervention for removing spurious correlations"
  ],
  "results": [
    "Consistent performance gains compared with competitive methods on unseen domains"
  ],
  "paper_id": "619eff095244ab9dcbdda68a",
  "title": "Causality-inspired Single-source Domain Generalization for Medical Image\n  Segmentation",
  "abstract": "  Deep learning models usually suffer from domain shift issues, where models trained on one source domain do not generalize well to other unseen domains. In this work, we investigate the single-source domain generalization problem: training a deep network that is robust to unseen domains, under the condition that training data is only available from one source domain, which is common in medical imaging applications. We tackle this problem in the context of cross-domain medical image segmentation. Under this scenario, domain shifts are mainly caused by different acquisition processes. We propose a simple causality-inspired data augmentation approach to expose a segmentation model to synthesized domain-shifted training examples. Specifically, 1) to make the deep model robust to discrepancies in image intensities and textures, we employ a family of randomly-weighted shallow networks. They augment training images using diverse appearance transformations. 2) Further we show that spurious correlations among objects in an image are detrimental to domain robustness. These correlations might be taken by the network as domain-specific clues for making predictions, and they may break on unseen domains. We remove these spurious correlations via causal intervention. This is achieved by resampling the appearances of potentially correlated objects independently. The proposed approach is validated on three cross-domain segmentation tasks: cross-modality (CT-MRI) abdominal image segmentation, cross-sequence (bSSFP-LGE) cardiac MRI segmentation, and cross-center prostate MRI segmentation. The proposed approach yields consistent performance gains compared with competitive methods when tested on unseen domains. "
}