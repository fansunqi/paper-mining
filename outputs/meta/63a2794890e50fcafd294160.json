{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Domain adaptation of GANs",
    "One-shot and Few-shot Domain Adaptation"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "StyleGAN parameterizations",
    "StyleDomain directions",
    "Affine$+$ and AffineLight$+$ parameterizations"
  ],
  "results": [
    "None"
  ],
  "paper_id": "63a2794890e50fcafd294160",
  "title": "StyleDomain: Efficient and Lightweight Parameterizations of StyleGAN for\n  One-shot and Few-shot Domain Adaptation",
  "abstract": "  Domain adaptation of GANs is a problem of fine-tuning the state-of-the-art GAN models (e.g. StyleGAN) pretrained on a large dataset to a specific domain with few samples (e.g. painting faces, sketches, etc.). While there are a great number of methods that tackle this problem in different ways, there are still many important questions that remain unanswered.   In this paper, we provide a systematic and in-depth analysis of the domain adaptation problem of GANs, focusing on the StyleGAN model. First, we perform a detailed exploration of the most important parts of StyleGAN that are responsible for adapting the generator to a new domain depending on the similarity between the source and target domains. As a result of this in-depth study, we propose new efficient and lightweight parameterizations of StyleGAN for domain adaptation. Particularly, we show there exist directions in StyleSpace (StyleDomain directions) that are sufficient for adapting to similar domains and they can be reduced further. For dissimilar domains, we propose Affine$+$ and AffineLight$+$ parameterizations that allows us to outperform existing baselines in few-shot adaptation with low data regime. Finally, we examine StyleDomain directions and discover their many surprising properties that we apply for domain mixing and cross-domain image morphing. "
}