{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Visual Spatial Reasoning"
  ],
  "datasets": [
    "Visual Spatial Reasoning (VSR)"
  ],
  "methods": [
    "None"
  ],
  "results": [
    "Human ceiling above 95%",
    "State-of-the-art models achieve around 70%",
    "Little correlation between VLMs' by-relation performances and number of training examples",
    "Models incapable of recognising relations concerning object orientations"
  ],
  "paper_id": "62708f615aee126c0fa6906b",
  "title": "Visual Spatial Reasoning",
  "abstract": "  Spatial relations are a basic part of human cognition. However, they are expressed in natural language in a variety of ways, and previous work has suggested that current vision-and-language models (VLMs) struggle to capture relational information. In this paper, we present Visual Spatial Reasoning (VSR), a dataset containing more than 10k natural text-image pairs with 66 types of spatial relations in English (such as: under, in front of, and facing). While using a seemingly simple annotation format, we show how the dataset includes challenging linguistic phenomena, such as varying reference frames. We demonstrate a large gap between human and model performance: the human ceiling is above 95%, while state-of-the-art models only achieve around 70%. We observe that VLMs' by-relation performances have little correlation with the number of training examples and the tested models are in general incapable of recognising relations concerning the orientations of objects. "
}