{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Low Dimensional Sensing Mapless Navigation of Terrestrial Mobile Robots"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Deep Q-Network (DQN)",
    "Double Deep Q-Network (DDQN)"
  ],
  "results": [
    "Enhanced navigation for mobile robots",
    "Double Deep structures improve over simple Q structures"
  ],
  "paper_id": "63d340ef90e50fcafd91172e",
  "title": "Double Deep Reinforcement Learning Techniques for Low Dimensional\n  Sensing Mapless Navigation of Terrestrial Mobile Robots",
  "abstract": "  In this work, we present two Deep Reinforcement Learning (Deep-RL) approaches to enhance the problem of mapless navigation for a terrestrial mobile robot. Our methodology focus on comparing a Deep-RL technique based on the Deep Q-Network (DQN) algorithm with a second one based on the Double Deep Q-Network (DDQN) algorithm. We use 24 laser measurement samples and the relative position and angle of the agent to the target as information for our agents, which provide the actions as velocities for our robot. By using a low-dimensional sensing structure of learning, we show that it is possible to train an agent to perform navigation-related tasks and obstacle avoidance without using complex sensing information. The proposed methodology was successfully used in three distinct simulated environments. Overall, it was shown that Double Deep structures further enhance the problem for the navigation of mobile robots when compared to the ones with simple Q structures. "
}