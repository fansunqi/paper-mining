{
  "code_links": [
    "https://ori-drs.github.io/lfmc/"
  ],
  "tasks": [
    "Robotic locomotion"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Deep reinforcement learning (RL)",
    "Low-frequency motion control"
  ],
  "results": [
    "Low-frequency policies are less sensitive to actuation latencies and variations in system dynamics",
    "Sim-to-real transfer without dynamics randomization or actuation modeling"
  ],
  "paper_id": "63365e7d90e50fcafd1a30fa",
  "title": "Learning Low-Frequency Motion Control for Robust and Dynamic Robot\n  Locomotion",
  "abstract": "  Robotic locomotion is often approached with the goal of maximizing robustness and reactivity by increasing motion control frequency. We challenge this intuitive notion by demonstrating robust and dynamic locomotion with a learned motion controller executing at as low as 8 Hz on a real ANYmal C quadruped. The robot is able to robustly and repeatably achieve a high heading velocity of 1.5 m/s, traverse uneven terrain, and resist unexpected external perturbations. We further present a comparative analysis of deep reinforcement learning (RL) based motion control policies trained and executed at frequencies ranging from 5 Hz to 200 Hz. We show that low-frequency policies are less sensitive to actuation latencies and variations in system dynamics. This is to the extent that a successful sim-to-real transfer can be performed even without any dynamics randomization or actuation modeling. We support this claim through a set of rigorous empirical evaluations. Moreover, to assist reproducibility, we provide the training and deployment code along with an extended analysis at https://ori-drs.github.io/lfmc/. "
}