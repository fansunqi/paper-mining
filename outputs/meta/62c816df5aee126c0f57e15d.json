{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Hyperparameter Optimization"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Conformal Hyperparameter Optimization"
  ],
  "results": [
    "Superior performance to random search"
  ],
  "paper_id": "62c816df5aee126c0f57e15d",
  "title": "Parallel Conformal Hyperparameter Optimization",
  "abstract": "  Several novel frameworks for hyperparameter search have emerged in the last decade, but most rely on strict, often normal, distributional assumptions, limiting search model flexibility. This paper proposes a novel optimization framework based on upper confidence bound sampling of conformal confidence intervals, whose assumption of exchangeability enables greater choice of search model architectures. Several such architectures were explored and benchmarked on hyperparameter tuning of both dense and convolutional neural networks, displaying superior performance to random search. "
}