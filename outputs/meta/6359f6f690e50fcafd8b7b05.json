{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Multi-Domain Long-Tailed Learning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Selective balanced sampling strategy",
    "Domain-invariant class prototype"
  ],
  "results": [
    "Consistently outperforms other state-of-the-art methods in both subpopulation shift and domain shift"
  ],
  "paper_id": "6359f6f690e50fcafd8b7b05",
  "title": "Multi-Domain Long-Tailed Learning by Augmenting Disentangled\n  Representations",
  "abstract": "  There is an inescapable long-tailed class-imbalance issue in many real-world classification problems. Existing long-tailed classification methods focus on the single-domain setting, where all examples are drawn from the same distribution. However, real-world scenarios often involve multiple domains with distinct imbalanced class distributions. We study this multi-domain long-tailed learning problem and aim to produce a model that generalizes well across all classes and domains. Towards that goal, we introduce TALLY, which produces invariant predictors by balanced augmenting hidden representations over domains and classes. Built upon a proposed selective balanced sampling strategy, TALLY achieves this by mixing the semantic representation of one example with the domain-associated nuisances of another, producing a new representation for use as data augmentation. To improve the disentanglement of semantic representations, TALLY further utilizes a domain-invariant class prototype that averages out domain-specific effects. We evaluate TALLY on four long-tailed variants of classical domain generalization benchmarks and two real-world imbalanced multi-domain datasets. The results indicate that TALLY consistently outperforms other state-of-the-art methods in both subpopulation shift and domain shift. "
}