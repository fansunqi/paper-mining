{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Hyperparameter Optimization",
    "Interpretable Machine Learning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Bayesian Optimization",
    "Bayesian Algorithm Execution"
  ],
  "results": [
    "More reliable explanations of the underlying black-box without a loss of optimization performance"
  ],
  "paper_id": "62a7fc625aee126c0ff5dc91",
  "title": "Improving Accuracy of Interpretability Measures in Hyperparameter\n  Optimization via Bayesian Algorithm Execution",
  "abstract": "  Despite all the benefits of automated hyperparameter optimization (HPO), most modern HPO algorithms are black-boxes themselves. This makes it difficult to understand the decision process which leads to the selected configuration, reduces trust in HPO, and thus hinders its broad adoption. Here, we study the combination of HPO with interpretable machine learning (IML) methods such as partial dependence plots. These techniques are more and more used to explain the marginal effect of hyperparameters on the black-box cost function or to quantify the importance of hyperparameters. However, if such methods are naively applied to the experimental data of the HPO process in a post-hoc manner, the underlying sampling bias of the optimizer can distort interpretations. We propose a modified HPO method which efficiently balances the search for the global optimum w.r.t. predictive performance \\emph{and} the reliable estimation of IML explanations of an underlying black-box function by coupling Bayesian optimization and Bayesian Algorithm Execution. On benchmark cases of both synthetic objectives and HPO of a neural network, we demonstrate that our method returns more reliable explanations of the underlying black-box without a loss of optimization performance. "
}