{
  "code_links": [
    "https://github.com/aradha/recursive_feature_machines"
  ],
  "tasks": [
    "Feature learning in neural networks",
    "Understanding neural network performance"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Recursive Feature Machines (RFMs)",
    "Average gradient outer product"
  ],
  "results": [
    "RFMs accurately capture features learned by deep fully connected neural networks",
    "Outperform a broad spectrum of models including neural networks on tabular data",
    "Shed light on deep learning phenomena"
  ],
  "paper_id": "63ae56c890e50fcafda9583d",
  "title": "Feature learning in neural networks and kernel machines that recursively\n  learn features",
  "abstract": "  Neural networks have achieved impressive results on many technological and scientific tasks. Yet, their empirical successes have outpaced our fundamental understanding of their structure and function. Identifying mechanisms driving the successes of neural networks can provide principled approaches for improving neural network performance and developing simple and effective alternatives. In this work, we isolate a key mechanism driving feature learning in fully connected neural networks by connecting neural feature learning to a statistical estimator known as average gradient outer product. We subsequently leverage this mechanism to design \\textit{Recursive Feature Machines} (RFMs), which are kernel machines that learn features. We show that RFMs (1) accurately capture features learned by deep fully connected neural networks, and (2) outperform a broad spectrum of models including neural networks on tabular data. Furthermore, we show how RFMs shed light on recently observed deep learning phenomena including grokking, lottery tickets, simplicity biases, and spurious features. We provide a Python implementation to make our method easily accessible [\\url{https://github.com/aradha/recursive_feature_machines}]. "
}