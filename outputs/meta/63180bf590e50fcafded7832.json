{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Generalisation under gradient descent"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "deterministic PAC-Bayes",
    "gradient descent methods",
    "continuous gradient flows",
    "stochastic gradient descent (SGD)",
    "momentum-based schemes",
    "damped Hamiltonian dynamics"
  ],
  "results": [
    "None"
  ],
  "paper_id": "63180bf590e50fcafded7832",
  "title": "Generalisation under gradient descent via deterministic PAC-Bayes",
  "abstract": "  We establish disintegrated PAC-Bayesian generalisation bounds for models trained with gradient descent methods or continuous gradient flows. Contrary to standard practice in the PAC-Bayesian setting, our result applies to optimisation algorithms that are deterministic, without requiring any de-randomisation step. Our bounds are fully computable, depending on the density of the initial distribution and the Hessian of the training objective over the trajectory. We show that our framework can be applied to a variety of iterative optimisation algorithms, including stochastic gradient descent (SGD), momentum-based schemes, and damped Hamiltonian dynamics. "
}