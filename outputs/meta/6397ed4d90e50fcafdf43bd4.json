{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Audio-visual speech recognition (AVSR)"
  ],
  "datasets": [
    "LRS3"
  ],
  "methods": [
    "Reinforcement learning (RL) based framework called MSRL"
  ],
  "results": [
    "State-of-the-art performance in clean and various noisy conditions",
    "Better generality than other baselines with unseen noises"
  ],
  "paper_id": "6397ed4d90e50fcafdf43bd4",
  "title": "Leveraging Modality-specific Representations for Audio-visual Speech\n  Recognition via Reinforcement Learning",
  "abstract": "  Audio-visual speech recognition (AVSR) has gained remarkable success for ameliorating the noise-robustness of speech recognition. Mainstream methods focus on fusing audio and visual inputs to obtain modality-invariant representations. However, such representations are prone to over-reliance on audio modality as it is much easier to recognize than video modality in clean conditions. As a result, the AVSR model underestimates the importance of visual stream in face of noise corruption. To this end, we leverage visual modality-specific representations to provide stable complementary information for the AVSR task. Specifically, we propose a reinforcement learning (RL) based framework called MSRL, where the agent dynamically harmonizes modality-invariant and modality-specific representations in the auto-regressive decoding process. We customize a reward function directly related to task-specific metrics (i.e., word error rate), which encourages the MSRL to effectively explore the optimal integration strategy. Experimental results on the LRS3 dataset show that the proposed method achieves state-of-the-art in both clean and various noisy conditions. Furthermore, we demonstrate the better generality of MSRL system than other baselines when test set contains unseen noises. "
}