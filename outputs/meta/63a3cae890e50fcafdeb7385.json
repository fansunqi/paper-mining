{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Reinforcement Learning",
    "Exploration-Exploitation Trade-off"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Neighboring state-based, model-free exploration",
    "${\rho}$-explore"
  ],
  "results": [
    "49% improvement over Double DQN baseline in terms of Eval Reward Return"
  ],
  "paper_id": "63a3cae890e50fcafdeb7385",
  "title": "Neighboring state-based RL Exploration",
  "abstract": "  Reinforcement Learning is a powerful tool to model decision-making processes. However, it relies on an exploration-exploitation trade-off that remains an open challenge for many tasks. In this work, we study neighboring state-based, model-free exploration led by the intuition that, for an early-stage agent, considering actions derived from a bounded region of nearby states may lead to better actions when exploring. We propose two algorithms that choose exploratory actions based on a survey of nearby states, and find that one of our methods, ${\\rho}$-explore, consistently outperforms the Double DQN baseline in an discrete environment by 49\\% in terms of Eval Reward Return. "
}