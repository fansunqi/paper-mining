{
  "code_links": [
    "https://github.com/deeplearning-wisc/cider"
  ],
  "tasks": [
    "Out-of-distribution detection"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "CIDER, hyperspherical embeddings, dispersion loss, compactness loss"
  ],
  "results": [
    "Outperforms latest rival by 19.36% in FPR95"
  ],
  "paper_id": "62296c7a5aee126c0f57d1a1",
  "title": "How to Exploit Hyperspherical Embeddings for Out-of-Distribution\n  Detection?",
  "abstract": "  Out-of-distribution (OOD) detection is a critical task for reliable machine learning. Recent advances in representation learning give rise to distance-based OOD detection, where testing samples are detected as OOD if they are relatively far away from the centroids or prototypes of in-distribution (ID) classes. However, prior methods directly take off-the-shelf contrastive losses that suffice for classifying ID samples, but are not optimally designed when test inputs contain OOD samples. In this work, we propose CIDER, a novel representation learning framework that exploits hyperspherical embeddings for OOD detection. CIDER jointly optimizes two losses to promote strong ID-OOD separability: a dispersion loss that promotes large angular distances among different class prototypes, and a compactness loss that encourages samples to be close to their class prototypes. We analyze and establish the unexplored relationship between OOD detection performance and the embedding properties in the hyperspherical space, and demonstrate the importance of dispersion and compactness. CIDER establishes superior performance, outperforming the latest rival by 19.36% in FPR95. Code is available at https://github.com/deeplearning-wisc/cider. "
}