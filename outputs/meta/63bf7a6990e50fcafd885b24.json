{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Outlier detection"
  ],
  "datasets": [
    "20 data sets"
  ],
  "methods": [
    "ODIM: an outlier detection method exploiting the inlier-memorization effect"
  ],
  "results": [
    "ODIM is computationally efficient, tens of times faster than other deep-learning-based algorithms",
    "ODIM filters out outliers successfully for various data types"
  ],
  "paper_id": "63bf7a6990e50fcafd885b24",
  "title": "ODIM: an efficient method to detect outliers via inlier-memorization\n  effect of deep generative models",
  "abstract": "  Identifying whether a given sample is an outlier or not is an important issue in various real-world domains. This study aims to solve the unsupervised outlier detection problem where training data contain outliers, but any label information about inliers and outliers is not given. We propose a powerful and efficient learning framework to identify outliers in a training data set using deep neural networks. We start with a new observation called the inlier-memorization (IM) effect. When we train a deep generative model with data contaminated with outliers, the model first memorizes inliers before outliers. Exploiting this finding, we develop a new method called the outlier detection via the IM effect (ODIM). The ODIM only requires a few updates; thus, it is computationally efficient, tens of times faster than other deep-learning-based algorithms. Also, the ODIM filters out outliers successfully, regardless of the types of data, such as tabular, image, and sequential. We empirically demonstrate the superiority and efficiency of the ODIM by analyzing 20 data sets. "
}