{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Mobile Robotics"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "User-Conditioned Neural Control Policies",
    "Feature-wise Linear Modulation Layer (FiLM)",
    "Model-free reinforcement-learning"
  ],
  "results": [
    "Close to time-optimal flight performance",
    "Up to 60 km/h and 4.5g in acceleration"
  ],
  "paper_id": "637d8f5c90e50fcafd6e70d6",
  "title": "User-Conditioned Neural Control Policies for Mobile Robotics",
  "abstract": "  Recently, learning-based controllers have been shown to push mobile robotic systems to their limits and provide the robustness needed for many real-world applications. However, only classical optimization-based control frameworks offer the inherent flexibility to be dynamically adjusted during execution by, for example, setting target speeds or actuator limits. We present a framework to overcome this shortcoming of neural controllers by conditioning them on an auxiliary input. This advance is enabled by including a feature-wise linear modulation layer (FiLM). We use model-free reinforcement-learning to train quadrotor control policies for the task of navigating through a sequence of waypoints in minimum time. By conditioning the policy on the maximum available thrust or the viewing direction relative to the next waypoint, a user can regulate the aggressiveness of the quadrotor's flight during deployment. We demonstrate in simulation and in real-world experiments that a single control policy can achieve close to time-optimal flight performance across the entire performance envelope of the robot, reaching up to 60 km/h and 4.5g in acceleration. The ability to guide a learned controller during task execution has implications beyond agile quadrotor flight, as conditioning the control policy on human intent helps safely bringing learning based systems out of the well-defined laboratory environment into the wild. "
}