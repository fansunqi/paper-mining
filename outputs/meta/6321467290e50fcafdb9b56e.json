{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Automatic Speech Recognition (ASR)"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "ASR pathways",
    "sparse multilingual ASR model",
    "language-specific sub-networks",
    "overlapping sub-networks",
    "joint multilingual training"
  ],
  "results": [
    "outperforms dense models and language-agnostically pruned model",
    "better performance on low-resource languages compared to monolingual sparse models"
  ],
  "paper_id": "6321467290e50fcafdb9b56e",
  "title": "Learning ASR pathways: A sparse multilingual ASR model",
  "abstract": "  Neural network pruning compresses automatic speech recognition (ASR) models effectively. However, in multilingual ASR, language-agnostic pruning may lead to severe performance drops on some languages because language-agnostic pruning masks may not fit all languages and discard important language-specific parameters. In this work, we present ASR pathways, a sparse multilingual ASR model that activates language-specific sub-networks (\"pathways\"), such that the parameters for each language are learned explicitly. With the overlapping sub-networks, the shared parameters can also enable knowledge transfer for lower-resource languages via joint multilingual training. We propose a novel algorithm to learn ASR pathways, and evaluate the proposed method on 4 languages with a streaming RNN-T model. Our proposed ASR pathways outperform both dense models and a language-agnostically pruned model, and provide better performance on low-resource languages compared to the monolingual sparse models. "
}