{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Link Prediction",
    "Question Answering"
  ],
  "datasets": [
    "Aviation Knowledge Graph",
    "National Transportation Safety Board (NTSB) reports"
  ],
  "methods": [
    "T5",
    "BLOOM"
  ],
  "results": [
    "0.2 hits@1 score for link prediction",
    "0.7 hits@1 score for QA",
    "Cohen's kappa score of 0.76 for link prediction"
  ],
  "paper_id": "63be28d490e50fcafdf52e54",
  "title": "There is No Big Brother or Small Brother: Knowledge Infusion in Language\n  Models for Link Prediction and Question Answering",
  "abstract": "  The integration of knowledge graphs with deep learning is thriving in improving the performance of various natural language processing (NLP) tasks. In this paper, we focus on knowledge-infused link prediction and question answering using language models, T5, and BLOOM across three domains: Aviation, Movie, and Web. In this context, we infuse knowledge in large and small language models and study their performance, and find the performance to be similar. For the link prediction task on the Aviation Knowledge Graph, we obtain a 0.2 hits@1 score using T5-small, T5-base, T5-large, and BLOOM. Using template-based scripts, we create a set of 1 million synthetic factoid QA pairs in the aviation domain from National Transportation Safety Board (NTSB) reports. On our curated QA pairs, the three models of T5 achieve a 0.7 hits@1 score. We validate out findings with the paired student t-test and Cohen's kappa scores. For link prediction on Aviation Knowledge Graph using T5-small and T5-large, we obtain a Cohen's kappa score of 0.76, showing substantial agreement between the models. Thus, we infer that small language models perform similar to large language models with the infusion of knowledge. "
}