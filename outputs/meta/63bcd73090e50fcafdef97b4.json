{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Modular arithmetic learning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Neural network",
    "Grokking",
    "Feature maps",
    "Vanilla gradient descent",
    "MSE loss function",
    "AdamW"
  ],
  "results": [
    "Grokking on various modular arithmetic tasks",
    "Analytic expressions for weights",
    "Complete interpretability of the representations"
  ],
  "paper_id": "63bcd73090e50fcafdef97b4",
  "title": "Grokking modular arithmetic",
  "abstract": "  We present a simple neural network that can learn modular arithmetic tasks and exhibits a sudden jump in generalization known as ``grokking''. Concretely, we present (i) fully-connected two-layer networks that exhibit grokking on various modular arithmetic tasks under vanilla gradient descent with the MSE loss function in the absence of any regularization; (ii) evidence that grokking modular arithmetic corresponds to learning specific feature maps whose structure is determined by the task; (iii) analytic expressions for the weights -- and thus for the feature maps -- that solve a large class of modular arithmetic tasks; and (iv) evidence that these feature maps are also found by vanilla gradient descent as well as AdamW, thereby establishing complete interpretability of the representations learnt by the network. "
}