{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Semantic parsing",
    "Domain adaptation"
  ],
  "datasets": [
    "TOPv2",
    "SNIPS"
  ],
  "methods": [
    "seq2seq architecture",
    "concept encoder",
    "concept pretraining with Wikidata"
  ],
  "results": [
    "Outperforms prior approaches in few-shot settings for TOPv2 and SNIPS datasets"
  ],
  "paper_id": "63d340e790e50fcafd910633",
  "title": "Low-Resource Compositional Semantic Parsing with Concept Pretraining",
  "abstract": "  Semantic parsing plays a key role in digital voice assistants such as Alexa, Siri, and Google Assistant by mapping natural language to structured meaning representations. When we want to improve the capabilities of a voice assistant by adding a new domain, the underlying semantic parsing model needs to be retrained using thousands of annotated examples from the new domain, which is time-consuming and expensive. In this work, we present an architecture to perform such domain adaptation automatically, with only a small amount of metadata about the new domain and without any new training data (zero-shot) or with very few examples (few-shot). We use a base seq2seq (sequence-to-sequence) architecture and augment it with a concept encoder that encodes intent and slot tags from the new domain. We also introduce a novel decoder-focused approach to pretrain seq2seq models to be concept aware using Wikidata and use it to help our model learn important concepts and perform well in low-resource settings. We report few-shot and zero-shot results for compositional semantic parsing on the TOPv2 dataset and show that our model outperforms prior approaches in few-shot settings for the TOPv2 and SNIPS datasets. "
}