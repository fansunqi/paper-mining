{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Maximization of mutual information",
    "Multi-view representation learning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Difference of matrix-based entropies (DiME)"
  ],
  "results": [
    "None"
  ],
  "paper_id": "63ca06a190e50fcafd683819",
  "title": "DiME: Maximizing Mutual Information by a Difference of Matrix-Based\n  Entropies",
  "abstract": "  We introduce an information-theoretic quantity with similar properties to mutual information that can be estimated from data without making explicit assumptions on the underlying distribution. This quantity is based on a recently proposed matrix-based entropy that uses the eigenvalues of a normalized Gram matrix to compute an estimate of the eigenvalues of an uncentered covariance operator in a reproducing kernel Hilbert space. We show that a difference of matrix-based entropies (DiME) is well suited for problems involving maximization of mutual information between random variables. While many methods for such tasks can lead to trivial solutions, DiME naturally penalizes such outcomes. We provide several examples of use cases for the proposed quantity including a multi-view representation learning problem where DiME is used to encourage learning a shared representation among views with high mutual information. We also show the versatility of DiME by using it as objective function for a variety of tasks. "
}