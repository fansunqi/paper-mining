{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Transfer learning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "High-dimensional quantile regression models",
    "Double transfer learning estimator"
  ],
  "results": [
    "Lower error bounds for critical selection criterion and larger sample size",
    "Valid confidence interval and hypothesis test procedures",
    "Favorable performance in simulations"
  ],
  "paper_id": "6385788490e50fcafdf49812",
  "title": "Statistical inference for transfer learning with high-dimensional\n  quantile regression",
  "abstract": "  Transfer learning has become an essential technique to exploit information from the source domain to boost performance of the target task. Despite the prevalence in high-dimensional data, heterogeneity and/or heavy tails are insufficiently accounted for by current transfer learning approaches and thus may undermine the resulting performance. We propose a transfer learning procedure in the framework of high-dimensional quantile regression models to accommodate the heterogeneity and heavy tails in the source and target domains. We establish error bounds of the transfer learning estimator based on delicately selected transferable source domains, showing that lower error bounds can be achieved for critical selection criterion and larger sample size of source tasks. We further propose valid confidence interval and hypothesis test procedures for individual component of high-dimensional quantile regression coefficients by advocating a double transfer learning estimator, which is the one-step debiased estimator for the transfer learning estimator wherein the technique of transfer learning is designed again. Simulation results demonstrate that the proposed method exhibits some favorable performances, further corroborating our theoretical results. "
}