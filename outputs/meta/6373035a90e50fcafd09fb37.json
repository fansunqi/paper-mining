{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Graph structure learning",
    "Graph neural networks (GNNs) performance enhancement"
  ],
  "datasets": [
    "Cora"
  ],
  "methods": [
    "Graph structure refinement (GSR) framework",
    "Pretrain-finetune pipeline",
    "Multi-view contrastive learning",
    "Edge probability estimation"
  ],
  "results": [
    "Best performance on six benchmark datasets",
    "13.8x faster using 32.8% GPU memory compared to the best GSL baseline"
  ],
  "paper_id": "6373035a90e50fcafd09fb37",
  "title": "Self-Supervised Graph Structure Refinement for Graph Neural Networks",
  "abstract": "  Graph structure learning (GSL), which aims to learn the adjacency matrix for graph neural networks (GNNs), has shown great potential in boosting the performance of GNNs. Most existing GSL works apply a joint learning framework where the estimated adjacency matrix and GNN parameters are optimized for downstream tasks. However, as GSL is essentially a link prediction task, whose goal may largely differ from the goal of the downstream task. The inconsistency of these two goals limits the GSL methods to learn the potential optimal graph structure. Moreover, the joint learning framework suffers from scalability issues in terms of time and space during the process of estimation and optimization of the adjacency matrix. To mitigate these issues, we propose a graph structure refinement (GSR) framework with a pretrain-finetune pipeline. Specifically, The pre-training phase aims to comprehensively estimate the underlying graph structure by a multi-view contrastive learning framework with both intra- and inter-view link prediction tasks. Then, the graph structure is refined by adding and removing edges according to the edge probabilities estimated by the pre-trained model. Finally, the fine-tuning GNN is initialized by the pre-trained model and optimized toward downstream tasks. With the refined graph structure remaining static in the fine-tuning space, GSR avoids estimating and optimizing graph structure in the fine-tuning phase which enjoys great scalability and efficiency. Moreover, the fine-tuning GNN is boosted by both migrating knowledge and refining graphs. Extensive experiments are conducted to evaluate the effectiveness (best performance on six benchmark datasets), efficiency, and scalability (13.8x faster using 32.8% GPU memory compared to the best GSL baseline on Cora) of the proposed model. "
}