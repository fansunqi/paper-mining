{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Graph representation learning",
    "Expertise logic integration"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Structural Causal Model",
    "auxiliary causal logic learning paradigm",
    "counterfactual technique"
  ],
  "results": [
    "None"
  ],
  "paper_id": "63cdfab690e50fcafd106f69",
  "title": "Introducing Expertise Logic into Graph Representation Learning from A\n  Causal Perspective",
  "abstract": "  Benefiting from the injection of human prior knowledge, graphs, as derived discrete data, are semantically dense so that models can efficiently learn the semantic information from such data. Accordingly, graph neural networks (GNNs) indeed achieve impressive success in various fields. Revisiting the GNN learning paradigms, we discover that the relationship between human expertise and the knowledge modeled by GNNs still confuses researchers. To this end, we introduce motivating experiments and derive an empirical observation that the human expertise is gradually learned by the GNNs in general domains. By further observing the ramifications of introducing expertise logic into graph representation learning, we conclude that leading the GNNs to learn human expertise can improve the model performance. By exploring the intrinsic mechanism behind such observations, we elaborate the Structural Causal Model for the graph representation learning paradigm. Following the theoretical guidance, we innovatively introduce the auxiliary causal logic learning paradigm to improve the model to learn the expertise logic causally related to the graph representation learning task. In practice, the counterfactual technique is further performed to tackle the insufficient training issue during optimization. Plentiful experiments on the crafted and real-world domains support the consistent effectiveness of the proposed method. "
}