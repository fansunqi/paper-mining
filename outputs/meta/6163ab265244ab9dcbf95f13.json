{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Deep Metric Learning"
  ],
  "datasets": [
    "fine-grained images (specific datasets not named)"
  ],
  "methods": [
    "Multi Proxy Anchor Family Loss",
    "normalized discounted cumulative gain (nDCG@k) metric"
  ],
  "results": [
    "MPA-family losses achieve higher accuracy on two datasets for fine-grained images"
  ],
  "paper_id": "6163ab265244ab9dcbf95f13",
  "title": "Multi Proxy Anchor Family Loss for Several Types of Gradients",
  "abstract": "  The deep metric learning (DML) objective is to learn a neural network that maps into an embedding space where similar data are near and dissimilar data are far. However, conventional proxy-based losses for DML have two problems: gradient problem and application of the real-world dataset with multiple local centers. Additionally, the performance metrics of DML also have some issues with stability and flexibility. This paper proposes three multi-proxies anchor (MPA) family losses and a normalized discounted cumulative gain (nDCG@k) metric. This paper makes three contributions. (1) MPA-family losses can learn using a real-world dataset with multi-local centers. (2) MPA-family losses improve the training capacity of a neural network owing to solving the gradient problem. (3) MPA-family losses have data-wise or class-wise characteristics with respect to gradient generation. Finally, we demonstrate the effectiveness of MPA-family losses, and MPA-family losses achieves higher accuracy on two datasets for fine-grained images. "
}