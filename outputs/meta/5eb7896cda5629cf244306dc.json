{
  "code_links": "None",
  "tasks": [
    "Explainable AI",
    "Multi-class Explanation"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "LIMEtree",
    "Multi-output Regression Trees"
  ],
  "results": [
    "Offers faithful and consistent multiclass explanations",
    "Supports interactive customization of insights",
    "Demonstrates effectiveness across various scenarios in both quantitative and qualitative evaluations"
  ],
  "paper_id": "5eb7896cda5629cf244306dc",
  "title": "LIMEtree: Consistent and Faithful Surrogate Explanations of Multiple\n  Classes",
  "abstract": "  Explainable machine learning provides tools to better understand predictive models and their decisions, but many such methods are limited to producing insights with respect to a single class. When generating explanations for several classes, reasoning over them to obtain a complete view may be difficult since they can present competing or contradictory evidence. To address this issue we introduce a novel paradigm of multi-class explanations. We outline the theory behind such techniques and propose a local surrogate model based on multi-output regression trees -- called LIMEtree -- which offers faithful and consistent explanations of multiple classes for individual predictions while being post-hoc, model-agnostic and data-universal. In addition to strong fidelity guarantees, our implementation supports (interactive) customisation of the explanatory insights and delivers a range of diverse explanation types, including counterfactual statements favoured in the literature. We evaluate our algorithm with a collection of quantitative experiments, a qualitative analysis based on explainability desiderata and a preliminary user study on an image classification task, comparing it to LIME. Our contributions demonstrate the benefits of multi-class explanations and wide-ranging advantages of our method across a diverse set scenarios. "
}