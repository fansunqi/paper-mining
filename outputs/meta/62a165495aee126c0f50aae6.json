{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Neural Collapse"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Review of modelling principles",
    "Analysis of generalization and transfer learning capabilities"
  ],
  "results": [
    "Neural collapse represents a state with infinitesimally small within-class variability",
    "Simplifies last layer behavior to nearest-class center decision rule",
    "Dynamics and implications of reaching neural collapse are not fully understood"
  ],
  "paper_id": "62a165495aee126c0f50aae6",
  "title": "Neural Collapse: A Review on Modelling Principles and Generalization",
  "abstract": "  Deep classifier neural networks enter the terminal phase of training (TPT) when training error reaches zero and tend to exhibit intriguing Neural Collapse (NC) properties. Neural collapse essentially represents a state at which the within-class variability of final hidden layer outputs is infinitesimally small and their class means form a simplex equiangular tight frame. This simplifies the last layer behaviour to that of a nearest-class center decision rule. Despite the simplicity of this state, the dynamics and implications of reaching it are yet to be fully understood. In this work, we review the principles which aid in modelling neural collapse, followed by the implications of this state on generalization and transfer learning capabilities of neural networks. Finally, we conclude by discussing potential avenues and directions for future research. "
}