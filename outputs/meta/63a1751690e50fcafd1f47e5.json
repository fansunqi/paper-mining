{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Text Generation"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Difformer",
    "Transformer"
  ],
  "results": [
    "Superiority over compared embedding diffusion baselines"
  ],
  "paper_id": "63a1751690e50fcafd1f47e5",
  "title": "Difformer: Empowering Diffusion Models on the Embedding Space for Text\n  Generation",
  "abstract": "  Diffusion models have achieved state-of-the-art synthesis quality on both visual and audio tasks, and recent works further adapt them to textual data by diffusing on the embedding space. In this paper, we conduct systematic studies and analyze the challenges between the continuous data space and the embedding space which have not been carefully explored. Firstly, the data distribution is learnable for embeddings, which may lead to the collapse of the loss function. Secondly, as the norm of embeddings varies between popular and rare words, adding the same noise scale will lead to sub-optimal results. In addition, we find the normal level of noise causes insufficient training of the model. To address the above challenges, we propose Difformer, an embedding diffusion model based on Transformer, which consists of three essential modules including an additional anchor loss function, a layer normalization module for embeddings, and a noise factor to the Gaussian noise. Experiments on two seminal text generation tasks including machine translation and text summarization show the superiority of Difformer over compared embedding diffusion baselines. "
}