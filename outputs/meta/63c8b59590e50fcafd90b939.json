{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Open-Set Grounded Text-to-Image Generation"
  ],
  "datasets": [
    "COCO",
    "LVIS"
  ],
  "methods": [
    "Grounded-Language-to-Image Generation",
    "gated mechanism"
  ],
  "results": [
    "zero-shot performance on COCO and LVIS outperforms existing supervised layout-to-image baselines"
  ],
  "paper_id": "63c8b59590e50fcafd90b939",
  "title": "GLIGEN: Open-Set Grounded Text-to-Image Generation",
  "abstract": "  Large-scale text-to-image diffusion models have made amazing advances. However, the status quo is to use text input alone, which can impede controllability. In this work, we propose GLIGEN, Grounded-Language-to-Image Generation, a novel approach that builds upon and extends the functionality of existing pre-trained text-to-image diffusion models by enabling them to also be conditioned on grounding inputs. To preserve the vast concept knowledge of the pre-trained model, we freeze all of its weights and inject the grounding information into new trainable layers via a gated mechanism. Our model achieves open-world grounded text2img generation with caption and bounding box condition inputs, and the grounding ability generalizes well to novel spatial configurations and concepts. GLIGEN's zero-shot performance on COCO and LVIS outperforms that of existing supervised layout-to-image baselines by a large margin. "
}