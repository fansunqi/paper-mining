{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Neural NLP Model Interpretability",
    "Saliency Methods Evaluation"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Rank correlation analysis",
    "Pearson-$r$ correlation",
    "Regularization techniques"
  ],
  "results": [
    "Weak rank correlations in saliency methods",
    "Increased agreement with regularization",
    "Low agreement in easy-to-learn instances"
  ],
  "paper_id": "637454dd90e50fcafdfc6b1c",
  "title": "Easy to Decide, Hard to Agree: Reducing Disagreements Between Saliency\n  Methods",
  "abstract": "  A popular approach to unveiling the black box of neural NLP models is to leverage saliency methods, which assign scalar importance scores to each input component. A common practice for evaluating whether an interpretability method is faithful and plausible has been to use evaluation-by-agreement -- multiple methods agreeing on an explanation increases its credibility. However, recent work has found that even saliency methods have weak rank correlations and advocated for the use of alternative diagnostic methods. In our work, we demonstrate that rank correlation is not a good fit for evaluating agreement and argue that Pearson-$r$ is a better suited alternative. We show that regularization techniques that increase faithfulness of attention explanations also increase agreement between saliency methods. Through connecting our findings to instance categories based on training dynamics we show that, surprisingly, easy-to-learn instances exhibit low agreement in saliency method explanations. "
}