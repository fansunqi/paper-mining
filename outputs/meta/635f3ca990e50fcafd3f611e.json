{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Fairness in classification",
    "Differential privacy impact"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Lipschitz continuity analysis",
    "High probability bound"
  ],
  "results": [
    "Fairness level of private models close to non-private counterparts"
  ],
  "paper_id": "635f3ca990e50fcafd3f611e",
  "title": "Differential Privacy has Bounded Impact on Fairness in Classification",
  "abstract": "  We theoretically study the impact of differential privacy on fairness in classification. We prove that, given a class of models, popular group fairness measures are pointwise Lipschitz-continuous with respect to the parameters of the model. This result is a consequence of a more general statement on accuracy conditioned on an arbitrary event (such as membership to a sensitive group), which may be of independent interest. We use the aforementioned Lipschitz property to prove a high probability bound showing that, given enough examples, the fairness level of private models is close to the one of their non-private counterparts. "
}