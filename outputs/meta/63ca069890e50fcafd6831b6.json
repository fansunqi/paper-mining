{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Language generalizations in neural network models"
  ],
  "datasets": [
    "Bible translations in 1295 languages"
  ],
  "methods": [
    "Neural models for natural language processing tasks"
  ],
  "results": [
    "Some generalizations are close to traditional linguistic typology",
    "Most models do not make linguistically meaningful generalizations",
    "Resources released for linguistic evaluations"
  ],
  "paper_id": "63ca069890e50fcafd6831b6",
  "title": "Language Embeddings Sometimes Contain Typological Generalizations",
  "abstract": "  To what extent can neural network models learn generalizations about language structure, and how do we find out what they have learned? We explore these questions by training neural models for a range of natural language processing tasks on a massively multilingual dataset of Bible translations in 1295 languages. The learned language representations are then compared to existing typological databases as well as to a novel set of quantitative syntactic and morphological features obtained through annotation projection. We conclude that some generalizations are surprisingly close to traditional features from linguistic typology, but that most of our models, as well as those of previous work, do not appear to have made linguistically meaningful generalizations. Careful attention to details in the evaluation turns out to be essential to avoid false positives. Furthermore, to encourage continued work in this field, we release several resources covering most or all of the languages in our data: (i) multiple sets of language representations, (ii) multilingual word embeddings, (iii) projected and predicted syntactic and morphological features, (iv) software to provide linguistically sound evaluations of language representations. "
}