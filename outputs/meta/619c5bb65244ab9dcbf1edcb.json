{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Dynamic Graph Learning"
  ],
  "datasets": [
    "real-world datasets"
  ],
  "methods": [
    "Dynamic Graph Transformer (DyFormer)",
    "spatial-temporal encoding",
    "temporal-union graph structure",
    "subgraph-based node sampling strategy",
    "two complementary self-supervised pre-training tasks"
  ],
  "results": [
    "consistent 1%-3% AUC gain compared with baselines on all benchmarks"
  ],
  "paper_id": "619c5bb65244ab9dcbf1edcb",
  "title": "DyFormer: A Scalable Dynamic Graph Transformer with Provable Benefits on\n  Generalization Ability",
  "abstract": "  Transformers have achieved great success in several domains, including Natural Language Processing and Computer Vision. However, its application to real-world graphs is less explored, mainly due to its high computation cost and its poor generalizability caused by the lack of enough training data in the graph domain. To fill in this gap, we propose a scalable Transformer-like dynamic graph learning method named Dynamic Graph Transformer (DyFormer) with spatial-temporal encoding to effectively learn graph topology and capture implicit links. To achieve efficient and scalable training, we propose temporal-union graph structure and its associated subgraph-based node sampling strategy. To improve the generalization ability, we introduce two complementary self-supervised pre-training tasks and show that jointly optimizing the two pre-training tasks results in a smaller Bayesian error rate via an information-theoretic analysis. Extensive experiments on the real-world datasets illustrate that DyFormer achieves a consistent 1%-3% AUC gain (averaged over all time steps) compared with baselines on all benchmarks. "
}