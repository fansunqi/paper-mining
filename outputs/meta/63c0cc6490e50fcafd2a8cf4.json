{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Investigating The Fairness of Visual Privacy Preservation Algorithms"
  ],
  "datasets": [
    "PubFig"
  ],
  "methods": [
    "Facial recognition models on obfuscated images"
  ],
  "results": [
    "Privacy protection provided is unequal across groups"
  ],
  "paper_id": "63c0cc6490e50fcafd2a8cf4",
  "title": "Fairly Private: Investigating The Fairness of Visual Privacy\n  Preservation Algorithms",
  "abstract": "  As the privacy risks posed by camera surveillance and facial recognition have grown, so has the research into privacy preservation algorithms. Among these, visual privacy preservation algorithms attempt to impart bodily privacy to subjects in visuals by obfuscating privacy-sensitive areas. While disparate performances of facial recognition systems across phenotypes are the subject of much study, its counterpart, privacy preservation, is not commonly analysed from a fairness perspective. In this paper, the fairness of commonly used visual privacy preservation algorithms is investigated through the performances of facial recognition models on obfuscated images. Experiments on the PubFig dataset clearly show that the privacy protection provided is unequal across groups. "
}