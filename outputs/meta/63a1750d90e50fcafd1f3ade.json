{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Head avatar creation from video sequences"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Deformable point-based representation",
    "Disentangled color estimation",
    "Monocular video processing"
  ],
  "results": [
    "State-of-the-art quality in challenging cases",
    "Significantly more efficient in training than competing methods"
  ],
  "paper_id": "63a1750d90e50fcafd1f3ade",
  "title": "PointAvatar: Deformable Point-based Head Avatars from Videos",
  "abstract": "  The ability to create realistic, animatable and relightable head avatars from casual video sequences would open up wide ranging applications in communication and entertainment. Current methods either build on explicit 3D morphable meshes (3DMM) or exploit neural implicit representations. The former are limited by fixed topology, while the latter are non-trivial to deform and inefficient to render. Furthermore, existing approaches entangle lighting in the color estimation, thus they are limited in re-rendering the avatar in new environments. In contrast, we propose PointAvatar, a deformable point-based representation that disentangles the source color into intrinsic albedo and normal-dependent shading. We demonstrate that PointAvatar bridges the gap between existing mesh- and implicit representations, combining high-quality geometry and appearance with topological flexibility, ease of deformation and rendering efficiency. We show that our method is able to generate animatable 3D avatars using monocular videos from multiple sources including hand-held smartphones, laptop webcams and internet videos, achieving state-of-the-art quality in challenging cases where previous methods fail, e.g., thin hair strands, while being significantly more efficient in training than competing methods. "
}