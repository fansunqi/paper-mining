{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Human activity planning"
  ],
  "datasets": [
    "Charades extended"
  ],
  "methods": [
    "Neurosymbolic approach",
    "Multi-modal knowledge base",
    "Deep generative models"
  ],
  "results": [
    "Compositional generalization",
    "Effective use of information from both modalities"
  ],
  "paper_id": "6346305e90e50fcafda07abe",
  "title": "ViLPAct: A Benchmark for Compositional Generalization on Multimodal\n  Human Activities",
  "abstract": "  We introduce ViLPAct, a novel vision-language benchmark for human activity planning. It is designed for a task where embodied AI agents can reason and forecast future actions of humans based on video clips about their initial activities and intents in text. The dataset consists of 2.9k videos from \\charades extended with intents via crowdsourcing, a multi-choice question test set, and four strong baselines. One of the baselines implements a neurosymbolic approach based on a multi-modal knowledge base (MKB), while the other ones are deep generative models adapted from recent state-of-the-art (SOTA) methods. According to our extensive experiments, the key challenges are compositional generalization and effective use of information from both modalities. "
}