{
  "code_links": [
    "https://github.com/ankile/Adversarial-Diffusion"
  ],
  "tasks": [
    "Adversarial Attack Defense"
  ],
  "datasets": [
    "PatchCamelyon"
  ],
  "methods": [
    "Denoising Diffusion Probabilistic Models"
  ],
  "results": [
    "Robust accuracy improvement up to 88%"
  ],
  "paper_id": "63c8b59590e50fcafd90b746",
  "title": "Denoising Diffusion Probabilistic Models as a Defense against\n  Adversarial Attacks",
  "abstract": "  Neural Networks are infamously sensitive to small perturbations in their inputs, making them vulnerable to adversarial attacks. This project evaluates the performance of Denoising Diffusion Probabilistic Models (DDPM) as a purification technique to defend against adversarial attacks. This works by adding noise to an adversarial example before removing it through the reverse process of the diffusion model. We evaluate the approach on the PatchCamelyon data set for histopathologic scans of lymph node sections and find an improvement of the robust accuracy by up to 88\\% of the original model's accuracy, constituting a considerable improvement over the vanilla model and our baselines. The project code is located at https://github.com/ankile/Adversarial-Diffusion. "
}