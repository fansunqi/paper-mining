{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Document Summarization",
    "Book Abstract generation"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Transformer based techniques"
  ],
  "results": [
    "None"
  ],
  "paper_id": "63c8b59590e50fcafd90b907",
  "title": "Transformer Based Implementation for Automatic Book Summarization",
  "abstract": "  Document Summarization is the procedure of generating a meaningful and concise summary of a given document with the inclusion of relevant and topic-important points. There are two approaches: one is picking up the most relevant statements from the document itself and adding it to the Summary known as Extractive and the other is generating sentences for the Summary known as Abstractive Summarization. Training a machine learning model to perform tasks that are time-consuming or very difficult for humans to evaluate is a major challenge. Book Abstract generation is one of such complex tasks. Traditional machine learning models are getting modified with pre-trained transformers. Transformer based language models trained in a self-supervised fashion are gaining a lot of attention; when fine-tuned for Natural Language Processing(NLP) downstream task like text summarization. This work is an attempt to use Transformer based techniques for Abstract generation. "
}