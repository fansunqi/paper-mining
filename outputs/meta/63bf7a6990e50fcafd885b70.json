{
  "code_links": [
    "https://github.com/PhantomGrapes/MGeo"
  ],
  "tasks": [
    "Query-POI matching",
    "Location-based services (LBS)"
  ],
  "datasets": [
    "GeoTES"
  ],
  "methods": [
    "Multi-modal Geographic language model (MGeo)",
    "Geographic encoder",
    "Multi-modal interaction module"
  ],
  "results": [
    "Significant improvement in query-POI matching capability",
    "No publicly available benchmark for this topic"
  ],
  "paper_id": "63bf7a6990e50fcafd885b70",
  "title": "A Multi-Modal Geographic Pre-Training Method",
  "abstract": "  As a core task in location-based services (LBS) (e.g., navigation maps), query and point of interest (POI) matching connects users' intent with real-world geographic information. Recently, pre-trained models (PTMs) have made advancements in many natural language processing (NLP) tasks. Generic text-based PTMs do not have enough geographic knowledge for query-POI matching. To overcome this limitation, related literature attempts to employ domain-adaptive pre-training based on geo-related corpus. However, a query generally contains mentions of multiple geographic objects, such as nearby roads and regions of interest (ROIs). The geographic context (GC), i.e., these diverse geographic objects and their relationships, is therefore pivotal to retrieving the most relevant POI. Single-modal PTMs can barely make use of the important GC and therefore have limited performance. In this work, we propose a novel query-POI matching method Multi-modal Geographic language model (MGeo), which comprises a geographic encoder and a multi-modal interaction module. MGeo represents GC as a new modality and is able to fully extract multi-modal correlations for accurate query-POI matching. Besides, there is no publicly available benchmark for this topic. In order to facilitate further research, we build a new open-source large-scale benchmark Geographic TExtual Similarity (GeoTES). The POIs come from an open-source geographic information system (GIS). The queries are manually generated by annotators to prevent privacy issues. Compared with several strong baselines, the extensive experiment results and detailed ablation analyses on GeoTES demonstrate that our proposed multi-modal pre-training method can significantly improve the query-POI matching capability of generic PTMs, even when the queries' GC is not provided. Our code and dataset are publicly available at https://github.com/PhantomGrapes/MGeo. "
}