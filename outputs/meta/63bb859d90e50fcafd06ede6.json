{
  "code_links": [
    "https://github.com/author/PressureVision++"
  ],
  "tasks": [
    "Estimating Fingertip Pressure"
  ],
  "datasets": [
    "Dataset with 51 participants"
  ],
  "methods": [
    "PressureVision++ network"
  ],
  "results": [
    "Outperforms human annotators and prior work",
    "Application in mixed reality"
  ],
  "paper_id": "63bb859d90e50fcafd06ede6",
  "title": "PressureVision++: Estimating Fingertip Pressure from Diverse RGB Images",
  "abstract": "Touch plays a fundamental role in manipulation for humans; however, machine\nperception of contact and pressure typically requires invasive sensors. Recent\nresearch has shown that deep models can estimate hand pressure based on a\nsingle RGB image. However, evaluations have been limited to controlled settings\nsince collecting diverse data with ground-truth pressure measurements is\ndifficult. We present a novel approach that enables diverse data to be captured\nwith only an RGB camera and a cooperative participant. Our key insight is that\npeople can be prompted to apply pressure in a certain way, and this prompt can\nserve as a weak label to supervise models to perform well under varied\nconditions. We collect a novel dataset with 51 participants making fingertip\ncontact with diverse objects. Our network, PressureVision++, outperforms human\nannotators and prior work. We also demonstrate an application of\nPressureVision++ to mixed reality where pressure estimation allows everyday\nsurfaces to be used as arbitrary touch-sensitive interfaces. Code, data, and\nmodels are available online."
}