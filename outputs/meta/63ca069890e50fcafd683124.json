{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Keyword suggestion"
  ],
  "datasets": [
    "Scientific literature"
  ],
  "methods": [
    "Word2Vec",
    "FastText",
    "Negative sampling"
  ],
  "results": [
    "Considerable improvements over the baselines"
  ],
  "paper_id": "63ca069890e50fcafd683124",
  "title": "Keyword Embeddings for Query Suggestion",
  "abstract": "  Nowadays, search engine users commonly rely on query suggestions to improve their initial inputs. Current systems are very good at recommending lexical adaptations or spelling corrections to users' queries. However, they often struggle to suggest semantically related keywords given a user's query. The construction of a detailed query is crucial in some tasks, such as legal retrieval or academic search. In these scenarios, keyword suggestion methods are critical to guide the user during the query formulation. This paper proposes two novel models for the keyword suggestion task trained on scientific literature. Our techniques adapt the architecture of Word2Vec and FastText to generate keyword embeddings by leveraging documents' keyword co-occurrence. Along with these models, we also present a specially tailored negative sampling approach that exploits how keywords appear in academic publications. We devise a ranking-based evaluation methodology following both known-item and ad-hoc search scenarios. Finally, we evaluate our proposals against the state-of-the-art word and sentence embedding models showing considerable improvements over the baselines for the tasks. "
}