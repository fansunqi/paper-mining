{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Data Distillation"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Data Distillation approaches"
  ],
  "results": [
    "None"
  ],
  "paper_id": "63bf7a6990e50fcafd885b57",
  "title": "Data Distillation: A Survey",
  "abstract": "  The popularity of deep learning has led to the curation of a vast number of massive and multifarious datasets. Despite having close-to-human performance on individual tasks, training parameter-hungry models on large datasets poses multi-faceted problems such as (a) high model-training time; (b) slow research iteration; and (c) poor eco-sustainability. As an alternative, data distillation approaches aim to synthesize terse data summaries, which can serve as effective drop-in replacements of the original dataset for scenarios like model training, inference, architecture search, etc. In this survey, we present a formal framework for data distillation, along with providing a detailed taxonomy of existing approaches. Additionally, we cover data distillation approaches for different data modalities, namely images, graphs, and user-item interactions (recommender systems), while also identifying current challenges and future research directions. "
}