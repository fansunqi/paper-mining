{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Tokenization",
    "Tagging",
    "Dependency Parsing"
  ],
  "datasets": [
    "Tagalog language"
  ],
  "methods": [
    "Zero-shot and few-shot approaches",
    "Word embeddings",
    "Data augmentation"
  ],
  "results": [
    "Substantial improvements on grammatical analysis compared to state-of-the-art supervised baselines"
  ],
  "paper_id": "62eb392a5aee126c0fb76473",
  "title": "Benchmarking zero-shot and few-shot approaches for tokenization,\n  tagging, and dependency parsing of Tagalog text",
  "abstract": "  The grammatical analysis of texts in any written language typically involves a number of basic processing tasks, such as tokenization, morphological tagging, and dependency parsing. State-of-the-art systems can achieve high accuracy on these tasks for languages with large datasets, but yield poor results for languages which have little to no annotated data. To address this issue for the Tagalog language, we investigate the use of alternative language resources for creating task-specific models in the absence of dependency-annotated Tagalog data. We also explore the use of word embeddings and data augmentation to improve performance when only a small amount of annotated Tagalog data is available. We show that these zero-shot and few-shot approaches yield substantial improvements on grammatical analysis of both in-domain and out-of-domain Tagalog text compared to state-of-the-art supervised baselines. "
}