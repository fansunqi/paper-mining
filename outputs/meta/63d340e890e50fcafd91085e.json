{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Tree Ensemble Pruning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Hypothesis Test for split quality",
    "Tree pruning algorithms"
  ],
  "results": [
    "Reduction in out of sample loss",
    "Theoretically well-justified stopping condition"
  ],
  "paper_id": "63d340e890e50fcafd91085e",
  "title": "A Robust Hypothesis Test for Tree Ensemble Pruning",
  "abstract": "  Gradient boosted decision trees are some of the most popular algorithms in applied machine learning. They are a flexible and powerful tool that can robustly fit to any tabular dataset in a scalable and computationally efficient way. One of the most critical parameters to tune when fitting these models are the various penalty terms used to distinguish signal from noise in the current model. These penalties are effective in practice, but are lacking in robust theoretical justifications. In this paper we develop and present a novel theoretically justified hypothesis test of split quality for gradient boosted tree ensembles and demonstrate that using this method instead of the common penalty terms leads to a significant reduction in out of sample loss. Additionally, this method provides a theoretically well-justified stopping condition for the tree growing algorithm. We also present several innovative extensions to the method, opening the door for a wide variety of novel tree pruning algorithms. "
}