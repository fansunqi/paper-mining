{
  "code_links": [
    "None"
  ],
  "tasks": [
    "DeepFake Detection"
  ],
  "datasets": [
    "Deepfake datasets"
  ],
  "methods": [
    "Annotation analysis",
    "AI-bias analysis"
  ],
  "results": [
    "Lack of diversity in databases",
    "Strong bias in detection models",
    "Potential generalizability, fairness, and security issues"
  ],
  "paper_id": "62f5c4fe90e50fcafde9c75a",
  "title": "A Comprehensive Analysis of AI Biases in DeepFake Detection With\n  Massively Annotated Databases",
  "abstract": "  In recent years, image and video manipulations with Deepfake have become a severe concern for security and society. Many detection models and datasets have been proposed to detect Deepfake data reliably. However, there is an increased concern that these models and training databases might be biased and, thus, cause Deepfake detectors to fail. In this work, we investigate the bias issue caused by public Deepfake datasets by (a) providing large-scale demographic and non-demographic attribute annotations of 47 different attributes for five popular Deepfake datasets and (b) comprehensively analysing AI-bias of three state-of-the-art Deepfake detection backbone models on these datasets. The investigation analyses the influence of a large variety of distinctive attributes (from over 65M labels) on the detection performance, including demographic (age, gender, ethnicity) and non-demographic (hair, skin, accessories, etc.) information. The results indicate that investigated databases lack diversity and, more importantly, show that the utilised Deepfake detection backbone models are strongly biased towards many investigated attributes. The Deepfake detection backbone methods, which are trained with biased datasets, might output incorrect detection results, thereby leading to generalisability, fairness, and security issues. We hope that the findings of this study and the annotation databases will help to evaluate and mitigate bias in future Deepfake detection techniques. The annotation datasets are publicly available. "
}