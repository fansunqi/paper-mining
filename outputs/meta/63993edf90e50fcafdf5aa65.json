{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Hyperparameter optimization",
    "Multi-objective optimization",
    "Meta-learning"
  ],
  "datasets": [
    "tabular HPO benchmarks",
    "AutoML 2022 competition on Multiobjective Hyperparameter Optimization for Transformers"
  ],
  "methods": [
    "Task similarity-based meta-learning",
    "Tree-structured Parzen estimator (TPE)"
  ],
  "results": [
    "State-of-the-art performance on tabular HPO benchmarks",
    "Winning the AutoML 2022 competition"
  ],
  "paper_id": "63993edf90e50fcafdf5aa65",
  "title": "Speeding up Multi-objective Non-hierarchical Hyperparameter Optimization\n  by Task Similarity-Based Meta-Learning for the Tree-structured Parzen\n  Estimator",
  "abstract": "  Hyperparameter optimization (HPO) is a vital step in improving performance in deep learning (DL). Practitioners are often faced with the trade-off between multiple criteria, such as accuracy and latency. Given the high computational needs of DL and the growing demand for efficient HPO, the acceleration of multi-objective (MO) optimization becomes ever more important. Despite the significant body of work on meta-learning for HPO, existing methods are inapplicable to MO tree-structured Parzen estimator (MO-TPE), a simple yet powerful MO-HPO algorithm. In this paper, we extend TPE's acquisition function to the meta-learning setting using a task similarity defined by the overlap of top domains between tasks. We also theoretically analyze and address the limitations of our task similarity. In the experiments, we demonstrate that our method speeds up MO-TPE on tabular HPO benchmarks and attains state-of-the-art performance. Our method was also validated externally by winning the AutoML 2022 competition on ``Multiobjective Hyperparameter Optimization for Transformers''. "
}