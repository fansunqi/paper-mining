{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Referring image segmentation"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "LGFormer",
    "Transformer",
    "cross-modal interaction modules (e.g., vision-language bidirectional attention module, VLBA)"
  ],
  "results": [
    "None"
  ],
  "paper_id": "63c8b56c90e50fcafd905fff",
  "title": "Linguistic Query-Guided Mask Generation for Referring Image Segmentation",
  "abstract": "  Referring image segmentation aims to segment the image region of interest according to the given language expression, which is a typical multi-modal task. Existing methods either adopt the pixel classification-based or the learnable query-based framework for mask generation, both of which are insufficient to deal with various text-image pairs with a fix number of parametric prototypes. In this work, we propose an end-to-end framework built on transformer to perform Linguistic query-Guided mask generation, dubbed LGFormer. It views the linguistic features as query to generate a specialized prototype for arbitrary input image-text pair, thus generating more consistent segmentation results. Moreover, we design several cross-modal interaction modules (\\eg, vision-language bidirectional attention module, VLBA) in both encoder and decoder to achieve better cross-modal alignment. "
}