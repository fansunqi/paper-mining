{
  "code_links": [
    "None"
  ],
  "tasks": [
    "unsupervised machine learning",
    "generative modeling",
    "reinforcement learning in statistical physics"
  ],
  "datasets": [
    "synthetic data",
    "real-world data"
  ],
  "methods": [
    "autoregressive matrix product states (AMPS)",
    "tensor networks"
  ],
  "results": [
    "significantly outperforms existing tensor network models and restricted Boltzmann machines",
    "competitive with state-of-the-art neural network models"
  ],
  "paper_id": "60d6957391e011839f53ce25",
  "title": "Tensor networks for unsupervised machine learning",
  "abstract": "  Modeling the joint distribution of high-dimensional data is a central task in unsupervised machine learning. In recent years, many interests have been attracted to developing learning models based on tensor networks, which have the advantages of a principle understanding of the expressive power using entanglement properties, and as a bridge connecting classical computation and quantum computation. Despite the great potential, however, existing tensor network models for unsupervised machine learning only work as a proof of principle, as their performance is much worse than the standard models such as restricted Boltzmann machines and neural networks. In this Letter, we present autoregressive matrix product states (AMPS), a tensor network model combining matrix product states from quantum many-body physics and autoregressive modeling from machine learning. Our model enjoys the exact calculation of normalized probability and unbiased sampling. We demonstrate the performance of our model using two applications, generative modeling on synthetic and real-world data, and reinforcement learning in statistical physics. Using extensive numerical experiments, we show that the proposed model significantly outperforms the existing tensor network models and the restricted Boltzmann machines, and is competitive with state-of-the-art neural network models. "
}