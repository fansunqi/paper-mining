{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Multi-Objective Optimisation Problems"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Deep W-Networks",
    "Deep Q-Networks (DQN)",
    "W-learning"
  ],
  "results": [
    "Solves competition between multiple policies",
    "Outperforms baseline in DQN solution",
    "Finds Pareto front in tested environments"
  ],
  "paper_id": "636c6beb90e50fcafd2d3dc5",
  "title": "Deep W-Networks: Solving Multi-Objective Optimisation Problems With Deep\n  Reinforcement Learning",
  "abstract": "  In this paper, we build on advances introduced by the Deep Q-Networks (DQN) approach to extend the multi-objective tabular Reinforcement Learning (RL) algorithm W-learning to large state spaces. W-learning algorithm can naturally solve the competition between multiple single policies in multi-objective environments. However, the tabular version does not scale well to environments with large state spaces. To address this issue, we replace underlying Q-tables with DQN, and propose an addition of W-Networks, as a replacement for tabular weights (W) representations. We evaluate the resulting Deep W-Networks (DWN) approach in two widely-accepted multi-objective RL benchmarks: deep sea treasure and multi-objective mountain car. We show that DWN solves the competition between multiple policies while outperforming the baseline in the form of a DQN solution. Additionally, we demonstrate that the proposed algorithm can find the Pareto front in both tested environments. "
}