{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Understanding Transformer outputs"
  ],
  "datasets": [
    "ImageNet"
  ],
  "methods": [
    "B-cos transformers"
  ],
  "results": [
    "Highly interpretable models",
    "Competitive performance to baseline ViTs"
  ],
  "paper_id": "63cdfab690e50fcafd107054",
  "title": "Holistically Explainable Vision Transformers",
  "abstract": "  Transformers increasingly dominate the machine learning landscape across many tasks and domains, which increases the importance for understanding their outputs. While their attention modules provide partial insight into their inner workings, the attention scores have been shown to be insufficient for explaining the models as a whole. To address this, we propose B-cos transformers, which inherently provide holistic explanations for their decisions. Specifically, we formulate each model component - such as the multi-layer perceptrons, attention layers, and the tokenisation module - to be dynamic linear, which allows us to faithfully summarise the entire transformer via a single linear transform. We apply our proposed design to Vision Transformers (ViTs) and show that the resulting models, dubbed Bcos-ViTs, are highly interpretable and perform competitively to baseline ViTs on ImageNet. Code will be made available soon. "
}