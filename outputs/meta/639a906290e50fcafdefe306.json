{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Text-guided image editing"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Cascaded diffusion model",
    "Object detectors",
    "EditBench"
  ],
  "results": [
    "Object-masking during training leads to improvements in text-image alignment",
    "Imagen Editor is preferred over DALL-E 2 and Stable Diffusion",
    "Better object-rendering than text-rendering",
    "Better handling of material/color/size attributes than count/shape attributes"
  ],
  "paper_id": "639a906290e50fcafdefe306",
  "title": "Imagen Editor and EditBench: Advancing and Evaluating Text-Guided Image\n  Inpainting",
  "abstract": "  Text-guided image editing can have a transformative impact in supporting creative applications. A key challenge is to generate edits that are faithful to input text prompts, while consistent with input images. We present Imagen Editor, a cascaded diffusion model built, by fine-tuning Imagen on text-guided image inpainting. Imagen Editor's edits are faithful to the text prompts, which is accomplished by using object detectors to propose inpainting masks during training. In addition, Imagen Editor captures fine details in the input image by conditioning the cascaded pipeline on the original high resolution image. To improve qualitative and quantitative evaluation, we introduce EditBench, a systematic benchmark for text-guided image inpainting. EditBench evaluates inpainting edits on natural and generated images exploring objects, attributes, and scenes. Through extensive human evaluation on EditBench, we find that object-masking during training leads to across-the-board improvements in text-image alignment -- such that Imagen Editor is preferred over DALL-E 2 and Stable Diffusion -- and, as a cohort, these models are better at object-rendering than text-rendering, and handle material/color/size attributes better than count/shape attributes. "
}