{
  "code_links": [
    "http://virtualhumans.mpi-inf.mpg.de/ireplica/"
  ],
  "tasks": [
    "Tracking human-object interaction",
    "Scene change detection"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "iReplica: human-object interaction reasoning based on human motion"
  ],
  "results": [
    "None"
  ],
  "paper_id": "627483fd5aee126c0f07e67a",
  "title": "Interaction Replica: Tracking human-object interaction and scene changes\n  from human motion",
  "abstract": "  Humans naturally change their environment through interactions, e.g., by opening doors or moving furniture. To reproduce such interactions in virtual spaces (e.g., metaverse), we need to capture and model them, including changes in the scene geometry, ideally from egocentric input alone (head camera and body-worn inertial sensors). While the head camera can be used to localize the person in the scene, estimating dynamic object pose is much more challenging. As the object is often not visible from the head camera (e.g., a human not looking at a chair while sitting down), we can not rely on visual object pose estimation. Instead, our key observation is that human motion tells us a lot about scene changes. Motivated by this, we present iReplica, the first human-object interaction reasoning method which can track objects and scene changes based solely on human motion. iReplica is an essential first step towards advanced AR/VR applications in immersive virtual universes and can provide human-centric training data to teach machines to interact with their surroundings. Our code, data and model will be available on our project page at http://virtualhumans.mpi-inf.mpg.de/ireplica/ "
}