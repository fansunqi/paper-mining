{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Parameter-efficient tuning",
    "Natural language understanding"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Special token adaptation",
    "Transformer-based models"
  ],
  "results": [
    "Comparable performance to full finetuning",
    "0.029% of total parameters trained"
  ],
  "paper_id": "6344dede90e50fcafd24d279",
  "title": "Parameter-Efficient Tuning with Special Token Adaptation",
  "abstract": "  Parameter-efficient tuning aims at updating only a small subset of parameters when adapting a pretrained model to downstream tasks. In this work, we introduce PASTA, in which we only modify the special token representations (e.g., [SEP] and [CLS] in BERT) before the self-attention module at each layer in Transformer-based models. PASTA achieves comparable performance to full finetuning in natural language understanding tasks including text classification and NER with up to only 0.029% of total parameters trained. Our work not only provides a simple yet effective way of parameter-efficient tuning, which has a wide range of practical applications when deploying finetuned models for multiple tasks, but also demonstrates the pivotal role of special tokens in pretrained language models "
}