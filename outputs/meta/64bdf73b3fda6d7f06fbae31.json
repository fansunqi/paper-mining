{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Neural Networks Explanation"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "SpArX: Sparse Argumentative Explanations",
    "Quantitative Argumentation Frameworks (QAFs)"
  ],
  "results": [
    "SpArX provides more faithful explanations than existing approaches",
    "Deeper insights into the reasoning process of MLPs"
  ],
  "paper_id": "64bdf73b3fda6d7f06fbae31",
  "title": "SpArX: Sparse Argumentative Explanations for Neural Networks",
  "abstract": "  Neural networks (NNs) have various applications in AI, but explaining their decisions remains challenging. Existing approaches often focus on explaining how changing individual inputs affects NNs' outputs. However, an explanation that is consistent with the input-output behaviour of an NN is not necessarily faithful to the actual mechanics thereof. In this paper, we exploit relationships between multi-layer perceptrons (MLPs) and quantitative argumentation frameworks (QAFs) to create argumentative explanations for the mechanics of MLPs. Our SpArX method first sparsifies the MLP while maintaining as much of the original structure as possible. It then translates the sparse MLP into an equivalent QAF to shed light on the underlying decision process of the MLP, producing global and/or local explanations. We demonstrate experimentally that SpArX can give more faithful explanations than existing approaches, while simultaneously providing deeper insights into the actual reasoning process of MLPs. "
}