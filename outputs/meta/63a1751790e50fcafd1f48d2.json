{
  "code_links": [
    "https://github.com/WENGSYX/Self-Verification"
  ],
  "tasks": [
    "Complex reasoning by chain of thought (CoT)",
    "Self-verification of LLM conclusions"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Self-verification using LLM conclusions",
    "Re-predicting masked original conditions"
  ],
  "results": [
    "Improved accuracy on multiple arithmetics and logical reasoning datasets",
    "Competitive reasoning performance",
    "Avoidance of interference from incorrect CoT"
  ],
  "paper_id": "63a1751790e50fcafd1f48d2",
  "title": "Large Language Models are reasoners with Self-Verification",
  "abstract": "  When a large language model (LLM) performs complex reasoning by chain of thought (CoT), it can be highly sensitive to individual mistakes. We have had to train verifiers to address this issue. As we all know, after human inferring a conclusion, they often check it by re-verifying it, which can avoid some mistakes. We propose a new method called self-verification that uses the conclusion of the CoT as a condition to build a new sample and asks the LLM to re-predict the original conditions which be masked. We calculate an explainable verification score based on the accuracy. This method can improve the accuracy of multiple arithmetics and logical reasoning datasets when using few-shot learning. we have demonstrated that LLMs can conduct explainable self-verification of their own conclusions and achieve competitive reasoning performance. Extensive experimentals have demonstrated that our method can help multiple large language models with self-verification can avoid interference from incorrect CoT. Code is available at \\url{https://github.com/WENGSYX/Self-Verification} "
}