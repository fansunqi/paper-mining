{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Few-Shot Dialogue State Tracking"
  ],
  "datasets": [
    "Two task-oriented dialogue datasets"
  ],
  "methods": [
    "Dual Prompt Learning Framework",
    "Slot generation and value generation as dual tasks"
  ],
  "results": [
    "Outperforms existing state-of-the-art few-shot methods",
    "Can generate unseen slots"
  ],
  "paper_id": "61e781645244ab9dcbf9a276",
  "title": "A Dual Prompt Learning Framework for Few-Shot Dialogue State Tracking",
  "abstract": "  Dialogue state tracking (DST) module is an important component for task-oriented dialog systems to understand users' goals and needs. Collecting dialogue state labels including slots and values can be costly, especially with the wide application of dialogue systems in more and more new-rising domains. In this paper, we focus on how to utilize the language understanding and generation ability of pre-trained language models for DST. We design a dual prompt learning framework for few-shot DST. Specifically, we consider the learning of slot generation and value generation as dual tasks, and two prompts are designed based on such a dual structure to incorporate task-related knowledge of these two tasks respectively. In this way, the DST task can be formulated as a language modeling task efficiently under few-shot settings. Experimental results on two task-oriented dialogue datasets show that the proposed method not only outperforms existing state-of-the-art few-shot methods, but also can generate unseen slots. It indicates that DST-related knowledge can be probed from PLM and utilized to address low-resource DST efficiently with the help of prompt learning. "
}