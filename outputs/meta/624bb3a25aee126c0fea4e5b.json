{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Analysis of Semantically-Aligned Speech-Text Embeddings"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Joint speech-text embedding space",
    "Minimizing distance between paired utterance and transcription inputs",
    "Teacher-student model setup",
    "Pretraining and multitask scenarios for automatic speech recognition"
  ],
  "results": [
    "Incorporating automatic speech recognition aids semantic alignment significantly",
    "More tightly coupled embeddings",
    "Quantitative retrieval accuracy metric for semantic alignment",
    "Zero-shot classification for generalisability",
    "Probing encoders to observe knowledge transfer between modalities"
  ],
  "paper_id": "624bb3a25aee126c0fea4e5b",
  "title": "An Analysis of Semantically-Aligned Speech-Text Embeddings",
  "abstract": "  Embeddings play an important role in end-to-end solutions for multi-modal language processing problems. Although there has been some effort to understand the properties of single-modality embedding spaces, particularly that of text, their cross-modal counterparts are less understood. In this work, we study some intrinsic properties of a joint speech-text embedding space, constructed by minimizing the distance between paired utterance and transcription inputs in a teacher-student model setup, that are informative for several prominent use cases. We found that incorporating automatic speech recognition through both pretraining and multitask scenarios aid semantic alignment significantly, resulting in more tightly coupled embeddings. To analyse cross-modal embeddings we utilise a quantitative retrieval accuracy metric for semantic alignment, zero-shot classification for generalisability, and probing of the encoders to observe the extent of knowledge transfer from one modality to another. "
}