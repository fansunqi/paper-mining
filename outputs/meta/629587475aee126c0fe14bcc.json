{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Continuous Generative Neural Networks"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Continuous Generative Neural Networks (CGNNs)",
    "inspired by DCGAN",
    "fully connected layer",
    "convolutional layers",
    "nonlinear activation functions",
    "multiresolution analysis of compactly supported wavelet"
  ],
  "results": [
    "CGNNs belong to an infinite-dimensional function space",
    "injective CGNNs",
    "Lipschitz stability estimates for infinite-dimensional inverse problems"
  ],
  "paper_id": "629587475aee126c0fe14bcc",
  "title": "Continuous Generative Neural Networks",
  "abstract": "  In this work, we present and study Continuous Generative Neural Networks (CGNNs), namely, generative models in the continuous setting: the output of a CGNN belongs to an infinite-dimensional function space. The architecture is inspired by DCGAN, with one fully connected layer, several convolutional layers and nonlinear activation functions. In the continuous $L^2$ setting, the dimensions of the spaces of each layer are replaced by the scales of a multiresolution analysis of a compactly supported wavelet. We present conditions on the convolutional filters and on the nonlinearity that guarantee that a CGNN is injective. This theory finds applications to inverse problems, and allows for deriving Lipschitz stability estimates for (possibly nonlinear) infinite-dimensional inverse problems with unknowns belonging to the manifold generated by a CGNN. Several numerical simulations, including signal deblurring, illustrate and validate this approach. "
}