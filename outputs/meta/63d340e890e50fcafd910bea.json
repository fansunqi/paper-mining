{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Understanding and Improving Deep Graph Neural Networks"
  ],
  "datasets": [
    "Citation networks",
    "Natural language processing downstream tasks"
  ],
  "methods": [
    "Probabilistic graphical model",
    "Coupling graph neural network (CoGNet)"
  ],
  "results": [
    "CoGNet outperforms the SOTA models"
  ],
  "paper_id": "63d340e890e50fcafd910bea",
  "title": "Understanding and Improving Deep Graph Neural Networks: A Probabilistic\n  Graphical Model Perspective",
  "abstract": "  Recently, graph-based models designed for downstream tasks have significantly advanced research on graph neural networks (GNNs). GNN baselines based on neural message-passing mechanisms such as GCN and GAT perform worse as the network deepens. Therefore, numerous GNN variants have been proposed to tackle this performance degradation problem, including many deep GNNs. However, a unified framework is still lacking to connect these existing models and interpret their effectiveness at a high level. In this work, we focus on deep GNNs and propose a novel view for understanding them. We establish a theoretical framework via inference on a probabilistic graphical model. Given the fixed point equation (FPE) derived from the variational inference on the Markov random fields, the deep GNNs, including JKNet, GCNII, DGCN, and the classical GNNs, such as GCN, GAT, and APPNP, can be regarded as different approximations of the FPE. Moreover, given this framework, more accurate approximations of FPE are brought, guiding us to design a more powerful GNN: coupling graph neural network (CoGNet). Extensive experiments are carried out on citation networks and natural language processing downstream tasks. The results demonstrate that the CoGNet outperforms the SOTA models. "
}