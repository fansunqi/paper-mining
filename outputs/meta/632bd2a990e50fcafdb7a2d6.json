{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Shape and Force Reconstruction",
    "Optical Tactile Sensor for Collaborative Robots"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Neural Network for Calibrated Shape Reconstruction",
    "6-axis Wrench Estimation",
    "Transfer Learning"
  ],
  "results": [
    "Shape Reconstruction Accuracy: 0.3633mm per pixel",
    "Force Sensing Accuracy: 0.410N",
    "Torque Sensing Accuracy: 0.387Nmm",
    "Transfer Learning Performance: Comparable with 12% of non-transfer learning dataset size"
  ],
  "paper_id": "632bd2a990e50fcafdb7a2d6",
  "title": "DenseTact 2.0: Optical Tactile Sensor for Shape and Force Reconstruction",
  "abstract": "  Collaborative robots stand to have an immense impact on both human welfare in domestic service applications and industrial superiority in advanced manufacturing with dexterous assembly. The outstanding challenge is providing robotic fingertips with a physical design that makes them adept at performing dexterous tasks that require high-resolution, calibrated shape reconstruction and force sensing. In this work, we present DenseTact 2.0, an optical-tactile sensor capable of visualizing the deformed surface of a soft fingertip and using that image in a neural network to perform both calibrated shape reconstruction and 6-axis wrench estimation. We demonstrate the sensor accuracy of 0.3633mm per pixel for shape reconstruction, 0.410N for forces, 0.387Nmm for torques, and the ability to calibrate new fingers through transfer learning, which achieves comparable performance with only 12% of the non-transfer learning dataset size. "
}