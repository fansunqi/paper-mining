{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Neural text ranking models",
    "Black-box neural passage ranking models"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Imitation adversarial attack",
    "Gradient-based attack method",
    "Pairwise objective function",
    "Next sentence prediction loss",
    "Language model fluency constraint"
  ],
  "results": [
    "Ranking imitation attack model and adversarial triggers effective against various SOTA neural ranking models",
    "Camouflages effective against potential mitigation approaches"
  ],
  "paper_id": "632297f590e50fcafdc884ae",
  "title": "Order-Disorder: Imitation Adversarial Attacks for Black-box Neural\n  Ranking Models",
  "abstract": "  Neural text ranking models have witnessed significant advancement and are increasingly being deployed in practice. Unfortunately, they also inherit adversarial vulnerabilities of general neural models, which have been detected but remain underexplored by prior studies. Moreover, the inherit adversarial vulnerabilities might be leveraged by blackhat SEO to defeat better-protected search engines. In this study, we propose an imitation adversarial attack on black-box neural passage ranking models. We first show that the target passage ranking model can be transparentized and imitated by enumerating critical queries/candidates and then train a ranking imitation model. Leveraging the ranking imitation model, we can elaborately manipulate the ranking results and transfer the manipulation attack to the target ranking model. For this purpose, we propose an innovative gradient-based attack method, empowered by the pairwise objective function, to generate adversarial triggers, which causes premeditated disorderliness with very few tokens. To equip the trigger camouflages, we add the next sentence prediction loss and the language model fluency constraint to the objective function. Experimental results on passage ranking demonstrate the effectiveness of the ranking imitation attack model and adversarial triggers against various SOTA neural ranking models. Furthermore, various mitigation analyses and human evaluation show the effectiveness of camouflages when facing potential mitigation approaches. To motivate other scholars to further investigate this novel and important problem, we make the experiment data and code publicly available. "
}