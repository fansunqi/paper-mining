{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Constrained Reinforcement Learning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "State Augmentation",
    "Reward Penalties"
  ],
  "results": [
    "Outperforms leading approaches on multiple benchmark problems"
  ],
  "paper_id": "63d7352390e50fcafda30356",
  "title": "Solving Richly Constrained Reinforcement Learning through State\n  Augmentation and Reward Penalties",
  "abstract": "Constrained Reinforcement Learning has been employed to enforce safety\nconstraints on policy through the use of expected cost constraints. The key\nchallenge is in handling expected cost accumulated using the policy and not\njust in a single step. Existing methods have developed innovative ways of\nconverting this cost constraint over entire policy to constraints over local\ndecisions (at each time step). While such approaches have provided good\nsolutions with regards to objective, they can either be overly aggressive or\nconservative with respect to costs. This is owing to use of estimates for\n\"future\" or \"backward\" costs in local cost constraints.\n  To that end, we provide an equivalent unconstrained formulation to\nconstrained RL that has an augmented state space and reward penalties. This\nintuitive formulation is general and has interesting theoretical properties.\nMore importantly, this provides a new paradigm for solving constrained RL\nproblems effectively. As we show in our experimental results, we are able to\noutperform leading approaches on multiple benchmark problems from literature."
}