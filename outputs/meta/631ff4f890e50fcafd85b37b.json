{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Multilingual Visual Question Answering"
  ],
  "datasets": [
    "Crossmodal-3600"
  ],
  "methods": [
    "Translation-based framework for mVQA data generation",
    "Efficient annotation protocol for MaXM",
    "Unified, extensible, open-ended, and end-to-end mVQA modeling"
  ],
  "results": [
    "Strong performance in 13 languages"
  ],
  "paper_id": "631ff4f890e50fcafd85b37b",
  "title": "MaXM: Towards Multilingual Visual Question Answering",
  "abstract": "  Visual Question Answering (VQA) has been primarily studied through the lens of the English language. Yet, tackling VQA in other languages in the same manner would require a considerable amount of resources. In this paper, we propose scalable solutions to multilingual visual question answering (mVQA), on both data and modeling fronts. We first propose a translation-based framework to mVQA data generation that requires much less human annotation efforts than the conventional approach of directly collection questions and answers. Then, we apply our framework to the multilingual captions in the Crossmodal-3600 dataset and develop an efficient annotation protocol to create MaXM, a test-only VQA benchmark in 7 diverse languages. Finally, we propose an approach to unified, extensible, open-ended, and end-to-end mVQA modeling and demonstrate strong performance in 13 languages. "
}