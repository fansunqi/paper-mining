{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Instruction-tuned NLP model performance improvement"
  ],
  "datasets": [
    "NATURAL INSTRUCTIONS"
  ],
  "methods": [
    "Instruction-augmentation"
  ],
  "results": [
    "Improves model performance up to 35%",
    "An additional instruction equivalent to ~200 data samples"
  ],
  "paper_id": "6233f88c5aee126c0f94b4ef",
  "title": "How Many Data Samples is an Additional Instruction Worth?",
  "abstract": "  Recently introduced instruction-paradigm empowers non-expert users to leverage NLP resources by defining a new task in natural language. Instruction-tuned models have significantly outperformed multitask learning models (without instruction); however they are far from state-of-the-art task-specific models. Conventional approaches to improve model performance via creating datasets with large number of task instances or architectural changes in the model may not be feasible for non-expert users. However, they can write alternate instructions to represent an instruction task. Is Instruction-augmentation helpful? We augment a subset of tasks in the expanded version of NATURAL INSTRUCTIONS with additional instructions and find that it significantly improves model performance (up to 35%), especially in the low-data regime. Our results indicate that an additional instruction can be equivalent to ~200 data samples on average across tasks. "
}