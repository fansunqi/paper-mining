{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Decision theoretic analysis of bandit experiments"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Asymptotic Bayes and minimax risk",
    "Second-order partial differential equation (PDE)",
    "Limit of experiments approach",
    "Sparse matrix routines"
  ],
  "results": [
    "Minimal Bayes risk characterized by PDE",
    "Optimal Bayes and minimax policies dominate Thompson sampling and UCB by a factor of two",
    "Covers time discounting and pure exploration"
  ],
  "paper_id": "61b80b6e5244ab9dcbf4905e",
  "title": "Risk and optimal policies in bandit experiments",
  "abstract": "  We provide a decision theoretic analysis of bandit experiments. Working within the framework of diffusion asymptotics, we define suitable notions of asymptotic Bayes and minimax risk for these experiments. For normally distributed rewards, the minimal Bayes risk can be characterized as the solution to a second-order partial differential equation (PDE). Using a limit of experiments approach, we show that this PDE characterization also holds asymptotically under both parametric and non-parametric distributions of the rewards. The approach further describes the state variables it is asymptotically sufficient to restrict attention to, and thereby suggests a practical strategy for dimension reduction. The PDEs characterizing minimal Bayes risk can be solved efficiently using sparse matrix routines. We derive the optimal Bayes and minimax policies from their numerical solutions. These optimal policies substantially dominate existing methods such as Thompson sampling and UCB, often by a factor of two. The framework also covers time discounting and pure exploration. "
}