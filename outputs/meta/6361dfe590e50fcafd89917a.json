{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Semi-supervised Expectation Maximization"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Expectation Maximization (EM) algorithm",
    "Exponential family mixture model",
    "Gaussian mixture model",
    "Symmetric mixture of two Gaussians"
  ],
  "results": [
    "Improves convergence rate",
    "Comprehensive description of convergence rate",
    "Alternative proof for population EM's convergence rate"
  ],
  "paper_id": "6361dfe590e50fcafd89917a",
  "title": "On the Semi-supervised Expectation Maximization",
  "abstract": "  The Expectation Maximization (EM) algorithm is widely used as an iterative modification to maximum likelihood estimation when the data is incomplete. We focus on a semi-supervised case to learn the model from labeled and unlabeled samples. Existing work in the semi-supervised case has focused mainly on performance rather than convergence guarantee, however we focus on the contribution of the labeled samples to the convergence rate. The analysis clearly demonstrates how the labeled samples improve the convergence rate for the exponential family mixture model. In this case, we assume that the population EM (EM with unlimited data) is initialized within the neighborhood of global convergence for the population EM that consists solely of samples that have not been labeled. The analysis for the labeled samples provides a comprehensive description of the convergence rate for the Gaussian mixture model. In addition, we extend the findings for labeled samples and offer an alternative proof for the population EM's convergence rate with unlabeled samples for the symmetric mixture of two Gaussians. "
}