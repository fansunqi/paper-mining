{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Graph Parsing"
  ],
  "datasets": [
    "English Resource Grammar (ERG)",
    "standard in-domain",
    "seven OOD corpora"
  ],
  "methods": [
    "Neural-Symbolic Inference",
    "Compositional Uncertainty Quantification"
  ],
  "results": [
    "35.26% and 35.60% error reduction in aggregated Smatch score",
    "14% absolute accuracy gain in key tail linguistic categories"
  ],
  "paper_id": "63d7352390e50fcafda302a5",
  "title": "Neural-Symbolic Inference for Robust Autoregressive Graph Parsing via\n  Compositional Uncertainty Quantification",
  "abstract": "  Pre-trained seq2seq models excel at graph semantic parsing with rich annotated data, but generalize worse to out-of-distribution (OOD) and long-tail examples. In comparison, symbolic parsers under-perform on population-level metrics, but exhibit unique strength in OOD and tail generalization. In this work, we study compositionality-aware approach to neural-symbolic inference informed by model confidence, performing fine-grained neural-symbolic reasoning at subgraph level (i.e., nodes and edges) and precisely targeting subgraph components with high uncertainty in the neural parser. As a result, the method combines the distinct strength of the neural and symbolic approaches in capturing different aspects of the graph prediction, leading to well-rounded generalization performance both across domains and in the tail. We empirically investigate the approach in the English Resource Grammar (ERG) parsing problem on a diverse suite of standard in-domain and seven OOD corpora. Our approach leads to 35.26% and 35.60% error reduction in aggregated Smatch score over neural and symbolic approaches respectively, and 14% absolute accuracy gain in key tail linguistic categories over the neural model, outperforming prior state-of-art methods that do not account for compositionality or uncertainty. "
}