{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Federated learning",
    "Image classification",
    "Semantic segmentation"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Bayesian nonparametric framework",
    "Kullback-Leibler divergence penalty"
  ],
  "results": [
    "None"
  ],
  "paper_id": "637454dc90e50fcafdfc6897",
  "title": "Bayesian Federated Neural Matching that Completes Full Information",
  "abstract": "  Federated learning is a contemporary machine learning paradigm where locally trained models are distilled into a global model. Due to the intrinsic permutation invariance of neural networks, Probabilistic Federated Neural Matching (PFNM) employs a Bayesian nonparametric framework in the generation process of local neurons, and then creates a linear sum assignment formulation in each alternative optimization iteration. But according to our theoretical analysis, the optimization iteration in PFNM omits global information from existing. In this study, we propose a novel approach that overcomes this flaw by introducing a Kullback-Leibler divergence penalty at each iteration. The effectiveness of our approach is demonstrated by experiments on both image classification and semantic segmentation tasks. "
}