{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Risk-sensitive policy learning in reinforcement learning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Policy gradient algorithms for distortion risk measures",
    "Likelihood ratio-based gradient estimation"
  ],
  "results": [
    "Convergence to an approximate stationary point of the DRM objective",
    "Non-asymptotic bounds established"
  ],
  "paper_id": "60ebf64791e0119571374121",
  "title": "Policy Gradient Methods for Distortion Risk Measures",
  "abstract": "  We propose policy gradient algorithms which learn risk-sensitive policies in a reinforcement learning (RL) framework. Our proposed algorithms maximize the distortion risk measure (DRM) of the cumulative reward in an episodic Markov decision process in on-policy as well as off-policy RL settings. We derive a variant of the policy gradient theorem that caters to the DRM objective, and use this theorem in conjunction with a likelihood ratio-based gradient estimation scheme. We derive non-asymptotic bounds that establish the convergence of our proposed algorithms to an approximate stationary point of the DRM objective. "
}