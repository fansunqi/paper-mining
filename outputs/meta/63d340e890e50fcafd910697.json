{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Image Super-Resolution"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Efficient Striped Window Transformer",
    "Striped window mechanism",
    "Flexible window training strategy"
  ],
  "results": [
    "Outperforms state-of-the-art LSR transformers",
    "Better trade-off between model performance and complexity",
    "Fewer parameters",
    "Faster inference",
    "Smaller FLOPs",
    "Less memory consumption"
  ],
  "paper_id": "63d340e890e50fcafd910697",
  "title": "Image Super-Resolution using Efficient Striped Window Transformer",
  "abstract": "  Transformers have achieved remarkable results in single-image super-resolution (SR). However, the challenge of balancing model performance and complexity has hindered their application in lightweight SR (LSR). To tackle this challenge, we propose an efficient striped window transformer (ESWT). We revisit the normalization layer in the transformer and design a concise and efficient transformer structure to build the ESWT. Furthermore, we introduce a striped window mechanism to model long-term dependencies more efficiently. To fully exploit the potential of the ESWT, we propose a novel flexible window training strategy that can improve the performance of the ESWT without additional cost. Extensive experiments show that ESWT outperforms state-of-the-art LSR transformers, and achieves a better trade-off between model performance and complexity. The ESWT requires fewer parameters, incurs faster inference, smaller FLOPs, and less memory consumption, making it a promising solution for LSR. "
}