{
  "code_links": [
    "https://github.com/czczup/ViT-Adapter/tree/main/wsdm2023"
  ],
  "tasks": [
    "Visual Question Answering"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "ViT-Adapter",
    "Uni-Perceiver"
  ],
  "results": [
    "77.5 and 76.347 IoU on public and private test sets"
  ],
  "paper_id": "63f4367390e50fcafd6c76bd",
  "title": "Champion Solution for the WSDM2023 Toloka VQA Challenge",
  "abstract": "  In this report, we present our champion solution to the WSDM2023 Toloka Visual Question Answering (VQA) Challenge. Different from the common VQA and visual grounding (VG) tasks, this challenge involves a more complex scenario, i.e. inferring and locating the object implicitly specified by the given interrogative question. For this task, we leverage ViT-Adapter, a pre-training-free adapter network, to adapt multi-modal pre-trained Uni-Perceiver for better cross-modal localization. Our method ranks first on the leaderboard, achieving 77.5 and 76.347 IoU on public and private test sets, respectively. It shows that ViT-Adapter is also an effective paradigm for adapting the unified perception model to vision-language downstream tasks. Code and models will be released at https://github.com/czczup/ViT-Adapter/tree/main/wsdm2023. "
}