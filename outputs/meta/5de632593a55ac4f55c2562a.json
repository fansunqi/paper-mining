{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Evaluating language models' knowledge of major grammatical phenomena in English"
  ],
  "datasets": [
    "BLiMP"
  ],
  "methods": [
    "n-gram",
    "LSTM",
    "Transformer (GPT-2 and Transformer-XL)"
  ],
  "results": [
    "State-of-the-art models identify morphological contrasts reliably",
    "Models struggle with semantic restrictions on quantifiers and negative polarity items",
    "Models struggle with subtle syntactic phenomena such as extraction islands"
  ],
  "paper_id": "5de632593a55ac4f55c2562a",
  "title": "BLiMP: The Benchmark of Linguistic Minimal Pairs for English",
  "abstract": "  We introduce The Benchmark of Linguistic Minimal Pairs (shortened to BLiMP), a challenge set for evaluating what language models (LMs) know about major grammatical phenomena in English. BLiMP consists of 67 sub-datasets, each containing 1000 minimal pairs isolating specific contrasts in syntax, morphology, or semantics. The data is automatically generated according to expert-crafted grammars, and aggregate human agreement with the labels is 96.4%. We use it to evaluate n-gram, LSTM, and Transformer (GPT-2 and Transformer-XL) LMs. We find that state-of-the-art models identify morphological contrasts reliably, but they struggle with semantic restrictions on the distribution of quantifiers and negative polarity items and subtle syntactic phenomena such as extraction islands. "
}