{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Single image super-resolution (SISR)"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Neural architecture search with pruning search",
    "Automatic search framework for sparse SR models",
    "Weight sharing strategy with supernet",
    "Three-stage search: supernet construction, compiler-aware architecture and pruning search, compiler-aware pruning ratio search"
  ],
  "results": [
    "Real-time SR inference (tens of milliseconds per frame)",
    "720p resolution with competitive image quality (PSNR and SSIM)",
    "Deployment on mobile platforms (Samsung Galaxy S20)"
  ],
  "paper_id": "612335b45244ab9dcb3943a2",
  "title": "Achieving on-Mobile Real-Time Super-Resolution with Neural Architecture\n  and Pruning Search",
  "abstract": "  Though recent years have witnessed remarkable progress in single image super-resolution (SISR) tasks with the prosperous development of deep neural networks (DNNs), the deep learning methods are confronted with the computation and memory consumption issues in practice, especially for resource-limited platforms such as mobile devices. To overcome the challenge and facilitate the real-time deployment of SISR tasks on mobile, we combine neural architecture search with pruning search and propose an automatic search framework that derives sparse super-resolution (SR) models with high image quality while satisfying the real-time inference requirement. To decrease the search cost, we leverage the weight sharing strategy by introducing a supernet and decouple the search problem into three stages, including supernet construction, compiler-aware architecture and pruning search, and compiler-aware pruning ratio search. With the proposed framework, we are the first to achieve real-time SR inference (with only tens of milliseconds per frame) for implementing 720p resolution with competitive image quality (in terms of PSNR and SSIM) on mobile platforms (Samsung Galaxy S20). "
}