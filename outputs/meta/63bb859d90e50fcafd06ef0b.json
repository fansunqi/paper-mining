{
  "code_links": [
    "None"
  ],
  "tasks": [
    "NER task of the optimization problem"
  ],
  "datasets": [
    "NL4Opt competition dataset"
  ],
  "methods": [
    "Fine-tuning pretrained language models",
    "Differential learning rates",
    "Adversarial training",
    "Model ensemble"
  ],
  "results": [
    "Micro-averaged F1 score of 93.3%",
    "Second prize in the NER task"
  ],
  "paper_id": "63bb859d90e50fcafd06ef0b",
  "title": "OPD@NL4Opt: An ensemble approach for the NER task of the optimization\n  problem",
  "abstract": "  In this paper, we present an ensemble approach for the NL4Opt competition subtask 1(NER task). For this task, we first fine tune the pretrained language models based on the competition dataset. Then we adopt differential learning rates and adversarial training strategies to enhance the model generalization and robustness. Additionally, we use a model ensemble method for the final prediction, which achieves a micro-averaged F1 score of 93.3% and attains the second prize in the NER task. "
}