{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Discrete-time decision-making problems",
    "Worst-case control actions",
    "Reinforcement learning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Information states",
    "Approximate information states",
    "Dynamic program (DP)"
  ],
  "results": [
    "None"
  ],
  "paper_id": "63c0cc6490e50fcafd2a8dae",
  "title": "Approximate Information States for Worst-Case Control and Learning in\n  Uncertain Systems",
  "abstract": "  In this paper, we investigate discrete-time decision-making problems in uncertain systems with partially observed states. We consider a non-stochastic model, where uncontrolled disturbances acting on the system take values in bounded sets with unknown distributions. We present a general framework for decision-making in such problems by developing the notions of information states and approximate information states. In our definition of an information state, we introduce conditions to identify for an uncertain variable sufficient to construct a dynamic program (DP) that computes an optimal strategy. We show that many information states from the literature on worst-case control actions, e.g., the conditional range, are examples of our more general definition. Next, we relax these conditions to define approximate information states using only output variables, which can be learned from output data without knowledge of system dynamics. We use this notion to formulate an approximate DP that yields a strategy with a bounded performance loss. Finally, we illustrate the application of our results in control and reinforcement learning using numerical examples. "
}