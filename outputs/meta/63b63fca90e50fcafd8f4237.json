{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Explainable methods for infant brain"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "explainable geometric deep network"
  ],
  "results": [
    "None"
  ],
  "paper_id": "63b63fca90e50fcafd8f4237",
  "title": "A attention way in Explainable methods for infant brain",
  "abstract": "  Deploying reliable deep learning techniques in interdisciplinary applications needs learned models to output accurate and ({even more importantly}) explainable predictions. Existing approaches typically explicate network outputs in a post-hoc fashion, under an implicit assumption that faithful explanations come from accurate predictions/classifications. We have an opposite claim that explanations boost (or even determine) classification. That is, end-to-end learning of explanation factors to augment discriminative representation extraction could be a more intuitive strategy to inversely assure fine-grained explainability, e.g., in those neuroimaging and neuroscience studies with high-dimensional data containing noisy, redundant, and task-irrelevant information. In this paper, we propose such an explainable geometric deep network dubbed. "
}