{
  "code_links": [
    "https://github.com/wlin-at/CycDA"
  ],
  "tasks": [
    "Unsupervised Cycle Domain Adaptation",
    "Image-to-Video Adaptation"
  ],
  "datasets": [
    "benchmark datasets for image-to-video",
    "mixed-source domain adaptation"
  ],
  "methods": [
    "Cycle Domain Adaptation (CycDA)",
    "joint spatial information leveraging",
    "independent spatio-temporal model",
    "knowledge transfer between spatial and spatio-temporal learning"
  ],
  "results": [
    "state-of-the-art results for image-to-video adaptation",
    "demonstrated benefits of cyclic adaptation"
  ],
  "paper_id": "62451c325aee126c0f47b4a5",
  "title": "CycDA: Unsupervised Cycle Domain Adaptation from Image to Video",
  "abstract": "  Although action recognition has achieved impressive results over recent years, both collection and annotation of video training data are still time-consuming and cost intensive. Therefore, image-to-video adaptation has been proposed to exploit labeling-free web image source for adapting on unlabeled target videos. This poses two major challenges: (1) spatial domain shift between web images and video frames; (2) modality gap between image and video data. To address these challenges, we propose Cycle Domain Adaptation (CycDA), a cycle-based approach for unsupervised image-to-video domain adaptation by leveraging the joint spatial information in images and videos on the one hand and, on the other hand, training an independent spatio-temporal model to bridge the modality gap. We alternate between the spatial and spatio-temporal learning with knowledge transfer between the two in each cycle. We evaluate our approach on benchmark datasets for image-to-video as well as for mixed-source domain adaptation achieving state-of-the-art results and demonstrating the benefits of our cyclic adaptation. Code is available at \\url{https://github.com/wlin-at/CycDA}. "
}