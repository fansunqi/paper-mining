{
  "code_links": [
    "https://github.com/ethz-asl/maplab"
  ],
  "tasks": [
    "Simultaneous Localization And Mapping (SLAM)"
  ],
  "datasets": [
    "HILTI 2021 benchmark"
  ],
  "methods": [
    "Multi-modality",
    "Deep learning"
  ],
  "results": [
    "Accuracy comparable to state-of-the-art"
  ],
  "paper_id": "63896cd690e50fcafde7a14a",
  "title": "maplab 2.0 - A Modular and Multi-Modal Mapping Framework",
  "abstract": "  Integration of multiple sensor modalities and deep learning into Simultaneous Localization And Mapping (SLAM) systems are areas of significant interest in current research. Multi-modality is a stepping stone towards achieving robustness in challenging environments and interoperability of heterogeneous multi-robot systems with varying sensor setups. With maplab 2.0, we provide a versatile open-source platform that facilitates developing, testing, and integrating new modules and features into a fully-fledged SLAM system. Through extensive experiments, we show that maplab 2.0's accuracy is comparable to the state-of-the-art on the HILTI 2021 benchmark. Additionally, we showcase the flexibility of our system with three use cases: i) large-scale (approx. 10 km) multi-robot multi-session (23 missions) mapping, ii) integration of non-visual landmarks, and iii) incorporating a semantic object-based loop closure module into the mapping framework. The code is available open-source at https://github.com/ethz-asl/maplab. "
}