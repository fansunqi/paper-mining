{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Meta-learning",
    "Convex optimization"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Gradient-based meta-learning",
    "Convex optimization techniques",
    "Bootstrapped Meta-Gradients"
  ],
  "results": [
    "Convergence rates for meta-learning in the single task setting",
    "Optimism in meta-learning captured through Bootstrapped Meta-Gradients"
  ],
  "paper_id": "63bcd73690e50fcafdefa208",
  "title": "Optimistic Meta-Gradients",
  "abstract": "  We study the connection between gradient-based meta-learning and convex op-timisation. We observe that gradient descent with momentum is a special case of meta-gradients, and building on recent results in optimisation, we prove convergence rates for meta-learning in the single task setting. While a meta-learned update rule can yield faster convergence up to constant factor, it is not sufficient for acceleration. Instead, some form of optimism is required. We show that optimism in meta-learning can be captured through Bootstrapped Meta-Gradients (Flennerhag et al., 2022), providing deeper insight into its underlying mechanics. "
}