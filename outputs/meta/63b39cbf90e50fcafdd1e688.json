{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Federated Learning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Matching process between data representations and class representations",
    "Data encoder and label encoder",
    "Use of natural-language class names as common ground"
  ],
  "results": [
    "Outperforms various classical and state-of-the-art federated learning methods for non-IID data"
  ],
  "paper_id": "63b39cbf90e50fcafdd1e688",
  "title": "Federated Learning with Client-Exclusive Classes",
  "abstract": "  Existing federated classification algorithms typically assume the local annotations at every client cover the same set of classes. In this paper, we aim to lift such an assumption and focus on a more general yet practical non-IID setting where every client can work on non-identical and even disjoint sets of classes (i.e., client-exclusive classes), and the clients have a common goal which is to build a global classification model to identify the union of these classes. Such heterogeneity in client class sets poses a new challenge: how to ensure different clients are operating in the same latent space so as to avoid the drift after aggregation? We observe that the classes can be described in natural languages (i.e., class names) and these names are typically safe to share with all parties. Thus, we formulate the classification problem as a matching process between data representations and class representations and break the classification model into a data encoder and a label encoder. We leverage the natural-language class names as the common ground to anchor the class representations in the label encoder. In each iteration, the label encoder updates the class representations and regulates the data representations through matching. We further use the updated class representations at each round to annotate data samples for locally-unaware classes according to similarity and distill knowledge to local models. Extensive experiments on four real-world datasets show that the proposed method can outperform various classical and state-of-the-art federated learning methods designed for learning with non-IID data. "
}