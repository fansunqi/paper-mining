{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Video Anomaly Detection"
  ],
  "datasets": [
    "Avenue",
    "ShanghaiTech",
    "UBnormal"
  ],
  "methods": [
    "SSMTL++",
    "Multi-head self-attention modules",
    "Vision transformers",
    "CvT blocks",
    "Self-supervised learning tasks"
  ],
  "results": [
    "State-of-the-art performance on Avenue, ShanghaiTech, and UBnormal"
  ],
  "paper_id": "62d620f45aee126c0fad3ead",
  "title": "SSMTL++: Revisiting Self-Supervised Multi-Task Learning for Video\n  Anomaly Detection",
  "abstract": "  A self-supervised multi-task learning (SSMTL) framework for video anomaly detection was recently introduced in literature. Due to its highly accurate results, the method attracted the attention of many researchers. In this work, we revisit the self-supervised multi-task learning framework, proposing several updates to the original method. First, we study various detection methods, e.g. based on detecting high-motion regions using optical flow or background subtraction, since we believe the currently used pre-trained YOLOv3 is suboptimal, e.g. objects in motion or objects from unknown classes are never detected. Second, we modernize the 3D convolutional backbone by introducing multi-head self-attention modules, inspired by the recent success of vision transformers. As such, we alternatively introduce both 2D and 3D convolutional vision transformer (CvT) blocks. Third, in our attempt to further improve the model, we study additional self-supervised learning tasks, such as predicting segmentation maps through knowledge distillation, solving jigsaw puzzles, estimating body pose through knowledge distillation, predicting masked regions (inpainting), and adversarial learning with pseudo-anomalies. We conduct experiments to assess the performance impact of the introduced changes. Upon finding more promising configurations of the framework, dubbed SSMTL++v1 and SSMTL++v2, we extend our preliminary experiments to more data sets, demonstrating that our performance gains are consistent across all data sets. In most cases, our results on Avenue, ShanghaiTech and UBnormal raise the state-of-the-art performance bar to a new level. "
}