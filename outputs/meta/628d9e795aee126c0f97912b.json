{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Privacy-Preserving Data Filtering in Federated Learning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Practical influence approximation (`lazy' influence)`"
  ],
  "results": [
    "Recall of >90%, even up to 100%"
  ],
  "paper_id": "628d9e795aee126c0f97912b",
  "title": "A Practical Influence Approximation for Privacy-Preserving Data\n  Filtering in Federated Learning",
  "abstract": "  Federated Learning by nature is susceptible to low-quality, corrupted, or even malicious data that can severely degrade the quality of the learned model. Traditional techniques for data valuation cannot be applied as the data is never revealed. We present a novel technique for filtering, and scoring data based on a practical influence approximation (`lazy' influence) that can be implemented in a privacy-preserving manner. Each participant uses his own data to evaluate the influence of another participant's batch, and reports to the center an obfuscated score using differential privacy. Our technique allows for highly effective filtering of corrupted data in a variety of applications. Importantly, we show that most of the corrupted data can be filtered out (recall of $>90\\%$, and even up to $100\\%$), even under really strong privacy guarantees ($\\varepsilon \\leq 1$). "
}