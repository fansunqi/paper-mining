{
  "code_links": [
    "None"
  ],
  "tasks": [
    "AI Alignment in Support Agents"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "AI Alignment Dialogues"
  ],
  "results": [
    "None"
  ],
  "paper_id": "63c8b56c90e50fcafd905ff0",
  "title": "AI Alignment Dialogues: An Interactive Approach to AI Alignment in\n  Support Agents",
  "abstract": "  AI alignment is about ensuring AI systems only pursue goals and activities that are beneficial to humans. Most of the current approach to AI alignment is to learn what humans value from their behavioural data. This paper proposes a different way of looking at the notion of alignment, namely by introducing AI Alignment Dialogues: dialogues with which users and agents try to achieve and maintain alignment via interaction. We argue that alignment dialogues have a number of advantages in comparison to data-driven approaches, especially for behaviour support agents, which aim to support users in achieving their desired future behaviours rather than their current behaviours. The advantages of alignment dialogues include allowing the users to directly convey higher-level concepts to the agent, and making the agent more transparent and trustworthy. In this paper we outline the concept and high-level structure of alignment dialogues. Moreover, we conducted a qualitative focus group user study from which we developed a model that describes how alignment dialogues affect users, and created design suggestions for AI alignment dialogues. Through this we establish foundations for AI alignment dialogues and shed light on what requires further development and research. "
}