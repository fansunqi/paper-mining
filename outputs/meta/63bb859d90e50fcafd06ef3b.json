{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Multi-task learning",
    "Sequential dependence MTL"
  ],
  "datasets": [
    "Offline datasets",
    "Online A/B implementation"
  ],
  "methods": [
    "Task Aware Feature Extraction (TAFE)",
    "Dependence task learning loss"
  ],
  "results": [
    "None"
  ],
  "paper_id": "63bb859d90e50fcafd06ef3b",
  "title": "Adaptive Pattern Extraction Multi-Task Learning for Multi-Step\n  Conversion Estimations",
  "abstract": "  Multi-task learning (MTL) has been successfully implemented in many real-world applications, which aims to simultaneously solve multiple tasks with a single model. The general idea of multi-task learning is designing kinds of global parameter sharing mechanism and task-specific feature extractor to improve the performance of all tasks. However, sequential dependence between tasks are rarely studied but frequently encountered in e-commence online recommendation, e.g. impression, click and conversion on displayed product. There is few theoretical work on this problem and biased optimization object adopted in most MTL methods deteriorates online performance. Besides, challenge still remains in balancing the trade-off between various tasks and effectively learn common and specific representation. In this paper, we first analyze sequential dependence MTL from rigorous mathematical perspective and design a dependence task learning loss to provide an unbiased optimizing object. And we propose a Task Aware Feature Extraction (TAFE) framework for sequential dependence MTL, which enables to selectively reconstruct implicit shared representations from a sample-wise view and extract explicit task-specific information in an more efficient way. Extensive experiments on offline datasets and online A/B implementation demonstrate the effectiveness of our proposed TAFE. "
}