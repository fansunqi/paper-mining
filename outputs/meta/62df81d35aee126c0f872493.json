{
  "code_links": [
    "https://github.com/md-mohaiminul/ObjectStateChange"
  ],
  "tasks": [
    "Object State Change Classification in Egocentric Videos"
  ],
  "datasets": [
    "Ego4D"
  ],
  "methods": [
    "Transformer-based video recognition model",
    "Divided Space-Time Attention mechanism"
  ],
  "results": [
    "Second-best performance in the Ego4D: Object State Change Classification Challenge",
    "Temporal modeling ability required for object state change identification"
  ],
  "paper_id": "62df81d35aee126c0f872493",
  "title": "Object State Change Classification in Egocentric Videos using the\n  Divided Space-Time Attention Mechanism",
  "abstract": "  This report describes our submission called \"TarHeels\" for the Ego4D: Object State Change Classification Challenge. We use a transformer-based video recognition model and leverage the Divided Space-Time Attention mechanism for classifying object state change in egocentric videos. Our submission achieves the second-best performance in the challenge. Furthermore, we perform an ablation study to show that identifying object state change in egocentric videos requires temporal modeling ability. Lastly, we present several positive and negative examples to visualize our model's predictions. The code is publicly available at: https://github.com/md-mohaiminul/ObjectStateChange "
}