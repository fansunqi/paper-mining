{
  "code_links": [
    "https://github.com/nii-yamagishilab/midi-to-audio"
  ],
  "tasks": [
    "MIDI-to-audio synthesis"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "TTS-based MIDI-to-audio system",
    "feature computation",
    "model selection",
    "training strategy"
  ],
  "results": [
    "highly natural music synthesis",
    "thorough analytical approach",
    "useful outcomes for the community"
  ],
  "paper_id": "638426ba90e50fcafdeb22a0",
  "title": "Can Knowledge of End-to-End Text-to-Speech Models Improve Neural\n  MIDI-to-Audio Synthesis Systems?",
  "abstract": "  With the similarity between music and speech synthesis from symbolic input and the rapid development of text-to-speech (TTS) techniques, it is worthwhile to explore ways to improve the MIDI-to-audio performance by borrowing from TTS techniques. In this study, we analyze the shortcomings of a TTS-based MIDI-to-audio system and improve it in terms of feature computation, model selection, and training strategy, aiming to synthesize highly natural-sounding audio. Moreover, we conducted an extensive model evaluation through listening tests, pitch measurement, and spectrogram analysis. This work demonstrates not only synthesis of highly natural music but offers a thorough analytical approach and useful outcomes for the community. Our code, pre-trained models, supplementary materials, and audio samples are open sourced at https://github.com/nii-yamagishilab/midi-to-audio. "
}