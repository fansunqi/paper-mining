{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Continual Egocentric Activity Recognition"
  ],
  "datasets": [
    "UESTC-MMEA-CL"
  ],
  "methods": [
    "Multi-modal deep learning",
    "Continual learning"
  ],
  "results": [
    "None"
  ],
  "paper_id": "63d340ef90e50fcafd9115aa",
  "title": "Towards Continual Egocentric Activity Recognition: A Multi-modal\n  Egocentric Activity Dataset for Continual Learning",
  "abstract": "With the rapid development of wearable cameras, a massive collection of\negocentric video for first-person visual perception becomes available. Using\negocentric videos to predict first-person activity faces many challenges,\nincluding limited field of view, occlusions, and unstable motions. Observing\nthat sensor data from wearable devices facilitates human activity recognition,\nmulti-modal activity recognition is attracting increasing attention. However,\nthe deficiency of related dataset hinders the development of multi-modal deep\nlearning for egocentric activity recognition. Nowadays, deep learning in real\nworld has led to a focus on continual learning that often suffers from\ncatastrophic forgetting. But the catastrophic forgetting problem for egocentric\nactivity recognition, especially in the context of multiple modalities, remains\nunexplored due to unavailability of dataset. In order to assist this research,\nwe present a multi-modal egocentric activity dataset for continual learning\nnamed UESTC-MMEA-CL, which is collected by self-developed glasses integrating a\nfirst-person camera and wearable sensors. It contains synchronized data of\nvideos, accelerometers, and gyroscopes, for 32 types of daily activities,\nperformed by 10 participants. Its class types and scale are compared with other\npublicly available datasets. The statistical analysis of the sensor data is\ngiven to show the auxiliary effects for different behaviors. And results of\negocentric activity recognition are reported when using separately, and\njointly, three modalities: RGB, acceleration, and gyroscope, on a base network\narchitecture. To explore the catastrophic forgetting in continual learning\ntasks, four baseline methods are extensively evaluated with different\nmulti-modal combinations. We hope the UESTC-MMEA-CL can promote future studies\non continual learning for first-person activity recognition in wearable\napplications."
}