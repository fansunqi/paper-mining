{
  "code_links": [
    "None"
  ],
  "tasks": [
    "3D Articulation Transfer from Single Image"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "CA$^2$T-Net: A neural network with three distinct branches for pose prediction, part segmentation, and motion parameters"
  ],
  "results": [
    "Independent of object topology, works with objects from arbitrary categories",
    "Can animate a mesh, infer motion from real images, and transfer articulation to geometrically distinct 3D models"
  ],
  "paper_id": "641d14df90e50fcafdf739c8",
  "title": "CA$^2$T-Net: Category-Agnostic 3D Articulation Transfer from Single\n  Image",
  "abstract": "  We present a neural network approach to transfer the motion from a single image of an articulated object to a rest-state (i.e., unarticulated) 3D model. Our network learns to predict the object's pose, part segmentation, and corresponding motion parameters to reproduce the articulation shown in the input image. The network is composed of three distinct branches that take a shared joint image-shape embedding and is trained end-to-end. Unlike previous methods, our approach is independent of the topology of the object and can work with objects from arbitrary categories. Our method, trained with only synthetic data, can be used to automatically animate a mesh, infer motion from real images, and transfer articulation to functionally similar but geometrically distinct 3D models at test time. "
}