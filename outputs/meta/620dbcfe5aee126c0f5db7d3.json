{
  "code_links": [
    "https://github.com/ZitongYu/Flex-Modal-FAS"
  ],
  "tasks": [
    "Face anti-spoofing"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Flexible-modal FAS benchmark",
    "Train one for all",
    "Multi-modal (RGB+Depth+IR) FAS models",
    "Intra- and cross-dataset testings",
    "Deep models and feature fusion strategies"
  ],
  "results": [
    "Established the first flexible-modal FAS benchmark"
  ],
  "paper_id": "620dbcfe5aee126c0f5db7d3",
  "title": "Flexible-Modal Face Anti-Spoofing: A Benchmark",
  "abstract": "  Face anti-spoofing (FAS) plays a vital role in securing face recognition systems from presentation attacks. Benefitted from the maturing camera sensors, single-modal (RGB) and multi-modal (e.g., RGB+Depth) FAS has been applied in various scenarios with different configurations of sensors/modalities. Existing single- and multi-modal FAS methods usually separately train and deploy models for each possible modality scenario, which might be redundant and inefficient. Can we train a unified model, and flexibly deploy it under various modality scenarios? In this paper, we establish the first flexible-modal FAS benchmark with the principle `train one for all'. To be specific, with trained multi-modal (RGB+Depth+IR) FAS models, both intra- and cross-dataset testings are conducted on four flexible-modal sub-protocols (RGB, RGB+Depth, RGB+IR, and RGB+Depth+IR). We also investigate prevalent deep models and feature fusion strategies for flexible-modal FAS. We hope this new benchmark will facilitate the future research of the multi-modal FAS. The protocols and codes are available at https://github.com/ZitongYu/Flex-Modal-FAS. "
}