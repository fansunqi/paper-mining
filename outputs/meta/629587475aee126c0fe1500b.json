{
  "code_links": [
    "https://github.com/DingXiaoH/RepOptimizers"
  ],
  "tasks": [
    "Neural network optimization"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Gradient Re-parameterization",
    "RepOptimizers",
    "RepOpt-VGG"
  ],
  "results": [
    "RepOpt-VGG performs on par with or better than recent well-designed models",
    "Simple structure, high inference speed, and training efficiency"
  ],
  "paper_id": "629587475aee126c0fe1500b",
  "title": "Re-parameterizing Your Optimizers rather than Architectures",
  "abstract": "  The well-designed structures in neural networks reflect the prior knowledge incorporated into the models. However, though different models have various priors, we are used to training them with model-agnostic optimizers such as SGD. In this paper, we propose to incorporate model-specific prior knowledge into optimizers by modifying the gradients according to a set of model-specific hyper-parameters. Such a methodology is referred to as Gradient Re-parameterization, and the optimizers are named RepOptimizers. For the extreme simplicity of model structure, we focus on a VGG-style plain model and showcase that such a simple model trained with a RepOptimizer, which is referred to as RepOpt-VGG, performs on par with or better than the recent well-designed models. From a practical perspective, RepOpt-VGG is a favorable base model because of its simple structure, high inference speed and training efficiency. Compared to Structural Re-parameterization, which adds priors into models via constructing extra training-time structures, RepOptimizers require no extra forward/backward computations and solve the problem of quantization. We hope to spark further research beyond the realms of model structure design. Code and models \\url{https://github.com/DingXiaoH/RepOptimizers}. "
}