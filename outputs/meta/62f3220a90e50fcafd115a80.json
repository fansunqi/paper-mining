{
  "code_links": [
    "https://github.com/lkeab/BCNet"
  ],
  "tasks": [
    "Instance Segmentation"
  ],
  "datasets": [
    "COCO",
    "KINS",
    "COCOA",
    "YTVIS",
    "OVIS",
    "BDD100K MOTS"
  ],
  "methods": [
    "Bilayer Convolutional Network (BCNet)",
    "Fully Convolutional Network (FCN)",
    "Graph Convolutional Network (GCN)",
    "Vision Transformer (ViT)"
  ],
  "results": [
    "Large and consistent improvements",
    "Especially for heavy occlusion cases"
  ],
  "paper_id": "62f3220a90e50fcafd115a80",
  "title": "Occlusion-Aware Instance Segmentation via BiLayer Network Architectures",
  "abstract": "  Segmenting highly-overlapping image objects is challenging, because there is typically no distinction between real object contours and occlusion boundaries on images. Unlike previous instance segmentation methods, we model image formation as a composition of two overlapping layers, and propose Bilayer Convolutional Network (BCNet), where the top layer detects occluding objects (occluders) and the bottom layer infers partially occluded instances (occludees). The explicit modeling of occlusion relationship with bilayer structure naturally decouples the boundaries of both the occluding and occluded instances, and considers the interaction between them during mask regression. We investigate the efficacy of bilayer structure using two popular convolutional network designs, namely, Fully Convolutional Network (FCN) and Graph Convolutional Network (GCN). Further, we formulate bilayer decoupling using the vision transformer (ViT), by representing instances in the image as separate learnable occluder and occludee queries. Large and consistent improvements using one/two-stage and query-based object detectors with various backbones and network layer choices validate the generalization ability of bilayer decoupling, as shown by extensive experiments on image instance segmentation benchmarks (COCO, KINS, COCOA) and video instance segmentation benchmarks (YTVIS, OVIS, BDD100K MOTS), especially for heavy occlusion cases. Code and data are available at https://github.com/lkeab/BCNet. "
}