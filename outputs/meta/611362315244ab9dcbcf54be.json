{
  "code_links": [
    "None"
  ],
  "tasks": [
    "External Validity of interventions and counterfactuals",
    "Out-of-sample prediction",
    "Intervention effect prediction",
    "Causal effect estimation"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Simple definition for External Validity",
    "Combinatorial enumeration problems",
    "Generalized supervised, explaining, and causal-effect estimators"
  ],
  "results": [
    "Improvements in External Validity, Unconfoundness, and Accuracy",
    "Gains in out-of-sample prediction, intervention effect prediction, and causal effect estimation",
    "Applications in COVID19 pandemic predictions with small samples and missing variables"
  ],
  "paper_id": "611362315244ab9dcbcf54be",
  "title": "The External Validity of Combinatorial Samples and Populations",
  "abstract": "  The widely used 'Counterfactual' definition of Causal Effects was derived for unbiasedness and accuracy - and not generalizability. We propose a simple definition for the External Validity (EV) of interventions and counterfactuals. The definition leads to EV statistics for individual counterfactuals, and to non-parametric effect estimators for sets of counterfactuals (i.e., for samples). We use this definition to discuss several issues that have baffled the original counterfactual formulation: out-of-sample validity, reliance on independence assumptions or estimation, concurrent estimation of multiple effects and full-models, bias-variance tradeoffs, statistical power, omitted variables, and connections to current predictive and explaining techniques.   Methodologically, the definition also allows us to replace the parametric, and generally ill-posed, estimation problems that followed the counterfactual definition by combinatorial enumeration problems in non-experimental samples. We use this framework to generalize popular supervised, explaining, and causal-effect estimators, improving their performance across three dimensions (External Validity, Unconfoundness and Accuracy), and enabling their use in non-i.i.d. samples. We demonstrate gains in out-of-sample prediction, intervention effect prediction, and causal effect estimation tasks. The COVID19 pandemic highlighted the need for learning solutions to provide general predictions in small samples - many times with missing variables. We also demonstrate applications in this pressing problem. "
}