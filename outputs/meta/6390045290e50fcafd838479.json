{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Sperm motility analysis",
    "Spermatozoa tracking"
  ],
  "datasets": [
    "VISEM-Tracking"
  ],
  "methods": [
    "YOLOv5 deep learning model"
  ],
  "results": [
    "Baseline sperm detection performances"
  ],
  "paper_id": "6390045290e50fcafd838479",
  "title": "VISEM-Tracking, a human spermatozoa tracking dataset",
  "abstract": "  A manual assessment of sperm motility requires microscopy observation, which is challenging due to the fast-moving spermatozoa in the field of view. To obtain correct results, manual evaluation requires extensive training. Therefore, computer-assisted sperm analysis (CASA) has become increasingly used in clinics. Despite this, more data is needed to train supervised machine learning approaches in order to improve accuracy and reliability in the assessment of sperm motility and kinematics. In this regard, we provide a dataset called VISEM-Tracking with 20 video recordings of 30 seconds (comprising 29,196 frames) of wet sperm preparations with manually annotated bounding-box coordinates and a set of sperm characteristics analyzed by experts in the domain. In addition to the annotated data, we provide unlabeled video clips for easy-to-use access and analysis of the data via methods such as self- or unsupervised learning. As part of this paper, we present baseline sperm detection performances using the YOLOv5 deep learning (DL) model trained on the VISEM-Tracking dataset. As a result, we show that the dataset can be used to train complex DL models to analyze spermatozoa. "
}