{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Automatic term extraction"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Transformer-based neural models"
  ],
  "results": [
    "None"
  ],
  "paper_id": "63c8b59590e50fcafd90b6b2",
  "title": "The Recent Advances in Automatic Term Extraction: A survey",
  "abstract": "  Automatic term extraction (ATE) is a Natural Language Processing (NLP) task that eases the effort of manually identifying terms from domain-specific corpora by providing a list of candidate terms. As units of knowledge in a specific field of expertise, extracted terms are not only beneficial for several terminographical tasks, but also support and improve several complex downstream tasks, e.g., information retrieval, machine translation, topic detection, and sentiment analysis. ATE systems, along with annotated datasets, have been studied and developed widely for decades, but recently we observed a surge in novel neural systems for the task at hand. Despite a large amount of new research on ATE, systematic survey studies covering novel neural approaches are lacking. We present a comprehensive survey of deep learning-based approaches to ATE, with a focus on Transformer-based neural models. The study also offers a comparison between these systems and previous ATE approaches, which were based on feature engineering and non-neural supervised learning algorithms. "
}