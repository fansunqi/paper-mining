{
  "code_links": [
    "https://github.com/kkahatapitiya/SSDet"
  ],
  "tasks": [
    "Temporal Activity Detection"
  ],
  "datasets": [
    "Charades",
    "MultiTHUMOS"
  ],
  "methods": [
    "Weakly-guided self-supervised pretraining",
    "Generating frame-level pseudo labels",
    "Multi-action frames",
    "Action segments"
  ],
  "results": [
    "Outperforms prior work on multiple activity detection benchmarks"
  ],
  "paper_id": "61a444e15244ab9dcb6e77ea",
  "title": "Weakly-guided Self-supervised Pretraining for Temporal Activity\n  Detection",
  "abstract": "  Temporal Activity Detection aims to predict activity classes per frame, in contrast to video-level predictions in Activity Classification (i.e., Activity Recognition). Due to the expensive frame-level annotations required for detection, the scale of detection datasets is limited. Thus, commonly, previous work on temporal activity detection resorts to fine-tuning a classification model pretrained on large-scale classification datasets (e.g., Kinetics-400). However, such pretrained models are not ideal for downstream detection, due to the disparity between the pretraining and the downstream fine-tuning tasks. In this work, we propose a novel 'weakly-guided self-supervised' pretraining method for detection. We leverage weak labels (classification) to introduce a self-supervised pretext task (detection) by generating frame-level pseudo labels, multi-action frames, and action segments. Simply put, we design a detection task similar to downstream, on large-scale classification data, without extra annotations. We show that the models pretrained with the proposed weakly-guided self-supervised detection task outperform prior work on multiple challenging activity detection benchmarks, including Charades and MultiTHUMOS. Our extensive ablations further provide insights on when and how to use the proposed models for activity detection. Code is available at https://github.com/kkahatapitiya/SSDet. "
}