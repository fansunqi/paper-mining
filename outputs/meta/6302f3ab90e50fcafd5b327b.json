{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Multi-person motion prediction"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Social-Aware Motion Transformer (SoMoFormer)",
    "social-aware motion attention mechanism"
  ],
  "results": [
    "Outperforms state-of-the-art methods on multi-person motion prediction"
  ],
  "paper_id": "6302f3ab90e50fcafd5b327b",
  "title": "SoMoFormer: Social-Aware Motion Transformer for Multi-Person Motion\n  Prediction",
  "abstract": "  Multi-person motion prediction remains a challenging problem, especially in the joint representation learning of individual motion and social interactions. Most prior methods only involve learning local pose dynamics for individual motion (without global body trajectory) and also struggle to capture complex interaction dependencies for social interactions. In this paper, we propose a novel Social-Aware Motion Transformer (SoMoFormer) to effectively model individual motion and social interactions in a joint manner. Specifically, SoMoFormer extracts motion features from sub-sequences in displacement trajectory space to effectively learn both local and global pose dynamics for each individual. In addition, we devise a novel social-aware motion attention mechanism in SoMoFormer to further optimize dynamics representations and capture interaction dependencies simultaneously via motion similarity calculation across time and social dimensions. On both short- and long-term horizons, we empirically evaluate our framework on multi-person motion datasets and demonstrate that our method greatly outperforms state-of-the-art methods of single- and multi-person motion prediction. Code will be made publicly available upon acceptance. "
}