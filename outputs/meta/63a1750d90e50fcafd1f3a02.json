{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Multi-agent Patrolling Problems"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Deep Reinforcement Learning",
    "Distributed homogeneous multi-agent architecture"
  ],
  "results": [
    "Fault-tolerant and robust patrolling system",
    "Simulation experiments validating overall patrol performance, battery recharging efficiency, and fault tolerance"
  ],
  "paper_id": "63a1750d90e50fcafd1f3a02",
  "title": "An Energy-aware, Fault-tolerant, and Robust Deep Reinforcement Learning\n  based approach for Multi-agent Patrolling Problems",
  "abstract": "  Autonomous vehicles are suited for continuous area patrolling problems. However, finding an optimal patrolling strategy can be challenging for many reasons. Firstly, patrolling environments are often complex and can include unknown environmental factors. Secondly, autonomous vehicles can have failures or hardware constraints, such as limited battery life. Importantly, patrolling large areas often requires multiple agents that need to collectively coordinate their actions. In this work, we consider these limitations and propose an approach based on model-free, deep multi-agent reinforcement learning. In this approach, the agents are trained to automatically recharge themselves when required, to support continuous collective patrolling. A distributed homogeneous multi-agent architecture is proposed, where all patrolling agents execute identical policies locally based on their local observations and shared information. This architecture provides a fault-tolerant and robust patrolling system that can tolerate agent failures and allow supplementary agents to be added to replace failed agents or to increase the overall patrol performance. The solution is validated through simulation experiments from multiple perspectives, including the overall patrol performance, the efficiency of battery recharging strategies, and the overall fault tolerance and robustness. "
}