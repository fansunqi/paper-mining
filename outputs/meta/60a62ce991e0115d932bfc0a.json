{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Anomaly Detection"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Masked Contrastive Learning",
    "Self-ensemble Inference"
  ],
  "results": [
    "Outperforms previous state-of-the-art methods by a significant margin on various benchmark datasets"
  ],
  "paper_id": "60a62ce991e0115d932bfc0a",
  "title": "Masked Contrastive Learning for Anomaly Detection",
  "abstract": "  Detecting anomalies is one fundamental aspect of a safety-critical software system, however, it remains a long-standing problem. Numerous branches of works have been proposed to alleviate the complication and have demonstrated their efficiencies. In particular, self-supervised learning based methods are spurring interest due to their capability of learning diverse representations without additional labels. Among self-supervised learning tactics, contrastive learning is one specific framework validating their superiority in various fields, including anomaly detection. However, the primary objective of contrastive learning is to learn task-agnostic features without any labels, which is not entirely suited to discern anomalies. In this paper, we propose a task-specific variant of contrastive learning named masked contrastive learning, which is more befitted for anomaly detection. Moreover, we propose a new inference method dubbed self-ensemble inference that further boosts performance by leveraging the ability learned through auxiliary self-supervision tasks. By combining our models, we can outperform previous state-of-the-art methods by a significant margin on various benchmark datasets. "
}