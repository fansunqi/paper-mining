{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Trade-off analysis for linear predictors"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Non-asymptotic trade-off analysis",
    "Marchenko-Pastur analysis"
  ],
  "results": [
    "Classical models have training loss close to noise level",
    "Modern models have a much larger number of parameters",
    "Marchenko-Pastur analysis coincides with distribution independent bound as overparametrization increases"
  ],
  "paper_id": "62df81d35aee126c0f87228f",
  "title": "A Universal Trade-off Between the Model Size, Test Loss, and Training\n  Loss of Linear Predictors",
  "abstract": "  In this work we establish an algorithm and distribution independent non-asymptotic trade-off between the model size, excess test loss, and training loss of linear predictors. Specifically, we show that models that perform well on the test data (have low excess loss) are either \"classical\" -- have training loss close to the noise level, or are \"modern\" -- have a much larger number of parameters compared to the minimum needed to fit the training data exactly.   We also provide a more precise asymptotic analysis when the limiting spectral distribution of the whitened features is Marchenko-Pastur. Remarkably, while the Marchenko-Pastur analysis is far more precise near the interpolation peak, where the number of parameters is just enough to fit the training data, it coincides exactly with the distribution independent bound as the level of overparametrization increases. "
}