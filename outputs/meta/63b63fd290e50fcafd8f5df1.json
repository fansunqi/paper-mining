{
  "code_links": [
    "None"
  ],
  "tasks": [
    "State-space modeling",
    "Spatio-temporal data modeling"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Encoder-decoder architecture",
    "Probabilistic formulation"
  ],
  "results": [
    "Optimal forecasting performance",
    "Effective extraction of relational information"
  ],
  "paper_id": "63b63fd290e50fcafd8f5df1",
  "title": "Graph state-space models",
  "abstract": "  State-space models constitute an effective modeling tool to describe multivariate time series and operate by maintaining an updated representation of the system state from which predictions are made. Within this framework, relational inductive biases, e.g., associated with functional dependencies existing among signals, are not explicitly exploited leaving unattended great opportunities for effective modeling approaches. The manuscript aims, for the first time, at filling this gap by matching state-space modeling and spatio-temporal data where the relational information, say the functional graph capturing latent dependencies, is learned directly from data and is allowed to change over time. Within a probabilistic formulation that accounts for the uncertainty in the data-generating process, an encoder-decoder architecture is proposed to learn the state-space model end-to-end on a downstream task. The proposed methodological framework generalizes several state-of-the-art methods and demonstrates to be effective in extracting meaningful relational information while achieving optimal forecasting performance in controlled environments. "
}