{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Explainable NLP"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Rationalization"
  ],
  "results": [
    "None"
  ],
  "paper_id": "6552e13d939a5f40823d943f",
  "title": "Rationalization for Explainable NLP: A Survey",
  "abstract": "Recent advances in deep learning have improved the performance of many\nNatural Language Processing (NLP) tasks such as translation,\nquestion-answering, and text classification. However, this improvement comes at\nthe expense of model explainability. Black-box models make it difficult to\nunderstand the internals of a system and the process it takes to arrive at an\noutput. Numerical (LIME, Shapley) and visualization (saliency heatmap)\nexplainability techniques are helpful; however, they are insufficient because\nthey require specialized knowledge. These factors led rationalization to emerge\nas a more accessible explainable technique in NLP. Rationalization justifies a\nmodel's output by providing a natural language explanation (rationale). Recent\nimprovements in natural language generation have made rationalization an\nattractive technique because it is intuitive, human-comprehensible, and\naccessible to non-technical users. Since rationalization is a relatively new\nfield, it is disorganized. As the first survey, rationalization literature in\nNLP from 2007-2022 is analyzed. This survey presents available methods,\nexplainable evaluations, code, and datasets used across various NLP tasks that\nuse rationalization. Further, a new subfield in Explainable AI (XAI), namely,\nRational AI (RAI), is introduced to advance the current state of\nrationalization. A discussion on observed insights, challenges, and future\ndirections is provided to point to promising research opportunities."
}