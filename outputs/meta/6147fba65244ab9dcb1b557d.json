{
  "code_links": [
    "https://github.com/Toloka/crowd-kit"
  ],
  "tasks": [
    "Crowdsourcing Quality Control"
  ],
  "datasets": [
    "Several datasets of different natures"
  ],
  "methods": [
    "Crowd-Kit",
    "Computational Quality Control Algorithms",
    "Data Quality Estimators",
    "Truth Inference Methods",
    "Aggregation Methods for Annotation Tasks"
  ],
  "results": [
    "Extensive evaluation",
    "Benchmarking computational quality control methods in a uniform, systematic, and reproducible way"
  ],
  "paper_id": "6147fba65244ab9dcb1b557d",
  "title": "Learning from Crowds with Crowd-Kit",
  "abstract": "  Quality control is a crux of crowdsourcing. While most means for quality control are organizational and imply worker selection, golden tasks, and post-acceptance, computational quality control techniques allow parameterizing the whole crowdsourcing process of workers, tasks, and labels, inferring and revealing relationships between them. In this paper, we present Crowd-Kit, a general-purpose crowdsourcing computational quality control toolkit. It provides efficient implementations in Python of computational quality control algorithms for crowdsourcing, including data quality estimators and truth inference methods. We focus on aggregation methods for all the major annotation tasks, from the categorical annotation in which latent label assumption is met to more complex tasks like image and sequence aggregation. We perform an extensive evaluation of our toolkit on several datasets of different natures, enabling benchmarking computational quality control methods in a uniform, systematic, and reproducible way using the same codebase. We release our code and data under an open-source license at https://github.com/Toloka/crowd-kit. "
}