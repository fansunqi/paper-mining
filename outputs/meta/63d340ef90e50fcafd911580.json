{
  "code_links": [
    "https://github.com/jennyzzt/perceptual-locomotion"
  ],
  "tasks": [
    "Perceptive legged locomotion"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Visual feedback",
    "Proprioceptive information",
    "CPG-based high-level imitation of foot-ground contacts"
  ],
  "results": [
    "Robust in presence of unseen terrains and external force perturbations"
  ],
  "paper_id": "63d340ef90e50fcafd911580",
  "title": "Perceptive Locomotion with Controllable Pace and Natural Gait\n  Transitions Over Uneven Terrains",
  "abstract": "  This work developed a learning framework for perceptive legged locomotion that combines visual feedback, proprioceptive information, and active gait regulation of foot-ground contacts. The perception requires only one forward-facing camera to obtain the heightmap, and the active regulation of gait paces and traveling velocity are realized through our formulation of CPG-based high-level imitation of foot-ground contacts. Through this framework, an end-user has the ability to command task-level inputs to control different walking speeds and gait frequencies according to the traversal of different terrains, which enables more reliable negotiation with encountered obstacles. The results demonstrated that the learned perceptive locomotion policy followed task-level control inputs with intended behaviors, and was robust in presence of unseen terrains and external force perturbations. A video demonstration can be found at https://youtu.be/OTzlWzDfAe8, and the codebase at https://github.com/jennyzzt/perceptual-locomotion. "
}