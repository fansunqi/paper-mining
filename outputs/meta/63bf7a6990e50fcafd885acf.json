{
  "code_links": [
    "None"
  ],
  "tasks": [
    "User-Centered Security in NLP",
    "Author profiling",
    "Cyberbullying detection"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Adversarial attacks on language"
  ],
  "results": [
    "None"
  ],
  "paper_id": "63bf7a6990e50fcafd885acf",
  "title": "User-Centered Security in Natural Language Processing",
  "abstract": "  This dissertation proposes a framework of user-centered security in Natural Language Processing (NLP), and demonstrates how it can improve the accessibility of related research. Accordingly, it focuses on two security domains within NLP with great public interest. First, that of author profiling, which can be employed to compromise online privacy through invasive inferences. Without access and detailed insight into these models' predictions, there is no reasonable heuristic by which Internet users might defend themselves from such inferences. Secondly, that of cyberbullying detection, which by default presupposes a centralized implementation; i.e., content moderation across social platforms. As access to appropriate data is restricted, and the nature of the task rapidly evolves (both through lexical variation, and cultural shifts), the effectiveness of its classifiers is greatly diminished and thereby often misrepresented.   Under the proposed framework, we predominantly investigate the use of adversarial attacks on language; i.e., changing a given input (generating adversarial samples) such that a given model does not function as intended. These attacks form a common thread between our user-centered security problems; they are highly relevant for privacy-preserving obfuscation methods against author profiling, and adversarial samples might also prove useful to assess the influence of lexical variation and augmentation on cyberbullying detection. "
}