{
  "code_links": [
    "None"
  ],
  "tasks": [
    "image classification",
    "nonlinear regression"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "neurons learning their own activation functions",
    "meta-learning efficient sets of nonlinear responses",
    "physics-informed Hamiltonian neural network"
  ],
  "results": [
    "neural networks outperform homogeneous counterparts",
    "examples of dynamical systems selecting diversity over uniformity"
  ],
  "paper_id": "62563f775aee126c0f6f12eb",
  "title": "Neuronal diversity can improve machine learning for physics and beyond",
  "abstract": "  Diversity conveys advantages in nature, yet homogeneous neurons typically comprise the layers of artificial neural networks. Here we construct neural networks from neurons that learn their own activation functions, quickly diversify, and subsequently outperform their homogeneous counterparts on image classification and nonlinear regression tasks. Sub-networks instantiate the neurons, which meta-learn especially efficient sets of nonlinear responses. Examples include conventional neural networks classifying digits and forecasting a van der Pol oscillator and a physics-informed Hamiltonian neural network learning H\\'enon-Heiles orbits. Such learned diversity provides examples of dynamical systems selecting diversity over uniformity and elucidates the role of diversity in natural and artificial systems. "
}