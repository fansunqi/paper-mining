{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Visual-inertial localization",
    "Relative pose regression",
    "Absolute pose regression"
  ],
  "datasets": [
    "EuRoC MAV",
    "PennCOSYVIO",
    "Industry dataset"
  ],
  "methods": [
    "Pose graph optimization",
    "Attention networks",
    "Auxiliary and Bayesian learning"
  ],
  "results": [
    "Accuracy improvements for APR-RPR and RPR-RPR tasks",
    "Experiments conducted on multiple datasets"
  ],
  "paper_id": "62ea18e05aee126c0fca2b7f",
  "title": "Benchmarking Visual-Inertial Deep Multimodal Fusion for Relative Pose\n  Regression and Odometry-aided Absolute Pose Regression",
  "abstract": "  Visual-inertial localization is a key problem in computer vision and robotics applications such as virtual reality, self-driving cars, and aerial vehicles. The goal is to estimate an accurate pose of an object when either the environment or the dynamics are known. Absolute pose regression (APR) techniques directly regress the absolute pose from an image input in a known scene using convolutional and spatio-temporal networks. Odometry methods perform relative pose regression (RPR) that predicts the relative pose from a known object dynamic (visual or inertial inputs). The localization task can be improved by retrieving information from both data sources for a cross-modal setup, which is a challenging problem due to contradictory tasks. In this work, we conduct a benchmark to evaluate deep multimodal fusion based on pose graph optimization and attention networks. Auxiliary and Bayesian learning are utilized for the APR task. We show accuracy improvements for the APR-RPR task and for the RPR-RPR task for aerial vehicles and hand-held devices. We conduct experiments on the EuRoC MAV and PennCOSYVIO datasets and record and evaluate a novel industry dataset. "
}