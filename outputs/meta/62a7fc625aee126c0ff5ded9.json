{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Gaussian Process Inference"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Gradient Boosting",
    "Kernel Ridge Regression",
    "Monte-Carlo estimation"
  ],
  "results": [
    "Improved knowledge uncertainty estimates",
    "Improved out-of-domain detection"
  ],
  "paper_id": "62a7fc625aee126c0ff5ded9",
  "title": "Gradient Boosting Performs Gaussian Process Inference",
  "abstract": "  This paper shows that gradient boosting based on symmetric decision trees can be equivalently reformulated as a kernel method that converges to the solution of a certain Kernel Ridge Regression problem. Thus, we obtain the convergence to a Gaussian Process' posterior mean, which, in turn, allows us to easily transform gradient boosting into a sampler from the posterior to provide better knowledge uncertainty estimates through Monte-Carlo estimation of the posterior variance. We show that the proposed sampler allows for better knowledge uncertainty estimates leading to improved out-of-domain detection. "
}