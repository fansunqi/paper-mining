{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Online Multi-Kernel Learning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Gossip algorithm",
    "Quantized communications"
  ],
  "results": [
    "Sub-linear regret",
    "Achieves sub-linear regret on non-fully connected graphs"
  ],
  "paper_id": "63d340e790e50fcafd910673",
  "title": "Gossiped and Quantized Online Multi-Kernel Learning",
  "abstract": "  In instances of online kernel learning where little prior information is available and centralized learning is unfeasible, past research has shown that distributed and online multi-kernel learning provides sub-linear regret as long as every pair of nodes in the network can communicate (i.e., the communications network is a complete graph). In addition, to manage the communication load, which is often a performance bottleneck, communications between nodes can be quantized. This letter expands on these results to non-fully connected graphs, which is often the case in wireless sensor networks. To address this challenge, we propose a gossip algorithm and provide a proof that it achieves sub-linear regret. Experiments with real datasets confirm our findings. "
}