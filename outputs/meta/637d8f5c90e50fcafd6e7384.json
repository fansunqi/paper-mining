{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Predicting 3D shape, articulation, viewpoint, texture, and lighting of articulated animals"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Implicit-explicit representation of articulated shape and appearance",
    "Neural fields and meshes",
    "Self-supervised vision transformer",
    "Viewpoint sampling scheme"
  ],
  "results": [
    "MagicPony outperforms prior work",
    "Excellent generalisation in reconstructing art"
  ],
  "paper_id": "637d8f5c90e50fcafd6e7384",
  "title": "MagicPony: Learning Articulated 3D Animals in the Wild",
  "abstract": "  We consider the problem of predicting the 3D shape, articulation, viewpoint, texture, and lighting of an articulated animal like a horse given a single test image as input. We present a new method, dubbed MagicPony, that learns this predictor purely from in-the-wild single-view images of the object category, with minimal assumptions about the topology of deformation. At its core is an implicit-explicit representation of articulated shape and appearance, combining the strengths of neural fields and meshes. In order to help the model understand an object's shape and pose, we distil the knowledge captured by an off-the-shelf self-supervised vision transformer and fuse it into the 3D model. To overcome local optima in viewpoint estimation, we further introduce a new viewpoint sampling scheme that comes at no additional training cost. MagicPony outperforms prior work on this challenging task and demonstrates excellent generalisation in reconstructing art, despite the fact that it is only trained on real images. "
}