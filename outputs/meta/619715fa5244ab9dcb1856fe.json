{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Zero-shot generalisation in deep Reinforcement Learning"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Unifying formalism and terminology",
    "Categorisation of existing benchmarks",
    "Critical discussion and recommendations"
  ],
  "results": [
    "Argues against purely procedural content generation for benchmark design",
    "Suggests fast online adaptation and tackling RL-specific problems",
    "Recommends building benchmarks in underexplored settings like offline RL ZSG and reward-function variation"
  ],
  "paper_id": "619715fa5244ab9dcb1856fe",
  "title": "A Survey of Zero-shot Generalisation in Deep Reinforcement Learning",
  "abstract": "  The study of zero-shot generalisation (ZSG) in deep Reinforcement Learning (RL) aims to produce RL algorithms whose policies generalise well to novel unseen situations at deployment time, avoiding overfitting to their training environments. Tackling this is vital if we are to deploy reinforcement learning algorithms in real world scenarios, where the environment will be diverse, dynamic and unpredictable. This survey is an overview of this nascent field. We rely on a unifying formalism and terminology for discussing different ZSG problems, building upon previous works. We go on to categorise existing benchmarks for ZSG, as well as current methods for tackling these problems. Finally, we provide a critical discussion of the current state of the field, including recommendations for future work. Among other conclusions, we argue that taking a purely procedural content generation approach to benchmark design is not conducive to progress in ZSG, we suggest fast online adaptation and tackling RL-specific problems as some areas for future work on methods for ZSG, and we recommend building benchmarks in underexplored problem settings such as offline RL ZSG and reward-function variation. "
}