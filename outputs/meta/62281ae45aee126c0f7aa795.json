{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Mitigation of Shortcut Behavior in Machine Learning Models"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "XIL Framework Typology",
    "Common Set of Basic Modules",
    "Measures and Benchmarks for XIL Evaluation"
  ],
  "results": [
    "All methods successfully revise models",
    "Remarkable differences in individual benchmark tasks"
  ],
  "paper_id": "62281ae45aee126c0f7aa795",
  "title": "A Typology for Exploring the Mitigation of Shortcut Behavior",
  "abstract": "  As machine learning models become increasingly larger, trained weakly supervised on large, possibly uncurated data sets, it becomes increasingly important to establish mechanisms for inspecting, interacting, and revising models to mitigate learning shortcuts and guarantee their learned knowledge is aligned with human knowledge. The recently proposed XIL framework was developed for this purpose, and several such methods have been introduced, each with individual motivations and methodological details. In this work, we provide a unification of various XIL methods into a single typology by establishing a common set of basic modules. In doing so, we pave the way for a principled comparison of existing, but, importantly, also future XIL approaches. In addition, we discuss existing and introduce novel measures and benchmarks for evaluating the overall abilities of a XIL method. Given this extensive toolbox, including our typology, measures, and benchmarks, we finally compare several recent XIL methods methodologically and quantitatively. In our evaluations, all methods prove to revise a model successfully. However, we found remarkable differences in individual benchmark tasks, revealing valuable application-relevant aspects for integrating these benchmarks in developing future methods. "
}