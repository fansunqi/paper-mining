{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Distributed Graph Neural Network Learning"
  ],
  "datasets": [
    "Alipay dataset",
    "small-, modest- to large-scale datasets"
  ],
  "methods": [
    "GraphTheta",
    "NN-TGAR",
    "hybrid-parallel execution",
    "cluster-batched training strategy"
  ],
  "results": [
    "Scales well to 1,024 workers",
    "Outperforms DistDGL by up to 2.02x",
    "Outperforms GraphLearn by up to 30.56x",
    "Comparable model accuracy to existing frameworks"
  ],
  "paper_id": "6081539c91e011bce6b8afdc",
  "title": "GraphTheta: A Distributed Graph Neural Network Learning System With\n  Flexible Training Strategy",
  "abstract": "  Graph neural networks (GNNs) have been demonstrated as a powerful tool for analyzing non-Euclidean graph data. However, the lack of efficient distributed graph learning systems severely hinders applications of GNNs, especially when graphs are big and GNNs are relatively deep. Herein, we present GraphTheta, the first distributed and scalable graph learning system built upon vertex-centric distributed graph processing with neural network operators implemented as user-defined functions. This system supports multiple training strategies and enables efficient and scalable big-graph learning on distributed (virtual) machines with low memory. To facilitate graph convolutions, GraphTheta puts forward a new graph learning abstraction named NN-TGAR to bridge the gap between graph processing and graph deep learning. A distributed graph engine is proposed to conduct the stochastic gradient descent optimization with a hybrid-parallel execution, and a new cluster-batched training strategy is supported. We evaluate GraphTheta using several datasets with network sizes ranging from small-, modest- to large-scale. Experimental results show that GraphTheta can scale well to 1,024 workers for training an in-house developed GNN on an industry-scale Alipay dataset of 1.4 billion nodes and 4.1 billion attributed edges, with a cluster of CPU virtual machines (dockers) of small memory each (5$\\sim$12GB). Moreover, GraphTheta can outperform DistDGL by up to $2.02\\times$, with better scalability, and GraphLearn by up to $30.56\\times$. As for model accuracy, GraphTheta is capable of learning as good GNNs as existing frameworks. To the best of our knowledge, this work presents the largest edge-attributed GNN learning task in the literature. "
}