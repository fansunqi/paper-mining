{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Cross-view camera pose estimation"
  ],
  "datasets": [
    "VIGOR"
  ],
  "methods": [
    "SliceMatch: ground and aerial feature extractors, feature aggregators, and a pose predictor; cross-view attention module; geometric projection; precomputed masks; contrastive learning"
  ],
  "results": [
    "19% lower median localization error on the VIGOR benchmark using VGG16 backbone",
    "50% lower error when using ResNet50 backbone"
  ],
  "paper_id": "6385788490e50fcafdf498c9",
  "title": "SliceMatch: Geometry-guided Aggregation for Cross-View Pose Estimation",
  "abstract": "  This work addresses cross-view camera pose estimation, i.e., determining the 3-Degrees-of-Freedom camera pose of a given ground-level image w.r.t. an aerial image of the local area. We propose SliceMatch, which consists of ground and aerial feature extractors, feature aggregators, and a pose predictor. The feature extractors extract dense features from the ground and aerial images. Given a set of candidate camera poses, the feature aggregators construct a single ground descriptor and a set of pose-dependent aerial descriptors. Notably, our novel aerial feature aggregator has a cross-view attention module for ground-view guided aerial feature selection and utilizes the geometric projection of the ground camera's viewing frustum on the aerial image to pool features. The efficient construction of aerial descriptors is achieved using precomputed masks. SliceMatch is trained using contrastive learning and pose estimation is formulated as a similarity comparison between the ground descriptor and the aerial descriptors. Compared to the state-of-the-art, SliceMatch achieves a 19% lower median localization error on the VIGOR benchmark using the same VGG16 backbone at 150 frames per second, and a 50% lower error when using a ResNet50 backbone. "
}