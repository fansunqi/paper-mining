{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Optimal precision for GANs"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Geometric measure theory",
    "Truncation method"
  ],
  "results": [
    "Proves a sufficient condition for optimality",
    "Improves the performance of GANs"
  ],
  "paper_id": "62da159d5aee126c0f5dba5d",
  "title": "Optimal precision for GANs",
  "abstract": "  Many deep generative models are defined as a push-forward of a Gaussian measure by a continuous generator, such as Generative Adversarial Networks (GANs) or Variational Auto-Encoders (VAEs). This work explores the latent space of such deep generative models. A key issue with these models is their tendency to output samples outside of the support of the target distribution when learning disconnected distributions. We investigate the relationship between the performance of these models and the geometry of their latent space. Building on recent developments in geometric measure theory, we prove a sufficient condition for optimality in the case where the dimension of the latent space is larger than the number of modes. Through experiments on GANs, we demonstrate the validity of our theoretical results and gain new insights into the latent space geometry of these models. Additionally, we propose a truncation method that enforces a simplicial cluster structure in the latent space and improves the performance of GANs. "
}