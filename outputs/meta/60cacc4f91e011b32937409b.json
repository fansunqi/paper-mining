{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Learning from noisy labels (LNL)",
    "Improving generalization performance of pre-trained models on noisy label data"
  ],
  "datasets": [
    "various real-world and synthetic benchmark datasets"
  ],
  "methods": [
    "Influential Rank post-training approach",
    "Gradually removes samples with high influence on the decision boundary",
    "Refines the decision boundary"
  ],
  "results": [
    "Significantly improves generalization performance of pre-trained models on noisy label data",
    "Creates great synergies when combined with existing LNL methods",
    "Validated on diverse realistic scenarios"
  ],
  "paper_id": "60cacc4f91e011b32937409b",
  "title": "Influential Rank: A New Perspective of Post-training for Robust Model\n  against Noisy Labels",
  "abstract": "  Deep neural network can easily overfit to even noisy labels due to its high capacity, which degrades the generalization performance of a model. To overcome this issue, we propose a new approach for learning from noisy labels (LNL) via post-training, which can significantly improve the generalization performance of any pre-trained model on noisy label data. To this end, we rather exploit the overfitting property of a trained model to identify mislabeled samples. Specifically, our post-training approach gradually removes samples with high influence on the decision boundary and refines the decision boundary to improve generalization performance. Our post-training approach creates great synergies when combined with the existing LNL methods. Experimental results on various real-world and synthetic benchmark datasets demonstrate the validity of our approach in diverse realistic scenarios. "
}