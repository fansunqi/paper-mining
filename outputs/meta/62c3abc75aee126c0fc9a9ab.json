{
  "code_links": [
    "https://github.com/xcyao00/BGAD"
  ],
  "tasks": [
    "Supervised Anomaly Detection"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "Explicit Boundary Guided Semi-Push-Pull Contrastive Learning"
  ],
  "results": [
    "Enhanced discriminability",
    "Mitigated bias issue",
    "Explicit and discriminative decision boundary"
  ],
  "paper_id": "62c3abc75aee126c0fc9a9ab",
  "title": "Explicit Boundary Guided Semi-Push-Pull Contrastive Learning for\n  Supervised Anomaly Detection",
  "abstract": "  Most anomaly detection (AD) models are learned using only normal samples in an unsupervised way, which may result in ambiguous decision boundary and insufficient discriminability. In fact, a few anomaly samples are often available in real-world applications, the valuable knowledge of known anomalies should also be effectively exploited. However, utilizing a few known anomalies during training may cause another issue that the model may be biased by those known anomalies and fail to generalize to unseen anomalies. In this paper, we tackle supervised anomaly detection, i.e., we learn AD models using a few available anomalies with the objective to detect both the seen and unseen anomalies. We propose a novel explicit boundary guided semi-push-pull contrastive learning mechanism, which can enhance model's discriminability while mitigating the bias issue. Our approach is based on two core designs: First, we find an explicit and compact separating boundary as the guidance for further feature learning. As the boundary only relies on the normal feature distribution, the bias problem caused by a few known anomalies can be alleviated. Second, a boundary guided semi-push-pull loss is developed to only pull the normal features together while pushing the abnormal features apart from the separating boundary beyond a certain margin region. In this way, our model can form a more explicit and discriminative decision boundary to distinguish known and also unseen anomalies from normal samples more effectively. Code will be available at https://github.com/xcyao00/BGAD. "
}