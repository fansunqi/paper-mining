{
  "code_links": [
    "None"
  ],
  "tasks": [
    "Meta-learning for Gaussian process posteriors"
  ],
  "datasets": [
    "None"
  ],
  "methods": [
    "GP-PCA (Principal Component Analysis for Gaussian Process Posteriors)",
    "Variational inference approximation"
  ],
  "results": [
    "Demonstrated effectiveness of GP-PCA as meta-learning"
  ],
  "paper_id": "60f175e75244ab9dcb0d8988",
  "title": "Principal component analysis for Gaussian process posteriors",
  "abstract": "  This paper proposes an extension of principal component analysis for Gaussian process (GP) posteriors, denoted by GP-PCA. Since GP-PCA estimates a low-dimensional space of GP posteriors, it can be used for meta-learning, which is a framework for improving the performance of target tasks by estimating a structure of a set of tasks. The issue is how to define a structure of a set of GPs with an infinite-dimensional parameter, such as coordinate system and a divergence. In this study, we reduce the infiniteness of GP to the finite-dimensional case under the information geometrical framework by considering a space of GP posteriors that have the same prior. In addition, we propose an approximation method of GP-PCA based on variational inference and demonstrate the effectiveness of GP-PCA as meta-learning through experiments. "
}