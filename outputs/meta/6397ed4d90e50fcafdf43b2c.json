{
  "code_links": [
    "https://magvit.cs.cmu.edu"
  ],
  "tasks": [
    "Video synthesis"
  ],
  "datasets": [
    "Kinetics-600"
  ],
  "methods": [
    "Masked Generative Video Transformer",
    "3D tokenizer",
    "embedding method for masked video token modeling"
  ],
  "results": [
    "Best-published FVD on three video generation benchmarks",
    "Two orders of magnitude faster inference time than diffusion models",
    "60x faster inference time than autoregressive models",
    "Supports ten diverse generation tasks",
    "Generalizes across videos from different visual domains"
  ],
  "paper_id": "6397ed4d90e50fcafdf43b2c",
  "title": "MAGVIT: Masked Generative Video Transformer",
  "abstract": "  We introduce the MAsked Generative VIdeo Transformer, MAGVIT, to tackle various video synthesis tasks with a single model. We introduce a 3D tokenizer to quantize a video into spatial-temporal visual tokens and propose an embedding method for masked video token modeling to facilitate multi-task learning. We conduct extensive experiments to demonstrate the quality, efficiency, and flexibility of MAGVIT. Our experiments show that (i) MAGVIT performs favorably against state-of-the-art approaches and establishes the best-published FVD on three video generation benchmarks, including the challenging Kinetics-600. (ii) MAGVIT outperforms existing methods in inference time by two orders of magnitude against diffusion models and by 60x against autoregressive models. (iii) A single MAGVIT model supports ten diverse generation tasks and generalizes across videos from different visual domains. The source code and trained models will be released to the public at https://magvit.cs.cmu.edu. "
}