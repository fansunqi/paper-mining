{
  "code_links": "None",
  "tasks": [
    "Tabular data prediction"
  ],
  "datasets": [
    "Wine",
    "Phoneme",
    "ANALCATDATA Supreme",
    "Mercedes Benz Greener Manufacturing",
    "KDD Cup 09 Upselling",
    "KDD IPUMS LA 97-Small",
    "Wine Quality",
    "ISOLET",
    "CPU Act",
    "Bank-Marketing",
    "Brazilian Houses",
    "MagicTelescope",
    "Ailerons",
    "MiamiHousing2016",
    "OnlineNewsPopularity",
    "Elevators",
    "Credit",
    "Pol",
    "Superconduct",
    "House Sales",
    "Medical Charges",
    "FIFA",
    "Jannis",
    "Road-Safety",
    "Particulate Matter UK Air 2017",
    "Taxi-Green-DEC-2016",
    "MiniBooNE",
    "Year",
    "Churn Modelling",
    "California Housing",
    "House 16H",
    "Adult",
    "Diamond OpenML",
    "Otto Group Products",
    "Higgs-Small",
    "Black Friday",
    "Covertype2",
    "Microsoft"
  ],
  "methods": [
    "GG MoE (Gumbel-Softmax MoE)",
    "MoE (Mixture of Experts)",
    "MLP (Multilayer Perceptron)"
  ],
  "results": [
    "GG MoE achieves the highest average performance across 38 datasets",
    "Both MoE and GG MoE utilize significantly fewer parameters than MLPs",
    "GG MoE with an embedding layer outperforms standard MoE and MLP models",
    "MoE models with embeddings outperform E+MLP in training time"
  ],
  "title": "(GG) MoE Vs. MLP on Tabular Data.pdf",
  "abstract": "In recent years, significant efforts have been di- rected toward adapting modern neural network architectures for tabular data. However, despite their larger number of parameters and longer train- ing and inference times, these models often fail to consistently outperform vanilla multilayer per- ceptron (MLP) neural networks. Moreover, MLP- based ensembles have recently demonstrated su- perior performance and efficiency compared to ad- vanced deep learning methods. Therefore, rather than focusing on building deeper and more com- plex deep learning models, we propose investi- gating whether MLP neural networks can be re- placed with more efficient architectures without sacrificing performance. In this paper, we first introduce GG MoE, a mixture-of-experts (MoE) model with a Gumbel-Softmax gating function. We then demonstrate that GG MoE with an em- bedding layer achieves the highest performance across 38 datasets compared to standard MoE and MLP models. Finally, we show that both MoE and GG MoE utilize significantly fewer parameters than MLPs, making them a promising alternative for scaling and ensemble methods. 1. Introduction Supervised machine learning on tabular data is widely ap- plied, and its business value is undeniable, leading to the development of numerous algorithms to address these prob- lems. Gradient Boosting Decision Tree (GBDT) models (Chen & Guestrin, 2016; Ke et al., 2017; Prokhorenkova et al., 2018) have demonstrated superior performance com- pared to deep learning methods (Shwartz-Ziv & Armon, 2022; Grinsztajn et al., 2022) and remain the most common and natural choice for tabular data prediction. As a result, tabular data remains one of the few domains where neural networks do not yet dominate. In recent years, many researchers have attempted to adapt 1Independent Researcher. Correspondence to: Andrei Chernov <chernov.andrey.998@gmail.com>. transformer-based neural network architectures for tabu- lar data (Huang et al., 2020; Somepalli et al., 2021; Song et al., 2019). While these approaches have shown promising results on specific subsets of datasets, they often fail to con- sistently outperform vanilla Multilayer Perceptron (MLP) neural networks across a wide range of datasets (Gorishniy et al., 2024). This is despite their significantly higher com- putational requirements and larger number of parameters. Furthermore, a study (Gorishniy et al., 2024) demonstrated that efficient ensembles of MLPs tend to outperform ad- vanced deep learning models. This raises a question that, in our view, has been overlooked in recent research: Is there a neural network architecture that is more efficient than MLPs in terms of parameter count while still achieving comparable performance? Framing the problem this way makes investigating the per- formance of Mixture-of-Experts (MoE) models on tabular data a natural choice. MoE has recently gained popularity in deep learning (Fedus et al., 2022). However, to the best of our knowledge, little research has explored the adaptation of MoE to the tabular deep learning domain or evaluated its performance across a broad range of datasets. In this paper, we aim to address this gap. Additionally, we introduce MoE with a Gumbel-Softmax activation function on the output of the gating network (GG MoE); see Section 3.4 for details. We compare the perfor- mance of MoE and GG MoE with MLP across 38 datasets and demonstrate that GG MoE achieves the highest average performance while both MoE and GG MoE are significantly more parameter-efficient than MLP. 2. Related Work 2.1. Tabular Deep Learning Although it is theoretically proven that feedforward neural networks can approximate functions from a wide family with arbitrary accuracy (Cybenko, 1989; Hornik, 1991), in practice, they often underperform compared to GBDT meth- ods in the tabular domain (Gorishniy et al., 2021; Shwartz- Ziv & Armon, 2022; Grinsztajn et al., 2022). To improve performance, extensive research has been conducted. Here, we highlight three main directions. 1 arXiv:2502.03608v1 [cs.LG] 5 Feb 2025"
}